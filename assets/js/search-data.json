{
  
    
        "post0": {
            "title": "What are Convolutional Neural Networks?",
            "content": "Introduction . Before we look at convolutional neural networks, we should look at what a convolution is. . Convolutions are a kind of feature engineering, which is transforming data to make it easier to model. With tabular data, we used add_datepart to split the date column into many metacolumns. The metacolumns (and the initial date column) are what we call features. . So, what kind of features can we make with images? With images, a feature is something that is visually distinctive. If we can&#39;t see it, it&#39;s unlikely the computer can see it. But, what if it&#39;s there and it&#39;s just difficult to see it at first glance? That&#39;s where we use convolutions. . . A convolution is an operation that applies a kernel across channels of an image. . . An image can be thought of as a three dimensional matrix: channel $ times$ height $ times$ width where channel can be indexed for the brightness of a single colour. In RGB images, we have three channels: red, green, and blue. In greyscale images, we have one channel: white. . A kernel is a square matrix that&#39;s applied to each channel of the image. For instance, the following 3 $ times$ 3 kernel is used to amplify horizontal edges, particularly the top edges: . top_edge = tensor([-1, -1, -1], [ 0, 0, 0], [ 1, 1, 1]).float() top_edge . tensor([[-1., -1., -1.], [ 0., 0., 0.], [ 1., 1., 1.]]) . When we apply a kernel across an image, we multiply a (valid) subtensor of the same shape as our kernel and take the sum as illustrated by this gif: . The bottom matrix is the image and the grey 3x3 matrix on the image is the kernel. The top matrix is what we get after the convolution. We start with a 7x7 image and get a 5x5 image, losing 1 pixels on each side. The subtensor is valid when every item in the kernel can be mapped to a unique item in the subtensor. So, we can&#39;t have a subtensor where the center is along an edge. . To show the kernel in action, we can define a function that applies what we described above: . def apply_kernel(img, row, col, kernel): # Through our indexing, you can see how having the center (row, col) # along the edge would cause errors with indexing return (img[row - 1:row + 2, col - 1:col + 2] * kernel).sum() . And we&#39;ll get an image: . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path . img = tensor(Image.open(path/&#39;train&#39;/&#39;3&#39;/&#39;52269.png&#39;)).float()/255 show_image(img); . Then, we can use list comprehension to show the convoluted image: . r = range(1, 27) img_ = tensor([[apply_kernel(img, i, j, top_edge) for j in r] for i in r]) show_image(img_); . To simplify it for future images, we can put the above convolution process into a function: . def apply_kernel_img(img, kernel): w = kernel.shape[0] // 2 r = range(w, img.shape[0] - w) return tensor([[apply_kernel(img, i, j, kernel) for j in r] for i in r]) . Next, we can create a few more kernels: . bot_edge = tensor([ 1, 1, 1], [ 0, 0, 0], [-1, -1, -1]).float() lef_edge = tensor([-1, 1, 0], [-1, 1, 0], [-1, 1, 0]).float() . And try them out: . show_image(apply_kernel_img(img, bot_edge), title=&#39;Bottom&#39;), show_image(apply_kernel_img(img, lef_edge), title=&#39;Left&#39;); . You can see how in our apply_kernel_img function that we create our range with bounds. If our kernel is 3 $ times$ 3, then our w becomes 1 and we need our center to be 1 pixel apart from the edge at all times. In general, if a kernel is k $ times$ k (with k being odd since an even kernel is almost never seen in practice and would require different padding on each side), then our center needs to stay k // 2 apart from the edge. . But, what if we could pad the edges? Then, we would be able to have our original image size returned when we apply a convolution; like this: . We have a 5x5 image with a padding of 1 pixel on each side since our kernel is 3x3 and 3 // 2 = 1. The convoluted image is then a 5x5 image. So with padding, we get to retain all the information from the image. . However, we don&#39;t always want the original image size returned. Therefore, we have the idea of strides, where a stride-n convolution applies the convolution every n pixels. . We have the same as above, where we have a padding of 1 pixel on each size, except we skip every other pixel to have stride-2 convolution. We skip every other pixel in both directions (vertical and horizontal). Overall, each dimension of the convoluted image will be of size (n + 2 * pad - k) // stride + 1, where n is the original size for the image, pad is the padding, k is the kernel size and stride is the stride. So, for the above example, we have n = 5, pad = 3 // 2 = 1, k = 3, and stride = 2 so our resulting image will have dimensions of (5 + 2 * 1 - 3) // 2 + 1 = 3 thus 3 $ times$ 3. . Now that we&#39;ve learned about strides, let&#39;s implement it in our previous function: . def apply_kernel_img_w_stride(img, kernel, stride=2): w = kernel.shape[0] // 2 r = range(w, img.shape[0] - w, stride) return tensor([[apply_kernel(img, i, j, kernel) for j in r] for i in r]) . Through striding, we get: . imgs = apply_kernel_img_w_stride(img, bot_edge), apply_kernel_img_w_stride(img, lef_edge) show_image(imgs[0], title=&#39;Bottom&#39;), show_image(imgs[1], title=&#39;Left&#39;); . Where the shape of our tensors are halved: . imgs[0].shape . torch.Size([13, 13]) . Moving onto Convolutional Neural Networks . We&#39;ve looked at convolutions, padding, and strides. How can we put them inside a neural network to get a convolution neural network (abbreviated to CNN)? . With LSTM RNNs, we made our model learn the importance of what to remember and what to forget. We can do the same with CNNs; actually, we can do the same with basically any neural network since it&#39;s based off of SGD: instead of defining our own kernels, we make our model learn its own kernels. . A neural network that uses convolutions instead of, or in addition to, linear layers is what we call a CNN. . Say we have a simple neural network like: . simple_nn = nn.Sequential( nn.Linear(28 * 28, 30), nn.ReLU(), nn.Linear(30, 1) ) simple_nn . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . We can turn it into a simple CNN by replacing the nn.Linear layers with nn.Conv2d layers which are convolutional layers (in 2-dimension): . simple_cnn = nn.Sequential( nn.Conv2d(1, 30, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(30, 1, kernel_size=3, padding=1) ) simple_cnn . Sequential( (0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(30, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) . With CNNs, we don&#39;t need to specify the size of the input image like in simple_nn. A linear layer needs a weight in the weight matrix for each pixel, but with a convolutional layer, we only need to define the kernel&#39;s size and the kernel is applied to each pixel automatically. The weights of the kernel only depend on the number of input and output channels and the kernel size. . So, why do we use strides? Well, the output of our simple_cnn has the following shape: . db = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label) dls = db.dataloaders(path) xb, yb = first(dls.valid) xb, yb = to_cpu(xb), to_cpu(yb) simple_cnn(xb).shape . torch.Size([64, 1, 28, 28]) . Instead of getting two activations that we can use for classification between 3s and 7s, we get 28 $ times$ 28 activations. One way we can narrow down to two activations is to use a stride of 2, where we half the dimensions of each output image with each layer. . By default, nn.Conv2d has a stride of (1, 1), which is 1 horizontally and 1 vertically. Since we don&#39;t want to define kernel_size and padding each time, we can refactor a layer like so: . def conv(in_channels, out_channels, kernel_size=3, nonlinearity=True): layer = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, stride=2) if nonlinearity: layer = nn.Sequential(layer, nn.ReLU()) return layer . Then, we can make a new model where we half our way to 2 activations for a 1 $ times$ 1 image: . simple_cnn = nn.Sequential( conv(1, 4), # 14x14 conv(4, 8), # 7x7 conv(8, 16), # 4x4 conv(16, 32), # 2x2 conv(32, 2, nonlinearity=False), # 1x1 Flatten() ) . With this new model, our resulting shape is: . simple_cnn(xb).shape . torch.Size([64, 2]) . Next, we&#39;ll make a Learner with our model: . learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy) . Then, with the summary method, we can see how the shape of the output changes at each layer: . learn.summary() . Sequential (Input shape: 64 x 1 x 28 x 28) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 14 x 14 Conv2d 40 True ReLU ____________________________________________________________________________ 64 x 8 x 7 x 7 Conv2d 296 True ReLU ____________________________________________________________________________ 64 x 16 x 4 x 4 Conv2d 1168 True ReLU ____________________________________________________________________________ 64 x 32 x 2 x 2 Conv2d 4640 True ReLU ____________________________________________________________________________ 64 x 2 x 1 x 1 Conv2d 578 True ____________________________________________________________________________ 64 x 2 Flatten ____________________________________________________________________________ Total params: 6,722 Total trainable params: 6,722 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f331e210050&gt; Loss function: &lt;function cross_entropy at 0x7f3348e15a70&gt; Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Our last conv layer produces a tensor of shape 64 x 2 x 1 x 1, but we only want 64 x 2, so we use Flatten to remove the extra 1 x 1 axes. . Because it&#39;s a deeper network than simple_nn, we&#39;ll have to train it with a lower learning rate with more epochs: . learn.fit_one_cycle(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.029754 | 0.026407 | 0.991659 | 00:19 | . 1 | 0.027752 | 0.033296 | 0.987242 | 00:19 | . 2 | 0.010745 | 0.010732 | 0.996075 | 00:20 | . 3 | 0.004099 | 0.011542 | 0.995584 | 00:20 | . What&#39;s up with the increasing out-channels? . When we use a stride of 2, we decrease the number of activations by a factor of 4 at each layer because the height and the width of the output image are being halved. . The number of trainable weights for a given layer is out_channels * in_channels * kernel_height * kernel_width. PyTorch has the axes of the tensor in the order of in_channels, out_channels, kernel_height, kernel_width: . simple_cnn[0][0].weight.shape . torch.Size([4, 1, 3, 3]) . So, we have 36 weights for the first layer. The number of trainable parameters, which is given in the summary is weights + biases. For each layer, the number of biases is equal to the number of out_channels. Since there&#39;s 4 out-channels for the first layer, the total trainable parameters is 40. . If we look at the multiplications the model is doing, at each layer the model is doing image_height * image_width * weights multiplications. Since the output image decreases by a factor of 4 after each layer, the number of multiplications also decreases by a factor of 4 for the next layer if we held the number of weights constant. To keep the number of multiplications the same or larger, we need to increase the number of channels (i.e. features) at each level. . Starting with a higher out-channel and then doubling it after each layer ends up working out since the in-channel of the next layer is double that of the current and the out-channel of the next layer is double that of the current. So, we get a total increase by a factor of 4. . Why do we even need to multiply the same or even more for later layers? Well, it wouldn&#39;t make sense for the later layers, which are meant to be learning semantically rich features, to be doing fewer computations than the earlier layers. . Don&#39;t believe me? Let&#39;s try it with our current data set and see how our accuracy differs: . bad_cnn = nn.Sequential( conv(1, 4), conv(4, 4), conv(4, 4), conv(4, 4), conv(4, 2, nonlinearity=False), Flatten() ) learn = Learner(dls, bad_cnn, loss_func=F.cross_entropy, metrics=accuracy) . learn.fit_one_cycle(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.123358 | 0.065982 | 0.978901 | 00:18 | . 1 | 0.039740 | 0.039391 | 0.988714 | 00:19 | . 2 | 0.029994 | 0.040954 | 0.986261 | 00:20 | . 3 | 0.019694 | 0.032848 | 0.989205 | 00:21 | . Unsurprisingly, the accuracy is a bit worse. However, it&#39;s also not worse by much. Does that mean our previous hypothesis of &quot;later layers should have more computations than earlier layers&quot; is false? Not really. Our current data set is just comparing two different categories: 3s and 7s. It&#39;s when we get to more complex image categories (or more categories) that we&#39;ll see a larger drop in accuracy. . Before we get to the full MNIST data set, let&#39;s look at how we can deal with images other than greyscale images. . &quot;Double raindow... what does it mean?&quot; . Well, a double rainbow is a phenomenon of optics that displays a spectrum of light due to the sun shining on droplets of moisture in the atmosphere. Does that explain it? . With each convolutional layer, we have in_channels and out_channels; that is, for each out_channels, there&#39;s in_channels kernels that are each of size kernel_height $ times$ kernel_width. These kernels are the weights that we train and as mentioned before, have weights that total to out_channels * in_channels * kernel_height * kernel_width. The total trainable parameters is equal to the total weights and the total biases. We don&#39;t have separate biases for each weight. Instead, we have a bias for each set of in_channels kernels, and, how many are there? There&#39;s out_channels sets. . But what exactly are in-channels and out-channels? Let&#39;s look at the first layer. For a greyscale image, we had an in-channel of 1, whose &quot;feature&quot; is a 2 dimensional tensor that contains the brightness for each pixel. With an out-channel of size $n$, we want out_channels ($n$) sets of in_channels kernels, each of size kernel_height $ times$ kernel_width, and each with a bias, that will somehow (trained through SGD) create $n$ different images that each depict (hopefully) a unique feature (i.e. channel) that can be helpful in classification; that&#39;s what I mentioned at the beginning of this blog:convolutions are a kind of feature engineering, which is transforming data to make it easier to model (classify). How does out_channels in_channels kernels work together? For each set (which is a set of in_channels kernels), each kernel is applied to its appropriate channel, which results in in_channels images. These images can be stacked and each stacked pixel are summed to result in a single tensor of shape new_image_height $ times$ new_image_width. Overall, we&#39;ll have out_channels of these &quot;images&quot; (technically, they&#39;re features; technically-technically, they&#39;re channels) that we feed as input for the next layer, or use for classification. . . With the way I&#39;m using the terms images, features, and channels, it can be a little confusing as to what they mean. . One of the channels of an image would be the 2 dimensional tensor that&#39;s being used as the initial input, or a part of the output of each layer (the last 2 axes). . Features and channels are essentially the same: they&#39;re the second axis of the output, which is how many activations there are for each output image pixel. . In general, they&#39;re actually &quot;features&quot; that you can take from an image like edges, which we showed in the beginning through actual code, gradients, colour, and other kinds of things. . The only difference between features and channels is that features are only used to describe the output of the layer (what we actually get after we apply the kernels), while channels can refer to both the input and the output (hence, why we have in_channels and out_channels instead of in_features). . . So, what&#39;s this about a double rainbow? Well, what do rainbows have? Colour! How can we incorporate coloured images into a CNN? It&#39;s actually really simple: instead of starting with an in_channels of 1 (currently used for the brightness of each pixel), we start with the appropriate number required to describe the image. An RGB image has an in_channels of 3: one for red, green, and blue. An RGBA image (Red-Green-Blue-Alpha) has an in_channels of 4. . What&#39;s even crazier is that it doesn&#39;t matter what colour scheme you use to describe your image when training: the results aren&#39;t that different. BUT, this rule only applies if no information is lost when you change the colour scheme. For example, going from a greyscale to a coloured image is fine since the image is still greyscale. But, going from a coloured image to greyscale is going to give different results since you&#39;re losing all the information that comes with colour. . Becoming a master of digits . In becoming a master of digits, one must be able to classify every digit in a handwritten number. You may not have noticed it, but we&#39;re already one step ahead by using fit_one_cycle on our learner. . Fastai actually has a fit function that you can use to train your model instead of fit_one_cycle. fit_one_cycle doesn&#39;t mean fit for one cycle (i.e. epoch). It actually implements 1cycle. . . The foundation of 1cycle training is that our randomized initial weights aren&#39;t suited for our task so it&#39;s a bad idea to start with a high learning rate: we may diverge instantly (get far away from a low loss) and skip over a minimum loss near the end of training. . So, what if we had a &quot;cycle&quot; using &quot;1&quot; learning rate, where we start with a learning rate lower than the given learning rate, gradually make our way to the maximum learning rate, and then go back down to a lower learning rate? By doing so, we get two benefits: . we can train faster since we take bigger steps at the optimization stage; and | we overfit less since we skip sharp minimums, which may not be &quot;minimums&quot; at the slightest change. | . Why do we get these benefits? We start with a lower learning rate so our loss won&#39;t explode to crazy places; eventually, we end up in a pretty smooth area. Then, we increase the learning rate, gradually, to the maximum learning rate. Since we&#39;re already in a smooth region (thus lower gradients), we step by a larger amount, but we aren&#39;t stepping by a crazy amount. Then, we eventually settle into a very nice smooth region and we begin lowering our learning rate to get the very nice minimum loss. . Instead of slowly inching towards a good minimum through a low learning rate, we inch a little in the beginning, zoom to a good place, and then inch again to a nice minimum. . . So, let&#39;s try training our simple_cnn model using fit and see how our accuracy changes: . learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy) learn.fit(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.045561 | 0.056122 | 0.980373 | 00:18 | . 1 | 0.028535 | 0.032803 | 0.988714 | 00:17 | . 2 | 0.014569 | 0.021691 | 0.993131 | 00:18 | . 3 | 0.014002 | 0.019942 | 0.995093 | 00:17 | . It doesn&#39;t decrease by much. Is 1cycle unnecessary? Let&#39;s try downloading the full MNIST data set and training it with fit. . path = untar_data(URLs.MNIST) . Unlike the sample MNIST data set, which contains just 3s and 7s, we get all the digits in this data set. But, it&#39;s also stored differently: . !ls {path} . testing training . We have our data separated into two folders: testing and training. In each folder, we have subfolders that contain images for each digit: . !ls {path}/training . 0 1 2 3 4 5 6 7 8 9 . So, we&#39;ll have to tell our DataBlock to make our DataLoaders differently: . def get_dls(bs=64): return DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(&#39;training&#39;, &#39;testing&#39;), # GrandparentSplitter defaults to &#39;train&#39; and &#39;test&#39; # so we&#39;ll have to tell it to look at different folders get_y=parent_label, batch_tfms=Normalize() # we also normalize the batches so that our input # batch is within a similar range of numbers ).dataloaders(path, bs=bs) . dls = get_dls() . To make sure we got our images correctly, we can look at a batch: . dls.show_batch(max_n=9, figsize=(4, 4)) . Now that we have our input data prepared, we&#39;ll have to make some adjustments to our simple_cnn since we&#39;re now dealing with 10 possible outputs instead of 2. . def simple_cnn(): return sequential( conv(1, 8, kernel_size=5), conv(8, 16), conv(16, 32), conv(32, 64), conv(64, 10, nonlinearity=False), Flatten() ) . This time, we start with more out-channels (because there&#39;ll be more features to distinguish between all digits compared to just 3s and 7s) and end with more out-channels (10 out-channels since we have 10 digits to classify). . We also change the initial kernel size since we wouldn&#39;t expect the model to learn 8 different features by looking at 9 pixels at a time. Instead, it&#39;d be better for the model to learn 8 unique features by looking at 25 pixels at a time (through a kernel size of 5 $ times$ 5). . Now, let&#39;s train this model: . # We default to a higher learning rate so that we can # keep training time short while making changes def fit(epochs=1, lr=6e-2): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit(epochs, lr) return learn . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 2.305712 | 2.308704 | 0.097400 | 01:35 | . See? We can&#39;t even get a 10% accuracy with just fit when we have a more complex data set. . To see why we&#39;re having such a bad accuracy, let&#39;s look at the activations (which we can since we added the ActivationStats CallBack that records the mean, standard deviation, and a histogram of every activation from every trainable layer): . learn.activation_stats.plot_layer_stats(0) . The x-axis of these graphs are the batches and the y-axis is given by the graph&#39;s name. . Ideally, we want our activations to have a smooth mean and standard deviation because that means our training is stable (i.e., isn&#39;t leading to sudden changes). . Currently, our first layer is training smoothly, but is getting increasing amounts of activations near zero. . Let&#39;s see what the final point means for our penultimate (second-last) layer: . learn.activation_stats.plot_layer_stats(-2) . The mean is getting to a constant (a bit too smooth) and our standard deviation is getting very close to 0. Why might this be the case? Almost all our activations are 0 by the time we get to the later layers. Just what is our model learning? . We don&#39;t want many activations near 0 since that means our model is disregarding information. We also don&#39;t want an increasing trend of &quot;zero&quot; activations because it snowballs for the later layers since multiplying by zero gives zero. . So, let&#39;s try improving our training stability by using 1cycle: . def fit(epochs=1, lr=6e-2): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit_one_cycle(epochs, lr) return learn . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.171545 | 0.149211 | 0.954500 | 01:53 | . We can inspect what 1cycle&#39;s doing through the Recorder callback. It starting with a lower learning rate, gradually increases to the given maximum (6e-2), then goes back to the lower one. We can see the changes across the batches through the left graph: . learn.recorder.plot_sched() . The right graph shows the change in momentum. Momentum, like the name indicates, keeps the optimizer stepping in the same direction as it did in the previous steps. Momentum is also changed according to 1cycle, although in the opposite direction (high-low-high). . Overall, we see a great improvement in our accuracy. But, is our activations more stable? . learn.activation_stats.plot_layer_stats(-2) . Not really... we see a few spikes here and there, but it eventually stabilizes. However, we also see that a lot of our activations get close to 0. And, we start with an unusually high number of &quot;zero&quot; activations. Why might this be the case? . Before we look into why and how we can fix it, let&#39;s look at this problem from a different perspective. Fastai also provides a colorful dimension graph, which plots histograms of the activations from each batch along the x-axis, where colormaps are used to simulate the height, hence viewing the height through the &quot;colorful dimension&quot;. . The activations are log&#39;d before they&#39;re recorded in the histograms, which is why the means displayed above are negative: the closer to 0, the closer to negative infinity in terms of log: . So, our colorful dimension graph for our penultimate layer is: . You should imagine the graph above to be the upper portion of what would be a mirrored graph along the x-axis. . If you&#39;re having trouble linking the colormap to height, this graph in 3-dimensions looks like this: . def color_dim3d(idx=-2, elev=90, azim=-90, cmap=&#39;inferno&#39;): res = learn.activation_stats.hist(idx) res.shape fig = plt.figure(figsize=(20, 15), constrained_layout=True) ax = plt.gca(projection=&#39;3d&#39;) x, y = res.shape X, Y = np.mgrid[0:x, 0:y] # ax.set_xlim3d(0, y) # ax.set_ylim3d(0, x) # ax.set_zlim3d(res.min().item(), res.max().item()) x_scale=1 y_scale=0.5 z_scale=0.1 scale=np.diag([x_scale, y_scale, z_scale, 1.0]) scale=scale*(1.0/scale.max()) scale[3,3]=1.0 def short_proj(): return np.dot(Axes3D.get_proj(ax), scale) ax.get_proj=short_proj ax.plot_surface(Y, X, res, cmap=cmap, rcount=50, ccount=150) ax.view_init(elev, azim) ax.grid(False) ax.axis(&#39;off&#39;) plt.savefig(&#39;a.png&#39;, bbox_inches=&#39;tight&#39;) . . . The colorful dimension graph shows that we have a case of &quot;bad training&quot; where over the first few batches the number of nonzero activations exponentially increases, but then it crashes and over the next course of batches, the model basically learns again from the beginning, but with less &quot;zero&quot; activations. This cycle repeats until it eventually reaches a decent spread of activations. . So, we&#39;ve looked at the statistics based on the activations. There&#39;s decent stability near the end of the training, but why can&#39;t we have that stability in the beginning? . Maybe we have a few batches that&#39;re much different compared to the other one. Those few odd batches may be causing a set of much different activations. . Then, what&#39;s a way we can reduce the odds of a batch being more different? If we look at probability, we can make the odds smaller by increasing the denominator. Therefore, we increase the batch size: . dls = get_dls(512) . With a larger batch size, we get to calculate our activations with more data, so our gradients will be more accurate. However, we&#39;ll have fewer batches per epoch, meaning we&#39;ll have fewer steps in the loss per epoch. . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.204972 | 0.078568 | 0.976200 | 01:24 | . We see an improvement in accuracy. Let&#39;s look at the activations of our penultimate layer: . learn.activation_stats.plot_layer_stats(-2) . Althrough the curves do look smoother, and we have less activations near zero, we still start with a very high number of zero activations, which leads to the same problem that&#39;s shown in the colorful dimension graph: . learn.activation_stats.color_dim(-2) . Is the stability of our model&#39;s training doomed? . Well, we&#39;re looking at the activations, remember? What can we do to fix much different activations that can also smoothen the mean and standard deviation? Normalization. . Normalization can even eliminate the higher number of near zero activations in the beginning since they&#39;ll be spread out along a normal distribution. . So, let&#39;s talk about batch normalization; this process isn&#39;t applied to batches given by DataLoaders (which is already handled by the Normalize we passed to batch_tfms in our DataBlocks), but is applied to a batch of activations given by a layer. Therefore, we add a layer that applies batch normalization, which we call batchnorm, after each &quot;layer&quot; (a linear/convolutional layer and an optional nonlinearity). . However, life isn&#39;t so simple and the values returned through normalization are small. Sometimes, the network needs larger activations to make good predictions; so, if a normalized activation is $y$, then the batchnorm layer actually returns $ gamma y + beta$, where $ gamma$ and $ beta$ are trainable parameters that are changed at every batch during the optimization step. . Consequently, batchnorm allows us to have any mean or variance in our activations, independent from the activations of the previous layers. Additionally, a model containing batchnorm tends to generalize better because of the additional parameters $ gamma$ and $ beta$ that work to account for varying means and variances amongst activations from different batches of input. . Finally, we normalize the activations differently depending on training and during inference. At training, we use the mean and standard deviation of the batch; but, during inference, we normalize by using the mean of the statistics (mean and standard deviation) of the batches that we had during training. . def conv(in_channels, out_channels, kernel_size=3, nonlinearity=True): layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride=2, padding=kernel_size//2)] if nonlinearity: layers.append(nn.ReLU()) layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) # the * unpacks the list into a sequence of elements . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.133140 | 0.057463 | 0.986000 | 01:22 | . learn.activation_stats.plot_layer_stats(-2) . Now, although our mean is still unsteady in the beginning, our standard deviation is much more stable and we have much less activations near zero in the beginning. . All these changes can also be seen through the colorful dimension graph: . learn.activation_stats.color_dim(-2) . With the above graph, we start with mostly near zero activations, which then become more distributed and stay distributed through a stable standard deviation. . Using what we&#39;ve covered thus far, let&#39;s train a model using more epochs and a higher learning rate (which batchnorm enables): . learn = fit(5, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.179509 | 0.100619 | 0.969700 | 01:22 | . 1 | 0.072631 | 0.051186 | 0.984000 | 01:22 | . 2 | 0.048186 | 0.041086 | 0.986900 | 01:21 | . 3 | 0.030579 | 0.030568 | 0.989800 | 01:20 | . 4 | 0.014664 | 0.025618 | 0.992000 | 01:21 | . Our colorful dimension graph indicates &quot;good training&quot; through its shape: . learn.activation_stats.color_dim(-2) . . Conclusion . In a previous blog, we trained a CNN-based architecture, resnet18, on the MNIST data set using cnn_learner and fit_one_cycle to get an accuracy of 98.3% after fine-tuning for 1 epoch at a 0.1 learning rate. . In this blog, we&#39;ve covered what convolutions are and how they&#39;re used to create convolutional layers in a convolutional neural network. We&#39;ve trained a model using a simple CNN on a sample MNIST data set to compare 3s and 7s. However, we&#39;ve found that task to be too simple. Thus, we moved onto the full MNIST data set. Little did we know, our simple CNN from before wouldn&#39;t even be able to get a 10% accuracy; it&#39;s worse than a chimp. . We began with too high of a learning rate. But, if we kept lowering it, our training would take far too long. Luckily, 1cycle allows us to have that learning rate as our maximum and we&#39;ll be able to train quickly, while taking advantage of lower learning rates. Nonetheless, we still had some instability in our training. As we saw with the colorful dimension graphs, our model had too many initial zero activations and it caused our model to explode and collapse, retraining basically from scratch after a few batches. Thankfully, we&#39;ve learned about batch normalization and that gave us the stability we needed to train our model and end up with an accuracy of 99.2%. . Not only did we beat the result on our previous blog, we even learned how everything works to get that score. . In the next blog, we&#39;ll be looking at residual networks (i.e. resnet) and how we can use that to get an even better score. .",
            "url": "https://geon-youn.github.io/DunGeon/vision/2022/04/30/Convolutional-Neural-Networks.html",
            "relUrl": "/vision/2022/04/30/Convolutional-Neural-Networks.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "What are Recurrent Neural Networks?",
            "content": "In a previous blog, we used a pretrained model that used the AWD-LSTM architecture. This architecture is built off a recurrent neural network. &quot;Recurrent&quot;, according to Cambridge Dictionary means &quot;happening again many times&quot;. And it just so happens that a recurrent neural network is a neural network with layers that happen again (repeat) many times. . To go over RNNs in this blog, we&#39;ll be using the human numbers data set that contains the first 10,000 numbers written out in English. . We&#39;ll download the data set from fastai&#39;s URLs class: . from fastai.text.all import * path = untar_data(URLs.HUMAN_NUMBERS) . Then, we&#39;ll see how the data set is laid out: . path.ls() . (#2) [Path(&#39;train.txt&#39;),Path(&#39;valid.txt&#39;)] . There&#39;s two text files that contain the numbers. Since we&#39;re creating a language model, we&#39;ll concatenate them: . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . Then, we can join the lines together, separated by dots so that we can tokenize them: . text = &#39; . &#39;.join([i.strip() for i in lines]) text[:100] . &#39;one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo&#39; . We&#39;ll then tokenize them by splitting them according to spaces: . tokens = text.split(&#39; &#39;) tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . We first joined them with a period instead of spaces because the spaces between the words are significant. We want to separate numbers, not words, as in, we want this: . text[text.rindex(&#39;.&#39;):] . &#39;. nine thousand nine hundred ninety nine&#39; . Not this: . nine . thousand . nine . hundred . ninety . nine . Next, we&#39;ll create our vocab by making a list of the unique tokens: . vocab = L(tokens).unique() vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . And then we&#39;ll numericalize the tokens. In this blog, we&#39;ll be keeping the notation of input_target like &quot;input&quot; to (_) &quot;target&quot;, so t_i means token to index: . t_i = {t: i for i, t in enumerate(vocab)} nums = L(t_i[t] for t in tokens) nums[:10] . (#10) [0,1,2,1,3,1,4,1,5,1] . We want our model to predict the next word given the last 3 words in the sequence, so we can do that with just Python: . seqs_tok = L((tokens[i:i+3], tokens[i+3]) for i in range(0, len(tokens)-4, 3)) seqs_tok . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . And since it looks right with the tokens, we&#39;ll do the same with the numericalized tokens (and it should look like the above, but with numericalized tokens instead): . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0, len(nums)-4, 3)) seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . Then, we&#39;ll make our DataLoaders: . cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . And check that we can make a batch: . dls.one_batch() . (tensor([[ 0, 1, 2], [ 1, 3, 1], [ 4, 1, 5], [ 1, 6, 1], [ 7, 1, 8], [ 1, 9, 1], [10, 1, 11], [ 1, 12, 1], [13, 1, 14], [ 1, 15, 1], [16, 1, 17], [ 1, 18, 1], [19, 1, 20], [ 1, 20, 0], [ 1, 20, 2], [ 1, 20, 3], [ 1, 20, 4], [ 1, 20, 5], [ 1, 20, 6], [ 1, 20, 7], [ 1, 20, 8], [ 1, 20, 9], [ 1, 21, 1], [21, 0, 1], [21, 2, 1], [21, 3, 1], [21, 4, 1], [21, 5, 1], [21, 6, 1], [21, 7, 1], [21, 8, 1], [21, 9, 1], [22, 1, 22], [ 0, 1, 22], [ 2, 1, 22], [ 3, 1, 22], [ 4, 1, 22], [ 5, 1, 22], [ 6, 1, 22], [ 7, 1, 22], [ 8, 1, 22], [ 9, 1, 23], [ 1, 23, 0], [ 1, 23, 2], [ 1, 23, 3], [ 1, 23, 4], [ 1, 23, 5], [ 1, 23, 6], [ 1, 23, 7], [ 1, 23, 8], [ 1, 23, 9], [ 1, 24, 1], [24, 0, 1], [24, 2, 1], [24, 3, 1], [24, 4, 1], [24, 5, 1], [24, 6, 1], [24, 7, 1], [24, 8, 1], [24, 9, 1], [25, 1, 25], [ 0, 1, 25], [ 2, 1, 25]]), tensor([ 1, 4, 1, 7, 1, 10, 1, 13, 1, 16, 1, 19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 0, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 0, 2, 3])) . . For a model that takes in 3 words as input and tries to predict the next word, we can: . Calculate the embeddings for the first word, | Pass the embeddings into a linear layer, | Apply a nonlinearity (like ReLU or softmax), | Calculate the embeddings for the second word, | Add the embeddings to the activations from step 3, | Pass the activations into the same linear layer in step 2, | Apply a nonlinearity, and | Repeat steps 4 to 7 with the third word. | By adding the next word&#39;s embeddings to the previous activations, every word is interpreted in the context of the preceding words. And, we use the same weight matrix (linear layer) since the way one word influences the activations from the previous words shouldn&#39;t change depending on the position of the word; so, we force the layer to learn all positions instead of limiting each layer to one position. . To turn this idea into code, we can make a model by inheriting from PyTorch&#39;s Module class: . class RNNish(Module): &quot;&quot;&quot; We have three different states: - Input (the words) - Hidden (activations) - Output (the probabilities for the next word) We then have three different layers: - i_h: input to hidden - The embedding matrix to turn our words into embeddings - h_h: hidden to hidden - Calculates the activations for the next word - h_o: hidden to output - Calculates the predictions for the next word &quot;&quot;&quot; def __init__(self, n_vocab, n_hidden): self.i_h = nn.Embedding(n_vocab, n_hidden) # If we want a more complex model, # we would be altering this # hidden to hidden layer into more layers self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden, n_vocab) # This is what our steps would look like def forward(self, x): h = self.i_h(x[:,0]) h = F.relu(self.h_h(h)) h = h + self.i_h(x[:,1]) h = F.relu(self.h_h(h)) h = h + self.i_h(x[:,2]) h = F.relu(self.h_h(h)) return self.h_o(h) . If you took any intro to CS course, you might&#39;ve had a point where you didn&#39;t learn while loops yet so you were copy pasting if statements and changing a couple numbers here and there. Well, we know loops so we turn our repetitive &quot;calculate the next embeddings, add it to the hidden state, then calculate the next activations&quot; into a loop. . A hidden state is the activations that&#39;re updated at each step of a recurrent neural network (which we can see below in the for loop): . class RNN(Module): def __init__(self, n_vocab, n_hidden): self.i_h = nn.Embedding(n_vocab, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden, n_vocab) # This is how we can simplify to turn it # into a recurrent (looped) neural network def forward(self, x): # We can set it to 0 because tensors have # a thing called &quot;broadcasting&quot; that tries # to expand the smaller shape tensor into # the same shape as the other one h = 0 for i in range(3): h = h + self.i_h(x[:,i]) h = F.relu(self.h_h(h)) return self.h_o(h) . So, a recurrent neural network is a neural network that&#39;s defined using a loop, hence recurrent. An RNN that isn&#39;t using a loop like RNNish is the unrolled representation of an RNN. . When we train a model with these two architectures, we should have about the same accuracy: . learn = Learner(dls, RNNish(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.794642 | 1.953518 | 0.473497 | 00:02 | . 1 | 1.401166 | 1.721939 | 0.475398 | 00:02 | . 2 | 1.425844 | 1.658773 | 0.492750 | 00:02 | . 3 | 1.379972 | 1.654874 | 0.490373 | 00:02 | . learn = Learner(dls, RNN(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.852343 | 1.971172 | 0.464226 | 00:02 | . 1 | 1.371547 | 1.797724 | 0.475160 | 00:02 | . 2 | 1.413824 | 1.689967 | 0.491324 | 00:02 | . 3 | 1.366402 | 1.645036 | 0.492988 | 00:02 | . And, we get about 49% for each. To see if it&#39;s actually good, we can compare it to if we just predicted the most commonly occurring token each time instead: . n, counts = 0, torch.zeros(len(vocab)) for x, y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y == i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . So, if we just predicted the most commonly occurring token, &quot;thousand&quot;, each time, we would have an accuracy of 15%, so our basic language model with an accuracy of 49% is much better. . You might be wondering, why don&#39;t you just use h += ... instead of h = h + ...? I thought so too, but you get a RuntimeError by PyTorch because you&#39;re using a tensor or its part to compute the tensor or its part; in other words, PyTorch can&#39;t calculate the gradient when you use +=. You can read more on why here. . Improving the simple RNN . Currently, we&#39;re initializing the hidden state h to 0 each time in forward. This effectively makes the model forget what it&#39;s seen before. To fix this, we can initialize h in __init__ and create a reset function to reinitialize h to 0. . But, this creates another problem: as we apply another layer to h, we add another thing on which we have to calculate the derivative during backpropagation. So, we can use PyTorch&#39;s detach method on h, which removes the gradient history of h (technically, it makes h no longer require gradient). . Overall, we made our new RNN stateful since it remembers its activations between different samples in the batch (between different calls to forward): . class RNN2(Module): def __init__(self, n_vocab, n_hidden): self.i_h = nn.Embedding(n_vocab, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden, n_vocab) self.h = 0 def forward(self, x): for i in range(3): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) out = self.h_o(self.h) self.h = self.h.detach() return out def reset(self): self.h = 0 . We can also have any sequence length we want since we&#39;ll have the same activations each time. The only difference is that we only calculate the gradients on sequence length tokens in the past instead of all of them. This approach is called truncated backpropagation through time (truncated BPTT). . BPTT is treating an RNN as one big model (which we did by initializing h to 0 in __init__), and calculating gradients on it the usual way. Truncated BPTT avoids running out of memory and time by &quot;detaching&quot; the history of computation steps in the hidden state every (or few) epochs (which we did by reinitializing h to 0 in reset). . To make our model work, we need it to see the data set in order, such that dset[0] is in the first line of the first batch, dset[1] is in the first line of the second batch, and so on. For the other lines, we can split the data set into chunks of size m = len(dset) // bs, so dset[i + j * m] is in the j+1-th line of the i+1-th batch). This is done automatically in LMDataLoader. . The following function does the reindexing: . def group_chunks(dset, bs): m = len(dset) // bs new_dset = L() for i in range(m): new_dset += L(dset[i + j * m] for j in range(bs)) return new_dset . Then, when we make our DataLoaders, we also need to drop the last batch since it might not be of size bs. We also need to avoid shuffling the data since that would ruin the purpose of our reindexing. . bs = 64 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . Finally, we need to adjust the training loop so that we call reset. We can do this by adding ModelResetter as a Callback (cbs), which calls reset before each epoch and each validation phase. Since we reinitialize the hidden state, we start with a clean state before each batch so we can train for more epochs. . learn = Learner(dls, RNN2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.739974 | 1.857636 | 0.474038 | 00:01 | . 1 | 1.270585 | 1.779141 | 0.451683 | 00:01 | . 2 | 1.106414 | 1.576123 | 0.522356 | 00:01 | . 3 | 1.020932 | 1.581516 | 0.552644 | 00:01 | . 4 | 0.954357 | 1.765170 | 0.551683 | 00:01 | . 5 | 0.894364 | 1.761537 | 0.568510 | 00:01 | . 6 | 0.850844 | 1.735529 | 0.553365 | 00:01 | . 7 | 0.813397 | 1.583861 | 0.581010 | 00:01 | . 8 | 0.766258 | 1.656481 | 0.605529 | 00:01 | . 9 | 0.751839 | 1.691648 | 0.609135 | 00:01 | . Turning it more like a language model . Remember how when we took the movie review data set, we made the independent variable and dependent variable the same token length, but the dependent variable was ahead by one token? By doing so, we get more signal that we can feed back to the model when we update the weights. Why predict the last word of the sequence when you can predict the next word for each word in the sequence, right? . So, we can adjust our seqs to be of sl length for both independent and dependent variables, with them offset by one token: . sl = 16 seqs_lm = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+1+sl])) for i in range(0, len(nums)-1-sl, sl)) [L(vocab[j] for j in seq) for seq in seqs_lm[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . Then we can make our DataLoaders the same way as before: . bs = 64 cut = int(len(seqs_lm) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs_lm[:cut], bs), group_chunks(seqs_lm[cut:], bs), bs=bs, drop_last=True, shuffle=False) . But, we need to change our model so that it predicts after every word and not after the last one: . class RNN3(Module): def __init__(self, n_vocab, n_hidden): self.i_h = nn.Embedding(n_vocab, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden, n_vocab) self.h = 0 def forward(self, x): outs = [] # We changed 3 to sl since we&#39;ll be # predicting the next word sl times for i in range(sl): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) outs.append(self.h_o(self.h)) self.h = self.h.detach() return torch.stack(outs, dim=1) def reset(self): self.h = 0 . And, we&#39;ll have to flatten the inputs and targets before using them in F.cross_entropy. The output of the model has a shape bs $ times$ sl $ times$ n_vocab since we stacked the output onto one dimension (through dim=1). Our targets have shape bs $ times$ sl. So, we can reshape them using torch.view. . def loss_func(input, target): # .view(-1, len(vocab)) means make len(vocab) # columns with as many rows as needed (-1) # # .view(-1) means flatten the entire tensor # into one row that&#39;s as long as it needs to be return F.cross_entropy(input.view(-1, len(vocab)), target.view(-1)) . Finally, we can train our model. We&#39;ll have to use an even larger number of epochs than last time because we have a more complex model: . learn = Learner(dls, RNN3(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.321696 | 3.182977 | 0.186442 | 00:01 | . 1 | 2.411823 | 1.992369 | 0.469482 | 00:01 | . 2 | 1.792959 | 1.769369 | 0.451253 | 00:01 | . 3 | 1.494330 | 1.655577 | 0.501872 | 00:01 | . 4 | 1.304660 | 1.634182 | 0.549967 | 00:01 | . 5 | 1.152728 | 1.675661 | 0.584391 | 00:01 | . 6 | 1.021364 | 1.745027 | 0.623698 | 00:01 | . 7 | 0.910983 | 1.595549 | 0.588135 | 00:01 | . 8 | 0.828727 | 1.685101 | 0.628092 | 00:01 | . 9 | 0.753475 | 1.665891 | 0.615479 | 00:01 | . 10 | 0.709412 | 1.602229 | 0.629069 | 00:01 | . 11 | 0.660277 | 1.695538 | 0.648031 | 00:01 | . 12 | 0.638024 | 1.641410 | 0.637695 | 00:01 | . 13 | 0.617738 | 1.700257 | 0.645426 | 00:01 | . 14 | 0.601591 | 1.764060 | 0.651204 | 00:01 | . We got a better accuracy, but we have an effectively very deep network. So, we can end up with very small or very large gradients that can lead to very different results when we run train the model: . learn = Learner(dls, RNN3(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.209394 | 3.080578 | 0.263021 | 00:01 | . 1 | 2.302940 | 1.904129 | 0.468262 | 00:01 | . 2 | 1.727188 | 1.796725 | 0.468994 | 00:01 | . 3 | 1.426145 | 1.770710 | 0.494548 | 00:01 | . 4 | 1.230578 | 1.784142 | 0.498291 | 00:01 | . 5 | 1.093605 | 1.889311 | 0.513591 | 00:01 | . 6 | 0.975858 | 1.930008 | 0.532389 | 00:01 | . 7 | 0.885223 | 2.018722 | 0.535319 | 00:01 | . 8 | 0.814634 | 2.065933 | 0.538656 | 00:01 | . 9 | 0.757224 | 2.108545 | 0.561686 | 00:01 | . 10 | 0.712159 | 2.211724 | 0.544678 | 00:01 | . 11 | 0.677356 | 2.263988 | 0.575033 | 00:01 | . 12 | 0.650655 | 2.421612 | 0.533610 | 00:01 | . 13 | 0.626848 | 2.389696 | 0.546549 | 00:01 | . 14 | 0.614016 | 2.390650 | 0.552246 | 00:01 | . By training a new model, we got a decrease of nearly 10% in accuracy. One way to fix this would be to try a deeper model: one with more than one linear layer between the hidden state and the output activations. . More layers. MORE LAYERS. . A multilayer RNN is more like a multiRNN model: we pass the activations from one RNN as inputs to another RNN. . This time, instead of creating a for loop, we can use PyTorch&#39;s nn.RNN class, which implements it for us while also letting us choose how many layers we want: . class RNN4(Module): def __init__(self, n_vocab, n_hidden, n_layers): self.i_h = nn.Embedding(n_vocab, n_hidden) # Our inputs are in order of (bs, sl, n_vocab) so we have to # tell PyTorch we want bs first instead of sl first self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, n_vocab) self.h = torch.zeros(n_layers, bs, n_hidden) def forward(self, x): acts, h = self.rnn(self.i_h(x), self.h) self.h = h.detach() return self.h_o(acts) def reset(self): self.h = self.h.zero_() . But, when we train our model, we get a worse accuracy than our previous single-layer RNN: . learn = Learner(dls, RNN4(len(vocab), 64, 2), # CrossEntropyLossFlat() does the # same thing as our loss_func loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.089907 | 2.664662 | 0.444417 | 00:01 | . 1 | 2.179285 | 1.815480 | 0.471354 | 00:01 | . 2 | 1.719844 | 1.864228 | 0.323079 | 00:01 | . 3 | 1.518661 | 1.860099 | 0.433594 | 00:01 | . 4 | 1.405865 | 1.868557 | 0.475911 | 00:01 | . 5 | 1.304332 | 1.898743 | 0.479329 | 00:01 | . 6 | 1.194302 | 2.151057 | 0.470785 | 00:01 | . 7 | 1.086549 | 2.219379 | 0.503988 | 00:01 | . 8 | 0.950983 | 2.215469 | 0.501709 | 00:01 | . 9 | 0.839067 | 2.280946 | 0.510661 | 00:01 | . 10 | 0.756243 | 2.429555 | 0.513021 | 00:01 | . 11 | 0.699213 | 2.520038 | 0.525065 | 00:01 | . 12 | 0.661544 | 2.569467 | 0.520671 | 00:01 | . 13 | 0.639416 | 2.584662 | 0.521973 | 00:01 | . 14 | 0.628166 | 2.575517 | 0.524089 | 00:01 | . Even when we add more layers, we get a worse accuracy than our single-layer RNN: . learn = Learner(dls, RNN4(len(vocab), 64, 5), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.996651 | 2.546187 | 0.407471 | 00:02 | . 1 | 2.138067 | 1.668146 | 0.470866 | 00:01 | . 2 | 1.629905 | 1.733835 | 0.497965 | 00:01 | . 3 | 1.352087 | 1.925911 | 0.537679 | 00:01 | . 4 | 1.158306 | 1.992638 | 0.541992 | 00:01 | . 5 | 0.996187 | 2.063406 | 0.544922 | 00:01 | . 6 | 0.872516 | 2.011257 | 0.568848 | 00:01 | . 7 | 0.745992 | 1.745594 | 0.566569 | 00:01 | . 8 | 0.609336 | 1.654734 | 0.593099 | 00:01 | . 9 | 0.494758 | 1.707728 | 0.603271 | 00:01 | . 10 | 0.406440 | 1.617404 | 0.607747 | 00:01 | . 11 | 0.343562 | 1.717202 | 0.604167 | 00:01 | . 12 | 0.304646 | 1.746353 | 0.599854 | 00:01 | . 13 | 0.281399 | 1.724579 | 0.605794 | 00:01 | . 14 | 0.268294 | 1.720216 | 0.603923 | 00:01 | . The reason is that we now have an even deeper model, which are more likely to lead to exploding or vanishing activations. . In practice, creating accurate models from multilayer RNNs are difficult because we&#39;re applying repeated matrix multiplication many, many times (each layer is another set of matrix multiplications). Multiplying by a number even a little greater than 1 will lead to exploding activations; and multiplying by a number even a little smaller than 1 will lead to vanishing activations. . We also have the problem of floating point numbers. Because of how they&#39;re stored on the computer, the numbers are more accurate the closer they are to 0. This inaccuracy leads to the vanishing gradients or exploding gradients problem, where in SGD, the weights are either not updated at all, or explode to infinity. . For RNNs, there&#39;re two types of layers that are commonly used to avoid exploding activations: gated recurrent units (GRUs) and long short-term memory (LSTM). . Long Short-Term Memory (LSTM) . LSTM introduces another hidden state called the cell state that retains important information that happened earlier in the sentence (e.g., the subject&#39;s gender to predict &quot;he/she/they&quot;), that is, long short-term memory. The other hidden state is then like the sensory short-term memory. . How human memory is theorized to work. Thank you IB psychology. So, LSTM looks like this: . Diagram of LSTM. From left to right on the top, we have the forget gate, the input gate, the cell gate, and the output gate. In essence, the blue box is our forward function, which uses the previous hidden states $h_{t-1}$ and $c_{t-1}$ and accepts an input batch $x_t$. The function updates the hidden states to yield $h_t$ and $c_t$, which become $h_{(t+1)-1}$ and $c_{(t+1)-1}$ for the next time step. . In LSTM, the hidden state $h_{t-1}$ and the input batch $x_t$ are concatenated instead of added like what we&#39;ve been doing so far to create a tensor of size $h_{t-1}+x_t$. So, all the layers have an input size of $h_{t-1}+x_t$ and have an output size of $h_{t-1}$. . LSTM has four layers called gates. There&#39;s two different activation functions being used in LSTM: sigmoid (squishes to 0 to 1) and tanh (squishes to -1 to 1). From left to right: . (1) Forget gate $f_t$: take what you currently know ($h_{t-1}$) and apply that to the input ($x_t$) to forget unimportant things in the cell state $c_{t-1}$. | (2) Input gate $i_t$ and (3) cell gate $g_t$: these two gates work together, so I&#39;ll group them together and call them the remember gate. Basically, take what you currently know ($h_{t-1}$) and apply that to the input ($x_t$) to remember the important stuff from the cell gate $g_t$. Add the output from the remember gate to the cell state. | (4) Output gate $o_t$: take important things from the new cell state that we might need for the next time step $t$. | . The importance mentioned above is what&#39;s learned when we train the model at each time step (i.e. epoch). . The cell state $c_t$ is able to remember stuff much better (maintain a longer-term state) than the hidden state $h_t$ since it doesn&#39;t go through a single layer, hence avoiding vanishing and exploding activations. . In code: . class LSTM(Module): def __init__(self, n_in, n_hid): n_cat = n_in + n_hid self.forget_gate = nn.Linear(n_cat, n_hid) self.input_gate = nn.Linear(n_cat, n_hid) self.cell_gate = nn.Linear(n_cat, n_hid) self.output_gate = nn.Linear(n_cat, n_hid) def forward(self, x, state): h, c = state h = torch.cat([h, x], dim=1) f = torch.sigmoid(self.forget_gate(h)) c = c * f i = torch.sigmoid(self.input_gate(h)) g = torch.tanh(self.cell_gate(h)) c = c + i * g o = torch.sigmoid(self.output_gate(h)) h = o * torch.tanh(c) return h, (h, c) . However, in practice, we refactor the code since it&#39;s inefficient to do four small matrix multiplications when we can do one big multiplication on the GPU in parallel. It&#39;s like typing with a single finger when you were given 10 (unless you&#39;re missing fingers). Also, since it takes time to concatenate the input $x_t$ and the hidden state $h_t$, we have two layers instead: one for the input and one for the hidden state. So: . class LSTM(Module): def __init__(self, n_in, n_hid): self.i_h = nn.Linear(n_in, 4 * n_hid) self.h_h = nn.Linear(n_hid, 4 * n_hid) def forward(self, x, state): h, c = state # .chunk(4, 1) splits the tensor into 4 tensors # along the first dimension gates = (self.i_h(x) + self.h_h(h).chunk(4, 1)) # It doesn&#39;t matter what order the gates are # as long as we keep the order throughout f, i, o = map(torch.sigmoid, gates[:3]) g = gates[3].tanh() c = c * f + i * g h = o * c.tanh() return h, (h, c) . And, our LSTM is essentially what we already have through PyTorch&#39;s nn.LSTM. So, we can recreate our multilayer RNN: . bs = 64 class LSTM(Module): def __init__(self, n_vocab, n_hidden, n_layers): self.i_h = nn.Embedding(n_vocab, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, n_vocab) # We have two hidden states (h, c) that we&#39;ll keep together in state self.state = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): h, state = self.rnn(self.i_h(x), self.state) self.state = [s.detach() for s in state] return self.h_o(h) def reset(self): for s in self.state: s.zero_() . learn = Learner(dls, LSTM(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.056956 | 2.708244 | 0.341553 | 00:01 | . 1 | 2.185629 | 1.798441 | 0.377360 | 00:01 | . 2 | 1.627687 | 1.897092 | 0.459717 | 00:01 | . 3 | 1.285119 | 2.024220 | 0.486979 | 00:01 | . 4 | 1.003079 | 1.560079 | 0.567301 | 00:01 | . 5 | 0.718694 | 1.565812 | 0.619303 | 00:01 | . 6 | 0.469903 | 1.525995 | 0.648519 | 00:01 | . 7 | 0.306789 | 1.473051 | 0.684814 | 00:01 | . 8 | 0.199153 | 1.369004 | 0.728516 | 00:01 | . 9 | 0.122797 | 1.450004 | 0.731364 | 00:01 | . 10 | 0.075972 | 1.254726 | 0.758545 | 00:01 | . 11 | 0.048235 | 1.323799 | 0.759684 | 00:01 | . 12 | 0.033595 | 1.288537 | 0.757080 | 00:01 | . 13 | 0.026236 | 1.295729 | 0.759359 | 00:01 | . 14 | 0.022860 | 1.290811 | 0.757406 | 00:01 | . Although we reduced the chances of vanishing or exploding gradients, now we have a bit of overfitting. Although there aren&#39;t many data augmentation techniques for text (like translating to another language and then back to the original), we can apply regularization techniques like dropout, activation regularization, and temporal activation regularization. . Averaged SGD (ASGD) Weight-Dropped LSTM (AWD-LSTM) . For AWD-LSTM, we need to include 4 (5) things: . Dropout: randomly (through a Bernoulli trial) remove some activations with probability $p$. | Activation regularization: weight decay, but with activations instead of weights. | Temporal activation regularization: activation regularization, but with the difference between two consecutive activations. | Weight tying: tying the hidden to output weights with the input to hidden weights. | (You also use non-monotically triggered average stochastic gradient descent (NT-ASGD) as the optimizer). | Dropout is where you randomly set some of the activations to zero during training to make sure all parameters are being useful in producing the output: . A neural network with 2 hidden layers. (a) Before dropout. (b) After dropout. But, we can&#39;t just zero some activations without doing something else since we won&#39;t have the same scale when we take the sum of 5 activations compared to 2 activations. So, if we have $n$ activations and apply dropout with probability $p$, then we&#39;ll have on average $(1-p)n$ activations left. Finally, we can divide them by $1-p$ to rescale the remaining to $n$, which effectively applies dropout while maintaining the scale as if we still had all activations (making dropout act like an identity function). . The PyTorch implementation of dropout is as follows: . class Dropout(Module): def __init__(self, p): self.p = p def forward(self, x): # Only apply dropout during training if self.training: # Creates a mask with 1s at a probability of (1-p) # and 0s at a probability of p mask = x.new(*x.shape).bernoulli_(1 - self.p) # Divide the mask in place by (1-p) and multiply with x return x * mask.div_(1 - self.p) # Don&#39;t apply dropout during inference else: return x . We apply dropout before passing the outputs of our LSTM layer to the final output layer. . To change the training attribute of a PyTorch Module, you can use the train method to set it to true and the eval method to set it to false. When you call these methods, it sets the training attribute for that Module and recursively applies it to the next Modules. You won&#39;t see it here often since it&#39;s applied automatically by fastai&#39;s Learner class. . Activation regularization (AR) and temporal activation regularization (TAR) are essentially weight decay, but with activations. With weight decay, we add a penalty to the loss (but in practice, we add to the gradient) to make the weights as small as possible to avoid overfitting (by making the loss have less steep points). For AR and TAR, we aim to make the final LSTM activations as small as possible. . With AR, we can do the following to the loss: . loss += alpha * activations.pow(2).mean() . But, we know from weight decay that it&#39;ll be more efficient to add them to the gradient instead of the loss: . grad += alpha * activations.mean() . Then, going straight to the gradient for TAR, we have: . grad += beta * (activation[:,1:] - activations[:,:-1]).mean() . We have two new hyperparameters that we can tune for AR and TAR: alpha and beta like how we could adjust wd for weight decay. To apply AR and TAR, we use the RNNRegularizer callback (although that class adds the square to the loss). . But, to make AR and TAR work, we need our new model to return three things: (1) the actual output, (2) the LSTM activations pre-dropout and (3) the LSTM activations post-dropout. . We apply AR on the post-dropout LSTM activations to not penalize the activations we dropped; and, we apply TAR on the pre-dropout LSTM activations because those dropped activations make a big difference between two consecutive time steps. . Finally, we have weight tying. Weight tying is used in language models because we go from our input vocab to some hidden state, then from the hidden state to our output, which are tokens from the same vocab. So, we can expect that the mappings from input to hidden will be the same for the mapping from hidden to output; that is, the mapping is invertible (or at least, try to enforce it to be invertible). Therefore, we can set the weights of the hidden to output layer to be equal to the weights of the input to hidden layer: . self.h_o.weight = self.i_h.weight . So, we now have our final model: . class AWDLSTM(Module): def __init__(self, n_vocab, n_hidden, n_layers, p): # What we had before in LSTM self.i_h = nn.Embedding(n_vocab, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, n_vocab) self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] # Dropout layer self.drop = nn.Dropout(p) # Weight tying self.h_o.weight = self.i_h.weight def forward(self, x): h, state = self.rnn(self.i_h(x), self.h) h_drop = self.drop(h) self.h = [s.detach() for s in state] return self.h_o(h_drop), h, h_drop def reset(self): for h in self.h: h.zero_() . Then, to train this model, we have: . learn = Learner(dls, AWDLSTM(len(vocab), 64, 2, 0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)]) . But, since we use those callbacks so often, we can instead use TextLearner which applies ModelResetter and RNNRegularizer (with alpha=2, beta=1 as defaults): . learn = TextLearner(dls, AWDLSTM(len(vocab), 64, 2, 0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . Finally, when we train our model, we can also add weight decay for additional regularization: . learn.fit_one_cycle(15, 1e-2, wd=0.1) . epoch train_loss valid_loss accuracy time . 0 | 2.681149 | 2.038707 | 0.457031 | 00:01 | . 1 | 1.733276 | 1.261145 | 0.619141 | 00:01 | . 2 | 1.007044 | 0.856758 | 0.762370 | 00:01 | . 3 | 0.523582 | 0.975427 | 0.793538 | 00:01 | . 4 | 0.276582 | 0.704902 | 0.820557 | 00:01 | . 5 | 0.151957 | 0.593644 | 0.867025 | 00:01 | . 6 | 0.093755 | 0.536299 | 0.866455 | 00:01 | . 7 | 0.061734 | 0.540390 | 0.870524 | 00:01 | . 8 | 0.043401 | 0.519793 | 0.879313 | 00:01 | . 9 | 0.034196 | 0.426708 | 0.882650 | 00:01 | . 10 | 0.027788 | 0.494268 | 0.883626 | 00:01 | . 11 | 0.023066 | 0.439835 | 0.886068 | 00:01 | . 12 | 0.019279 | 0.433105 | 0.880697 | 00:01 | . 13 | 0.017327 | 0.435248 | 0.886393 | 00:01 | . 14 | 0.015784 | 0.425923 | 0.887614 | 00:01 | . And we&#39;ve come a long ways from 49% accuracy with a single layer vanilla RNN. . Conclusion . So, a recurrent neural network is just a neural network that has some layers used repeatedly such that we can put them in a loop. A vanilla RNN is fairly difficult to get a good accuracy, and, when we attempt to do a vanilla multilayer RNN, it becomes even harder to get a good accuracy because of exploding and vanishing gradients. That&#39;s why we now have LSTM (but we can also use GRU, which has only one hidden state that splits during the time step into the hidden and cell states). However, LSTM has an issue of overfitting. So, what do we do when we overfit? We apply data augmentation techniques (since we might not have enough data), but there aren&#39;t many cheap and quick data augmentation techniques for text. Instead, we opt for regularization techniques like dropout, activation regularization, temporal activation regularization, and weight tying. Applying these regularization techniques creates a new kind of architecture that we could call a rudimentary AWD-LSTM. . For an actual AWD-LSTM, we have to apply dropout in a few more places: . Embedding dropout: inside the embeddings, drop some random rows of embeddings. | Input dropout: applied after the embedding layer. | Weight dropout: appled to the weights of LSTM after each epoch. | Hidden dropout: applied to the hidden state between two layers. | . These additional regularizations (and averaged SGD) completes AWD-LSTM, where AWD-LSTM uses 5 different kinds of dropout (the 5th is the one where we drop some activations after LSTM). There are already good defaults set in place in fastai&#39;s implementation of AWD-LSTM that we used in this blog and we were able to adjust the magnitude of the dropouts with the drop_mult parameter. .",
            "url": "https://geon-youn.github.io/DunGeon/nlp/2022/04/20/Recurrent-Neural-Networks.html",
            "relUrl": "/nlp/2022/04/20/Recurrent-Neural-Networks.html",
            "date": " • Apr 20, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "How to use fastai's mid-level API",
            "content": "Transformers! . In the previous blog where I trained a model to predict movie review sentiments, I created my DataLoaders by using a TextBlock. The fastai library is built on a layered API where the top layer has applications; we used the high level API (the DataBlock API). . . At a lower level, TextBlock applies both Tokenize and Numericalize, which tokenize, then numericalize the raw input text. Both classes inherit from the Transform class. . . An instance of a Transform class is an object that has . an encodes method that can be called by (), allowing the object to be used like a function, | an optional setup method to initialize some inner state, and | an optional decode method to reverse the function (which may or may not be fully reversible; the importance is that it makes it easier for humans to read). | . . A Transform can work on a tuple. Transforms usually have their inputs specified with types so only items in the tuple with the correct type have the Transform applied. . In writing your own Transform, you can either decorate a function with @Transform or inherit from Transform, where: . To call To implement . () | encodes | . setup() | setups | . decode() | decodes | . You have to implement them in a different name since Transform does some things before calling setups and decodes in setup() and decode(). . So with a decorator, we have: . @Transform def f(x: int): return x + 1 . When you define a function with the @Transform decorator, you can only define the encodes part. . f((2, 2.0, &#39;2&#39;)), f(2), f((2.0,)), f([2]) . ((3, 2.0, &#39;2&#39;), 3, (2.0,), [2]) . We can apply the Transform to a tuple (and since we defined it for ints it only applies to 2 and not 2.0 or &#39;2&#39;), a single element, a single element tuple, or a list. . If we wanted to implement setup() and decode(), we&#39;ll have to subclass Transform: . class Normalize_(Transform): def setups(self, items, pop=False): self.mean = sum(items) / len(items) self.std = sum([(i - self.mean)**2 for i in items]) / (len(items) - (0 if pop else 1)) def encodes(self, x): return (x - self.mean) / self.std def decodes(self, x): return x * self.std + self.mean . Then, we&#39;ll instantiate it and call setup with our items: . xs = [0, 1, 2, 3, 4, 5, 6] n = Normalize_() n.setup(xs) n.mean, n.std . (3.0, 4.666666666666667) . And finally, we&#39;ll test it: . x = (3, 3 + n.std) x_encoded = n(x) x_decoded = n.decode(x_encoded) x, x_encoded, x_decoded . ((3, 7.666666666666667), (0.0, 1.0), (3.0, 7.666666666666667)) . Transformers assemble! . Typically, you want to use multiple Transforms on your raw items. There&#39;s 3 main ways to do that with fastai: . Pipelines, | TfmdListss, and | Datasetss. | Each have their own uses and differences (and there&#39;s a reason why some of end with an s). . Starting with Pipelines, you pass it a list of instantiated Transforms: . # Add 1 to each item, then normalize them p = Pipeline([f, n]) p((2, 3)) . (0.0, 0.21428571428571427) . If we want to pass in a list of Transform classes or functions, we need to use TfmdLists instead: . # Add 1 to each item, then use those values to setup Normalize # (now the values are centered at 4 instead of 3) tl = TfmdLists(xs, [f, Normalize]) tl((2, 3, 4)) . (-0.21428571428571427, 0.0, 0.21428571428571427) . With TfmdLists, we provide the raw items needed for the setup of each Transform and a list of the Transforms we want to use. At initialization, TfmdLists calls the setup() of each Transform, but it passes the raw items transformed by all previous Transforms in order instead of the raw items. . TfmdLists ends with an s since it can handle splits for training and validation sets. To split the data, you have to specify a split: . tls = TfmdLists(xs, [f, Normalize], splits=RandomSplitter()(xs)) tls.mean, tls.std, tls.train.items, tls.valid.items . (4.5, 3.5, [6, 2, 4, 3, 5, 1], [0]) . You should be careful though because the setup of the Transforms will be done with the raw items in the train set instead of the entire set. . Finally with Datasets, you can think of it as multiple TfmsLists put together in a tuple, where each item produced by a Datasets is (tls1, tls2, ...). In general, we&#39;ll have two parallel pipelines of Transforms: (1) to process raw items into inputs and (2) to process raw items into targets. But, you can also have as many parallel pipelines as you want (for example, if you have multiple inputs and/or multiple targets; that&#39;s why there&#39;s the ... in (tls1, tls2, ...)). . So, for a Datasets, it could look like this: . class better_f(Transform): def encodes(self, x): return x + 1 def decodes(self, x): return x - 1 x_tfms = [better_f, Normalize] y_tfms = [] # a pipeline can also be empty z_tfms = [Identity] # if empty is boring, you can also use Identity dsets = Datasets(xs, [x_tfms, y_tfms, z_tfms], splits=RandomSplitter()(xs)) dsets, dsets.train, dsets.valid . ((#7) [(-0.5357142857142857, 0, 0),(-0.35714285714285715, 1, 1),(-0.17857142857142858, 2, 2),(0.0, 3, 3),(0.17857142857142858, 4, 4),(0.35714285714285715, 5, 5),(0.5357142857142857, 6, 6)], (#6) [(0.35714285714285715, 5, 5),(0.17857142857142858, 4, 4),(-0.35714285714285715, 1, 1),(-0.5357142857142857, 0, 0),(-0.17857142857142858, 2, 2),(0.5357142857142857, 6, 6)], (#1) [(0.0, 3, 3)]) . And since we redefined f as a subclass of Transform with a decode method, we can get our raw items by decoding them: . [dsets.decode(dsets[i]) for i in range(len(xs))] . [(0.0, 0, 0), (1.0, 1, 1), (2.0, 2, 2), (3.0, 3, 3), (4.0, 4, 4), (5.0, 5, 5), (6.0, 6, 6)] . Lastly, we can create DataLoaders from a Dataset using the dataloaders attribute: . dls = dsets.dataloaders(bs=2) dls.train.one_batch(), dls.valid.one_batch() . ((tensor([0.1786, 0.5357], dtype=torch.float64), tensor([4, 6]), tensor([4, 6])), (tensor([0.], dtype=torch.float64), tensor([3]), tensor([3]))) . dataloaders works by calling DataLoader on each subset of our Datasets (like train and valid) and then putting them together into a DataLoaders. . The dataloaders has a few important parameters that are equivalent to the ones we use in DataBlocks: . after_item takes Transforms and applies them on each item after grabbing them from the dataset (equivalent to item_tfms in DataBlock). | before_batch is applied to each item in a batch before they&#39;re collated. | after_batch is applied on the batch after collation (equivalent to batch_tfms in DataBlock). | . When would you want to use before_batch? When you want to apply something to each item in a batch instead of on the entire batch like in after_batch. For example, padding the documents for text so that all the items in the batch are of the same token length. . You can also specify the type of DataLoader you want. In NLP, you might want to use SortedDL through dsets.dataloaders(dl_type=SortedDL) which batches items of roughly the same length by sorting them beforehand. . Finally, when we call show_batch or show_results on a DataLoaders (or show on a TfmdLists or a Datasets), it continues to decode items until it reaches a type that has a show method. If there&#39;s no types with show (like Tensors), we get an error: . dls.show_batch() . AttributeError Traceback (most recent call last) &lt;ipython-input-93-03961d1c4ef6&gt; in &lt;module&gt;() 1 #collapse_output -&gt; 2 dls.show_batch() /usr/local/lib/python3.7/dist-packages/fastai/data/core.py in show_batch(self, b, max_n, ctxs, show, unique, **kwargs) 102 if b is None: b = self.one_batch() 103 if not show: return self._pre_show_batch(b, max_n=max_n) --&gt; 104 show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs) 105 if unique: self.get_idxs = old_get_idxs 106 /usr/local/lib/python3.7/dist-packages/fastcore/dispatch.py in __call__(self, *args, **kwargs) 121 elif self.inst is not None: f = MethodType(f, self.inst) 122 elif self.owner is not None: f = MethodType(f, self.owner) --&gt; 123 return f(*args, **kwargs) 124 125 def __get__(self, inst, owner): /usr/local/lib/python3.7/dist-packages/fastai/data/core.py in show_batch(x, y, samples, ctxs, max_n, **kwargs) 16 else: 17 for i in range_of(samples[0]): &gt; 18 ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))] 19 return ctxs 20 /usr/local/lib/python3.7/dist-packages/fastai/data/core.py in &lt;listcomp&gt;(.0) 16 else: 17 for i in range_of(samples[0]): &gt; 18 ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))] 19 return ctxs 20 AttributeError: &#39;Tensor&#39; object has no attribute &#39;show&#39; . . To fix this error, you&#39;ll have to define (or use) a custom type with a show method that can accept a ctx as a keyword argument (which could be a matplotlib axis for images or a row of a DataFrame for texts). . Then, you need to include a Transform in the pipeline that converts your inputs into that custom type and ideally is the first Transform in the pipeline so that when you call show, it decodes all the way to the raw items. . Conclusion . In this blog, I covered the lower-level parts of the fastai library: Transforms and how to use them through Pipelines, TfmdListss, and Datasetss. In the real world, the higher-level DataBlock API might not be flexible enough. So, you&#39;ll have to use the more flexible lower-level APIs that let you define your own Transforms, data types, and DataLoaders. .",
            "url": "https://geon-youn.github.io/DunGeon/fastai/2022/04/13/Fastai-Mid-Level-API.html",
            "relUrl": "/fastai/2022/04/13/Fastai-Mid-Level-API.html",
            "date": " • Apr 13, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Predicting the Sentiment of Movie Reviews with NLP",
            "content": "Downloading the data set . If you didn&#39;t know, I work on Colab; I don&#39;t own a GPU that I can use to train models on my own. Whenever I download data sets, I end up having to redownload them when I come back on the next day. However, I found out how to download the data sets into my Google drive so that I don&#39;t have to redownload them every time. It&#39;s also helpful since I can save the pickles to the same folder instead of saving them on my laptop and having to upload them later. . It&#39;s really simple too: you mount onto your drive and then cd to your desired folder. When you download data sets, you download them into that folder. But, if you use untar_data, you have to move the files to the folder since it only saves to ~/.fastai/.... . !mkdir -p &#39;/content/gdrive/MyDrive/data_sets/&#39; %cd &#39;/content/gdrive/MyDrive/data_sets/&#39; . There&#39;s also a difference between ! and % in Colab: ! is like a &quot;no side-effect&quot; way to do command-line commands. !mkdir ... will make a directory, and !cd ... will change directories, except it doesn&#39;t, because that would be a side effect. So, to actually change the current directory that Colab is in, you have to use %. You can think of % as being able to change the variables stored in Colab; hypothetically %cd path makes Colab&#39;s CURRENT_PATH = path. . from fastai.text.all import * . path = untar_data(URLs.IMDB_SAMPLE) . !mv {str(path)} &#39;/content/gdrive/MyDrive/data_sets/imdb_sample&#39; . path = Path(&#39;/content/gdrive/MyDrive/data_sets/imdb_sample&#39;) . Preparing the data for the language model . Since we&#39;re using the sample IMDB data set, we don&#39;t have separate folders, but instead have a single .csv file with 1000 reviews. So, we&#39;ll be using TextBlock.from_df instead of TextBlock.from_text_files to tell fastai how to get our data: . df = pd.read_csv(path/&#39;texts.csv&#39;, low_memory=False); df.head(3) . label text is_valid . 0 negative | Un-bleeping-believable! Meg Ryan doesn&#39;t even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff! | False | . 1 positive | This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som... | False | . 2 negative | Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li... | False | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . dls_lm = DataBlock( blocks = TextBlock.from_df([&#39;text&#39;], is_lm=True), get_items = ColReader(&#39;text&#39;), splitter = RandomSplitter(0.1)).dataloaders(df, path=path) . When we show a batch, you can see how the independent variable contains one more token before the dependent variable, but has one less token at the end. Fastai also has special tokens that it makes through TextBlock (which uses the Tokenizer class): . xxbos: beginning of stream. | xxeos: end of stream. | xxmaj: next token is capitalized. | xxunk: unknown (we ignore tokens that don&#39;t appear enough to minimize the size of the embedding matrix). | xxup: next token is all uppercase. | xxrep followed by a number n: the next actual token repeats n times. | . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos xxmaj even though the book was n&#39;t strictly accurate to the real situation it described it still carried a sense of xxmaj japan . i find it hard to believe that anyone who was involved in making this film had ever been to japan as it did n&#39;t feel xxmaj japanese in the slightest . xxmaj almost everything about it was terrible . i will admit the actors were generally quite | xxmaj even though the book was n&#39;t strictly accurate to the real situation it described it still carried a sense of xxmaj japan . i find it hard to believe that anyone who was involved in making this film had ever been to japan as it did n&#39;t feel xxmaj japanese in the slightest . xxmaj almost everything about it was terrible . i will admit the actors were generally quite good | . 1 ; &quot; the xxmaj return &quot; ( xxunk xxmaj xxunk , 2003 ) ; &quot; little xxmaj xxunk &quot; ( james xxmaj xxunk ) ; xxmaj xxunk , &quot; xxunk and xxmaj sons &quot; ; and , of course , xxmaj xxunk , &quot; the xxmaj brothers xxmaj xxunk . &quot; n n xxmaj credits in xxmaj english indicate intended international xxunk , meaning that the excuse can not be used that you | &quot; the xxmaj return &quot; ( xxunk xxmaj xxunk , 2003 ) ; &quot; little xxmaj xxunk &quot; ( james xxmaj xxunk ) ; xxmaj xxunk , &quot; xxunk and xxmaj sons &quot; ; and , of course , xxmaj xxunk , &quot; the xxmaj brothers xxmaj xxunk . &quot; n n xxmaj credits in xxmaj english indicate intended international xxunk , meaning that the excuse can not be used that you have | . . In general, Tokenizer does a lot of extra things on top of tokenizing the texts: . defaults.text_proc_rules . [&lt;function fastai.text.core.fix_html&gt;, &lt;function fastai.text.core.replace_rep&gt;, &lt;function fastai.text.core.replace_wrep&gt;, &lt;function fastai.text.core.spec_add_spaces&gt;, &lt;function fastai.text.core.rm_useless_spaces&gt;, &lt;function fastai.text.core.replace_all_caps&gt;, &lt;function fastai.text.core.replace_maj&gt;, &lt;function fastai.text.core.lowercase&gt;] . . Where . fix_html: replace special HTML characters with a readable version. | replace_rep: replace characters repeated three or more times with xxrep n c where xxrep is the special token, n is the number of times it repeats, and c is the character. | replace_wrep: replace words repeated three or more times with xxwrep n w where w is the word. | spec_add_spaces: add spaces around special characters like / and # so they get split into separate tokens. | rm_useless_spaces: remove repeated spaces. | replace_all_caps: lowercase a word in all caps and add a xxup before it. | replace_maj: lowercase a capitalized word and add a xxmaj before it. | lowercase: lowercase all words and add a xxbox to the beginning and/or xxeos to the end of the string. | . And here&#39;s most of it in action: . tok = WordTokenizer() tkn = Tokenizer(tok) list(tkn(&#39;amp; &amp;copy; #a&quot; &lt;br /&gt; www.google.ca/INDEX.html wow wow wow&#39;)) . [&#39;xxbos&#39;, &#39;&amp;&#39;, &#39;©&#39;, &#39;#&#39;, &#39;a&#39;, &#39;&#34;&#39;, &#39; n &#39;, &#39;xxrep&#39;, &#39;3&#39;, &#39;w&#39;, &#39;.google.ca&#39;, &#39;/&#39;, &#39;index.html&#39;, &#39;xxwrep&#39;, &#39;3&#39;, &#39;wow&#39;] . . Fine-tuning the language model . We&#39;ll be fine-tuning a language model pretrained on Wikepedia pages. This model uses a recurrent neural network using the AWD-LSTM architecture. . learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]) . Since pretrained is set to True for our language_model_learner by default, all the layers except the head are frozen. So, we&#39;ll first train the head of the model for one epoch: . lr = learn.lr_find().valley . learn.fit_one_cycle(1, lr) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.127933 | 3.944165 | 0.272763 | 51.633183 | 00:57 | . Then, we&#39;ll unfreeze all the layers and train all the layers for another epoch: . lr = learn.lr_find().valley . learn.unfreeze() learn.fit_one_cycle(1, lr) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.904036 | 3.806237 | 0.281909 | 44.980846 | 01:04 | . We&#39;ll save the model as an encoder, which is the body of the model (the model not including the task-specific final layer(s)): . learn.save_encoder(&#39;lm&#39;) . If we were to stop here, we&#39;d have a text generator (which is what a language model is). We provide a piece of text and how many words we want it to predict: . learn.predict(&quot;I liked the book better&quot;, 25, temperature=0.75) . &#34;i liked the book better , i had more movie time , so i was surprised , but i do n&#39;t like it . While i essentially liked this&#34; . Fine-tuning the language model for a classifier . Unlike language models, which are self-supervised, we have to provide our DataLoaders for our classifier with the labels: . dls_c = DataBlock( blocks = (TextBlock.from_df(&#39;text&#39;, vocab=dls_lm.vocab), CategoryBlock), get_x = ColReader(&#39;text&#39;), get_y = ColReader(&#39;label&#39;), splitter = ColSplitter() ).dataloaders(df, path=path) . This time, we have to specify our blocks differently so that our independent variable is a TextBlock and our dependent variable is a CategoryBlock. So, when show a part of a batch, it&#39;ll look different from a language model&#39;s DataLoaders: . dls_c.show_batch(max_n=2) . text category . 0 xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review n n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it &#39;s warm and xxunk , but you &#39;re not sure if it feels right . xxmaj try as i might , no matter how warm and xxunk xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n&#39;t quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director &#39;s part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n&#39;t fool me . xxmaj raising xxmaj victor xxmaj vargas is | negative | . 1 xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there &#39;s just no getting around that , and it &#39;s hard to actually put one &#39;s feeling for this film into words . xxmaj it &#39;s not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot … which is highly believable to xxunk . xxmaj it &#39;s easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n&#39;t often get from other romantic comedies | positive | . Next, we&#39;ll define a Learner by using the text_classifier_learner class: . learn = text_classifier_learner(dls_c, AWD_LSTM, drop_mult=0.5, metrics=accuracy) . And load the encoder: . learn = learn.load_encoder(&#39;lm&#39;) . When fine-tuning a language model for a text classifier, it&#39;s recommended to use discriminative learning rates and gradual unfreezing. So, we&#39;ll first fine-tune it for 1 epoch: . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.587637 | 0.625740 | 0.655000 | 00:16 | . Then unfreeze the last 2 layers and begin using discriminative learning rates: . den = 2.6**4 learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/den, 1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.445948 | 0.522101 | 0.715000 | 00:19 | . Then unfreeze up to the last 3 layers: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/den, 5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.306446 | 0.478002 | 0.780000 | 00:28 | . Next unfreeze the entire model: . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/den, 1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.208012 | 0.417342 | 0.795000 | 00:36 | . 1 | 0.193315 | 0.410684 | 0.805000 | 00:36 | . And finally train for a few more epochs: . learn.fit_one_cycle(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.162276 | 0.398424 | 0.820000 | 00:36 | . 1 | 0.146796 | 0.461882 | 0.790000 | 00:36 | . 2 | 0.114655 | 0.511482 | 0.835000 | 00:36 | . 3 | 0.092193 | 0.437872 | 0.825000 | 00:35 | . 4 | 0.074616 | 0.423975 | 0.835000 | 00:36 | . And we&#39;re done! We can try predicting the sentiment of a few Morbius reviews: . [learn.predict(i) for i in [ &quot;So many choices... just seem halfhearted. They&#39;re not terrible, but they&#39;re also not interesting.&quot;, &quot;Doesn&#39;t deliver a lot of high notes, but it&#39;s not unwatchable (especially if you enjoy the vampire genre). &quot;Morbius &quot; is not as great as you&#39;d hoped, but not as bad as you feared.&quot;, &quot;Overall, this is an entertaining movie. It sucks you in with the action and standout scenes between some of the characters. But, while Morbius has the makings of a great anti-hero revival, it wavers with the execution.&quot;, &quot;Like the Venom films and unlike the MCU movies Morbius eschews the mythmaking of the Avengers for darkly comic horror.&quot;, &quot;Sadly, director Daniel Espinosa&#39;s action horror is itself drained of atmosphere, shocks and drama.&quot; ]] . [(&#39;negative&#39;, TensorText(0), TensorText([0.9590, 0.0410])), (&#39;negative&#39;, TensorText(0), TensorText([0.8723, 0.1277])), (&#39;positive&#39;, TensorText(1), TensorText([0.0055, 0.9945])), (&#39;positive&#39;, TensorText(1), TensorText([0.0279, 0.9721])), (&#39;negative&#39;, TensorText(0), TensorText([0.5976, 0.4024]))] . Conclusion . In this blog, I applied the ULMFiT approach to NLP. I fine-tuned the pretrained language model that was trained on Wikepedia pages with the smaller version of the IMDB data set to create a language model that can generate text relevant to movie reviews. Then, I used the encoder from that model to train a movie review sentiment classifier with an accuracy of 83.5%. For a model that was only trained with 1000 reviews, it&#39;s not that bad! . For next steps, I&#39;d like to train a model using the full IMDB data set, but I don&#39;t have the necessary hardware to do it in a reasonable time. When I tried training a Learner on the full data set, the estimated time for the first epoch was two and a half hours. Maybe in the future when I pay for a GPU server or own my own setup. .",
            "url": "https://geon-youn.github.io/DunGeon/nlp/2022/04/08/Movie-Review-Sentiment.html",
            "relUrl": "/nlp/2022/04/08/Movie-Review-Sentiment.html",
            "date": " • Apr 8, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "So you wanna learn natural language processing",
            "content": "How to train your NLP model (ULMFit) . With NLP (natural language processing), we can use transfer learning to train an NLP model using a pretrained language model. By using a pretrained language model, you spend less time, data, and money. But, unlike computer vision, we don&#39;t need a model that was trained on similar data. . A language model is a model that was trained to predict the next word given a sequence of text. We train these models through self-supervised learning, where we don&#39;t give any labels, just a lot of text. The model can automatically (thus self-supervised) create the labels from the text and be trained on it to develop an understanding of the text&#39;s language. . So, you have a pretrained language model, but it isn&#39;t the best idea to train an NLP model right away. The language model probably doesn&#39;t know the vocabulary and style (like grammar, formality, etc.) of your problem domain. So, you first fine-tune the model on the corpus of your problem domain, then fine-tune that new model to train your NLP model. . With this method, we can fine-tune the language model on both the text from the training and validation (and maybe test) sets which will make the new language model very good at predicting the next word for your problem domain. . This process of having a language model, fine-tuning it on your corpus, and then fine-tuning that for an NLP model is called the Universal Language Model Fine-tuning (ULMFit) approach. . Training your NLP (classifier) model . In training an NLP model, we first have to fine-tune our language model. To do so, we need to preprocess the text such that it&#39;s ready to be put into a model. . Preprocessing the text . Text is a categorical variable. And, to put it inside of a neural network, we have to assign them an embedding matrix. This process is the first layer of the neural network: turning a cateogrical variable continuous through an embedding matrix assignment. . With regular categorical variables, we: . Make a list of all possible levels of that categorical variable (which we call the vocab). | Replace each level with its index in the vocab. | Make an embedding matrix for this categorical variable where each row corresponds to a level. | Use this embedding matrix as the first layer of a neural network. This layer takes the index $i$ created in step 2 and returns the $i$-th row in the matrix. | For text, the first step is a little different since there&#39;re many ways we can define levels for text. It also works differently for different languages. This process is called tokenization, where each item in the vocab is called a token. The second step, where we assign numbers to each token is called numericalization. . When we use a pretrained model, our new vocab will contain words that were in the old model, but also some that weren&#39;t. We&#39;ll keep the corresponding row in the embedding matrix for words that exist in the pretrained model and initialize a random vector in rows corresponding to new words. . In tokenizing a given corpus, there are three main methods: . Words: Split a sentence at every space and puntuation (like apostrophes) to create words and contractions. | Subwords: Split words into subwords based on the most commonly occurring substrings. | Characters: Split a sentence into characters. | When do we use which? Word tokenizers assume that spaces are special separators in a sentence. While this is usually correct for English, other languages like Chinese and Japanese that don&#39;t really have spaces are better off with subword and character tokenizers. And, when spaces are special, but the languages uses many subwords like in Turkish and Hungarian, it would be better to use subword tokenizers than word tokenizers. Lastly, when a language has many characters (unlike 26 in English) like Chinese, it may be better to use character tokenizers. . You want to be careful to not have too many items in your vocab. For subword, you have the positive that there&#39;ll be fewer tokens in each sentence, and thus have faster training, less memory, and less state for the model to remember. But in general, a larger vocab leads to larger embedding matrices, which require more data to learn, take longer and require more GPU memory to train. . Once we have our vocab, we can convert every token in the corpus into a number that represents its index in the vocab. . Then, we have to make our independent and dependent variables for our DataSet object (which is just a wrapper class for a tuple (independent, dependent)). . For a language model, we want it to be able to predict the next word in a sequence of words. So, given a sequence of words, we want our independent variable to be from the first word of the sequence to the second last word. Then, our dependent variable will be from the second word of the sequence to the last word. . We&#39;d also be dividing the text into small pieces while maintaining order (otherwise our model would just predict random words instead of the next word in the sequence). . At every epoch, we shuffle our collection of documents and concatenate them into a stream of tokens. Then, we cut that stream into a batch of fixed-size consecutive mini-streams. The model then reads the mini-streams in order and learns to predict the next word for each independent variable. . Unlike with images, the key thing in NLP is that we randomize the documents (blocks of text) but we always have to maintain order of the words in each document. . Fine-tuning the language model . When we&#39;re fine-tuning the pretrained language model, we use a recurrent neural network (unlike convolutional neural network for vision) and use the AWD-LSTM architecture. For our loss function, we use cross-entropy loss since (almost all) NLP problems are classification problems where the different categories are the words in the vocab. Finally, for our metrics we&#39;ll use accuracy (since cross-entropy is difficult to interpret and speaks more for the confidence of our model than its accuracy) and perplexity (which is the exponent of the loss). . If we don&#39;t want to train a text classifier and instead just wanted a language model, we stop here and end up with a text generator. If you add some randomness (so you don&#39;t get the same prediction twice), you can generate many different kinds of text given the first few words. . Otherwise, you&#39;ll use this fine-tuned language model to train a text classifier. . Fine-tuning the text classifier . Unlike with a language model, when making our DataLoaders, our independent variable will be the text, while our dependent variable must be supplied. And, when trying to make a mini-batch, the tensors have to be the same shape. So, we sort the text by token length before each epoch and for every mini-batch, pad every text to be the same token length as the text with the largest token length in the mini-batch. By &quot;pad&quot;, we add a special padding token that&#39;ll be ignored by the model. . Then, we fine-tune our fine-tuned language model by training it with discriminative learning rates and gradual unfreezing. . In the end, you have a language model that can generate text related to your problem domain and a text classifier that can classify text in your problem domain with certain labels. . Conclusion . With NLP, there&#39;s a lot of fine-tuning. By ULMFit, you start with a pretrained language model that could have been trained on a really big data set like Wikipedia, you fine-tune it on your own text to have a language model that can generate text really well for your problem domain, then you fine-tune that language model to train a text classifier. .",
            "url": "https://geon-youn.github.io/DunGeon/nlp/2022/04/05/Natural-Language-Processing.html",
            "relUrl": "/nlp/2022/04/05/Natural-Language-Processing.html",
            "date": " • Apr 5, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Just The Basics - Strata 2013 | predicting spam emails",
            "content": "Hello! I wanted to move onto natrual language processing by next week, but I also wanted some more practice with tabular data sets. And, what better way than to use a data set that&#39;s related to natural language: emails. . This Kaggle competition ran 9 years ago and is one of the &quot;getting started&quot; data sets. This data set contains &quot;100 features extracted from a corpus of emails. Some of the emails are spam and some are normal. [Our] task is to make a spam detector.&quot; . I&#39;ve already downloaded the data set and we can first import them: . xs = pd.read_csv(&#39;train.csv&#39;, low_memory=False) y = pd.read_csv(&#39;train_labels.csv&#39;, low_memory=False) test = pd.read_csv(&#39;test.csv&#39;, low_memory=False) . Then, we&#39;ll define functions for our random forest trainer and the metric that this competition requires, which is the area under the ROC curve. . def rf(xs, y, n_estimators=40, min_samples_leaf=5, max_samples=300, max_features=0.5): return RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_samples=max_samples, max_features=max_features, oob_score=True, n_jobs=-1).fit(xs, y) . def a_uc(preds, y): fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=1) return metrics.auc(fpr, tpr) def m_auc(m, xs, y): return a_uc(m.predict(xs), y) . In making our TabularPandas, we&#39;ll merge the independent and dependent variables. But, unlike before we&#39;ll be using a randomized split since this isn&#39;t a time series. . df_merged = pd.concat((xs, y), axis=1).copy() . procs = [Categorify, FillMissing, Normalize] . cont, cat = cont_cat_split(df_merged, dep_var=&#39;0&#39;) . tp = TabularPandas(df_merged, procs, cat_names=cat, cont_names=cont, y_names=&#39;0&#39;, splits=RandomSplitter()(xs)) . t_xs, v_xs, t_y, v_y = tp.train.xs, tp.valid.xs, tp.train.y, tp.valid.y . I&#39;m not sure how to use TabularPandas that well since there&#39;s no categorical columns in this data set, but it seems like TabularPandas requires it. If there aren&#39;t any, then it duplicates all the columns, so we remove them here: . t_xs = t_xs.drop(tp.train.x_names[0:100], axis=1) . v_xs = v_xs.drop(tp.valid.x_names[0:100], axis=1) . Then, we train a decision tree as a baseline: . dt = DecisionTreeRegressor(min_samples_leaf=40).fit(t_xs, t_y) . m_auc(dt, t_xs, t_y), m_auc(dt, v_xs, v_y) . (0.9117870857001292, 0.9124478856462179) . Surprisingly, the baseline is already at 0.912. I&#39;m also not sure if this metric is equivalent to accuracy in terms of saying it&#39;s 91.2% accurate, so I&#39;ll just refer to it as 0.912. . Next, let&#39;s train a random forest model: . m = rf(t_xs, t_y, min_samples_leaf=10, n_estimators=120) . m_auc(m, t_xs, t_y), m_auc(m, v_xs, v_y) . (0.9765178460830635, 0.9401429422275164) . And then, we&#39;ll train a neural network: . dls = tp.dataloaders(128) . Again, we&#39;re dropping the duplicated columns from TabularPandas: . dls.train.xs = dls.train.xs.drop(columns=dls.train.x_names[0:100]) dls.valid.xs = dls.valid.xs.drop(columns=dls.valid.x_names[0:100]) . learn = tabular_learner(dls, y_range=(0, 1.1), n_out=1) . lr = learn.lr_find().valley . learn.fit_one_cycle(20, lr) . epoch train_loss valid_loss time . 0 | 0.289919 | 0.264292 | 00:00 | . 1 | 0.280569 | 0.260043 | 00:00 | . 2 | 0.264891 | 0.248970 | 00:00 | . 3 | 0.243126 | 0.230470 | 00:00 | . 4 | 0.218251 | 0.207044 | 00:00 | . 5 | 0.195376 | 0.184551 | 00:00 | . 6 | 0.175897 | 0.165676 | 00:00 | . 7 | 0.158557 | 0.149466 | 00:00 | . 8 | 0.143941 | 0.136603 | 00:00 | . 9 | 0.131354 | 0.127658 | 00:00 | . 10 | 0.119663 | 0.120048 | 00:00 | . 11 | 0.110008 | 0.114571 | 00:00 | . 12 | 0.101210 | 0.111422 | 00:00 | . 13 | 0.093147 | 0.109775 | 00:00 | . 14 | 0.086211 | 0.108900 | 00:00 | . 15 | 0.080164 | 0.107172 | 00:00 | . 16 | 0.074857 | 0.106758 | 00:00 | . 17 | 0.069664 | 0.106035 | 00:00 | . 18 | 0.065428 | 0.105261 | 00:00 | . 19 | 0.061637 | 0.104235 | 00:00 | . . preds, targs = learn.get_preds() . a_uc(preds, targs) . 0.9252531268612271 . Finally, we ensemble the predictions from our baseline (since it&#39;s not far off from the neural network&#39;s ROC AUC), random forest model, and neural network: . rf_preds = m.predict(v_xs) ens_preds = (to_np(preds.squeeze()) + rf_preds + dt.predict(v_xs)) / 3 . a_uc(ens_preds, v_y) . 0.953543776057177 . From a fairly quick model with no really &quot;complex&quot; components added (that I would probably learn in the second part of fast.ai), we&#39;re able to come 36th in the leaderboards (which is top 75%). . Anyway, this post is just a quick recap on tabular model training using a relatively small data set. We didn&#39;t need to preprocess the data since TabularPandas handles the missing values for us. We trained a baseline, then trained a random forest and a neural network model, then ensembled the predictions. Perhaps I could have analyzed the columns and remove some of the unimportant features. Unlike last time, there aren&#39;t any categorical columns so I didn&#39;t use the embeddings to train another random forest model. . Maybe in the future I&#39;ll revisit this data set and aim to get a higher score. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/04/01/Predicting-Spam.html",
            "relUrl": "/tabular/2022/04/01/Predicting-Spam.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Using auction data to predict sale price",
            "content": "To practice the theory from my previous post, I&#39;m going to be training a random forest model on a data set used by a previous Kaggle competition: &quot;Blue Book for Bulldozers&quot;. . The contest took place 9 years ago and was sponsored by Fast Iron. Here is its description: . The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it&#39;s usage, equipment type, and configuaration. The data is sourced from auction result postings and includes information on usage and equipment configurations. . Fast Iron is creating a &quot;blue book for bull dozers,&quot; for customers to value what their heavy equipment fleet is worth at auction. . Preprocessing the data set . First, we download the data set using the Kaggle API (to see how I did this, open the blog in Colab). Post download, we have these files: . !ls . bluebook-for-bulldozers.zip Test.csv Train.zip &#39;Data Dictionary.xlsx&#39; tp.pkl Valid.7z kaggle.json Train.7z Valid.csv Machine_Appendix.csv TrainAndValid.7z ValidSolution.csv median_benchmark.csv TrainAndValid.csv Valid.zip random_forest_benchmark_test.csv TrainAndValid.zip . Then, we can import the full data set into a Pandas DataFrame object: . # When low_memory = True (default), Pandas only looks at a # few rows to see what type the data in a specific column # is. If we want to minimize errors and don&#39;t run into a # low memory issue, we should set low_memory = False. df = pd.read_csv(&#39;TrainAndValid.csv&#39;, low_memory = False) . This data set contains 53 different features: . df.columns, len(df.columns) . (Index([&#39;SalesID&#39;, &#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;), 53) . According to the &quot;Data&quot; tab in the Kaggle competition page, the key features are in the data set are: . SalesID: the uniue identifier of the sale. | MachineID: the unique identifier of a machine. A machine can be sold multiple times. | SalePrice: what the machine sold for at auction (only provided in train.csv). | saledate: the date of the sale. | . After we read the data, we should handle the orderable categorical variables (called ordinal). We can do so by explicitly setting an order and telling Pandas that we want this column to be ordered by this new rule. The only feature for which this applies is the ProductSize column: . df[&#39;ProductSize&#39;].unique() . array([nan, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;], dtype=object) . Currently, it&#39;s in a random order, but we can give it a specific order: . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered = True) . df[&#39;ProductSize&#39;].unique() . [NaN, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;] Categories (6, object): [&#39;Large&#39; &lt; &#39;Large / Medium&#39; &lt; &#39;Medium&#39; &lt; &#39;Small&#39; &lt; &#39;Mini&#39; &lt; &#39;Compact&#39;] . Next, we have to use the correct metric for the competition. For regression problems, we usually use RMSE, but the Kaggle competition usually states which metric to use. For this competition, we use RMSLE (root mean squared log error, which is the same as RMSE, but the predictions are log&#39;d before RMSE). So, as a little preprocessing, we log the SalePrice column (and we can just use RMSE later and handle smaller prediction values while training): . df[&#39;SalePrice&#39;] = np.log(df[&#39;SalePrice&#39;]) . RMSLE is better to use than RMSE when you want to get a more accurate prediction. RMSLE is also larger for when you underestimate vs. when you overestimate. To learn more about RMSLE, read this blog post. . We should also handle dates. Currently, the dates are just... the dates. . df[&#39;saledate&#39;].head() . 0 11/16/2006 0:00 1 3/26/2004 0:00 2 2/26/2004 0:00 3 5/19/2011 0:00 4 7/23/2009 0:00 Name: saledate, dtype: object . Dates are pretty special since some dates are more important than others. There&#39;s holidays, Fridays, Mondays, weekends, end of quarter, end of year, etc. So, we use fastai&#39;s add_datepart function that splits the date column into metadata columns. . df = add_datepart(df, &#39;saledate&#39;) . Evidently, fastai has generated many metadata columns and we now have 13 date related features: . L([i for i in df if i.startswith(&#39;sale&#39;)]), len(df.columns) . ((#13) [&#39;saleYear&#39;,&#39;saleMonth&#39;,&#39;saleWeek&#39;,&#39;saleDay&#39;,&#39;saleDayofweek&#39;,&#39;saleDayofyear&#39;,&#39;saleIs_month_end&#39;,&#39;saleIs_month_start&#39;,&#39;saleIs_quarter_end&#39;,&#39;saleIs_quarter_start&#39;...], 65) . Next, we need to transform our data such that it has no strings or missing values. To do so, we use fastai&#39;s TabularPandas by passing in the DataFrame, TabularProcs, which features are categorical and which are continuous, what the dependent variable is, and how we&#39;re splitting the data. . A TabularProc is a special kind of Transform that performs the transform when it&#39;s called instead of lazily as it&#39;s accessed, and returns the exact same object after modifying it in place. . Categorify sets a number to each level in categorical columns, effectively making them continuous, with each assigned number having no meaning (the model learns the meanings on its own). If we defined an order beforehand like we did with ProductSize, then it takes that order instead of assigning a random ordering. . FillMissing takes missing values in continuous columns and assigns the average value of that continuous column. . procs = [Categorify, FillMissing] . We can use fastai&#39;s cont_cat_split that returns a tuple: (continuous labels, categorical labels). When we define dep_var = &#39;SalePrice&#39;, we tell cont_cat_split to ignore that column. The 1 is for the cardinality. Any column with cardinality greater than 1 will be assigned as continuous. . cont, cat = cont_cat_split(df, 1, dep_var = &#39;SalePrice&#39;) L(cont), L(cat) . ((#14) [&#39;SalesID&#39;,&#39;MachineID&#39;,&#39;ModelID&#39;,&#39;datasource&#39;,&#39;auctioneerID&#39;,&#39;YearMade&#39;,&#39;MachineHoursCurrentMeter&#39;,&#39;saleYear&#39;,&#39;saleMonth&#39;,&#39;saleWeek&#39;...], (#50) [&#39;UsageBand&#39;,&#39;fiModelDesc&#39;,&#39;fiBaseModel&#39;,&#39;fiSecondaryDesc&#39;,&#39;fiModelSeries&#39;,&#39;fiModelDescriptor&#39;,&#39;ProductSize&#39;,&#39;fiProductClassDesc&#39;,&#39;state&#39;,&#39;ProductGroup&#39;...]) . To split our data as training and validation, we want to construct it so that our model can generalize to unseen data. Since this is a time series data set, we want our training set to occur before our validation set. So, we can split it according to time (TrainAndValid.csv goes up to April 2012, with Test.csv being six months after May 2012. We&#39;ll split our data such that the training is six months before April 2012: before November 2011): . # Forms a copy of our current DataFrame, but has # a True where the condition is true. The pipe # &#39;|&#39; acts as &#39;union&#39; for the sets. So, our # condition is that it is before 2011 or November cond = (df[&#39;saleYear&#39;] &lt; 2011) | (df[&#39;saleMonth&#39;] &lt; 11) # The first element of the tuple produced by # np.where contains the indices, i, where cond[i] # was True train_idx = np.where( cond)[0] # Applying ~ to cond turns all True values to # False and False to True valid_idx = np.where(~cond)[0] splits = (list(train_idx), list(valid_idx)) . Now, we can put all of them inside of a TabularPandas: . tp = TabularPandas(df, procs, cat, cont, y_names = &#39;SalePrice&#39;, splits = splits) . When we display the data, they don&#39;t seem changed: . tp.show(3) . UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start auctioneerID_na MachineHoursCurrentMeter_na SalesID MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleElapsed SalePrice . 0 Low | 521D | 521 | D | #na# | #na# | #na# | Wheel Loader - 110.0 to 120.0 Horsepower | Alabama | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | False | False | 1139246 | 999089 | 3157 | 121 | 3.0 | 2004 | 68.0 | 2006 | 11 | 46 | 16 | 3 | 320 | 1.163635e+09 | 11.097410 | . 1 Low | 950FII | 950 | F | II | #na# | Medium | Wheel Loader - 150.0 to 175.0 Horsepower | North Carolina | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | 23.5 | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | False | False | 1139248 | 117657 | 77 | 121 | 3.0 | 1996 | 4640.0 | 2004 | 3 | 13 | 26 | 4 | 86 | 1.080259e+09 | 10.950807 | . 2 High | 226 | 226 | #na# | #na# | #na# | #na# | Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity | New York | SSL | Skid Steer Loaders | #na# | OROPS | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Auxiliary | #na# | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | None or Unspecified | Standard | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | False | False | False | False | False | False | False | False | 1139249 | 434808 | 7009 | 121 | 3.0 | 2001 | 2838.0 | 2004 | 2 | 9 | 26 | 3 | 57 | 1.077754e+09 | 9.210340 | . But when we display them as items (what our model&#39;s going to see), all the categorical variables are changed to continuous variables: . tp.items.head(3) . SalesID SalePrice MachineID ModelID ... saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na . 0 1139246 | 11.097410 | 999089 | 3157 | ... | 1 | 1.163635e+09 | 1 | 1 | . 1 1139248 | 10.950807 | 117657 | 77 | ... | 1 | 1.080259e+09 | 1 | 1 | . 2 1139249 | 9.210340 | 434808 | 7009 | ... | 1 | 1.077754e+09 | 1 | 1 | . 3 rows × 67 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; And, like how we defined an order for ProductSize, the order is maintained: . tp.classes[&#39;ProductSize&#39;] . [&#39;#na#&#39;, &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39;] . But for a feature whose order we didn&#39;t define, they&#39;re ordered alphabetically: . tp.classes[&#39;state&#39;] . [&#39;#na#&#39;, &#39;Alabama&#39;, &#39;Alaska&#39;, &#39;Arizona&#39;, &#39;Arkansas&#39;, &#39;California&#39;, &#39;Colorado&#39;, &#39;Connecticut&#39;, &#39;Delaware&#39;, &#39;Florida&#39;, &#39;Georgia&#39;, &#39;Hawaii&#39;, &#39;Idaho&#39;, &#39;Illinois&#39;, &#39;Indiana&#39;, &#39;Iowa&#39;, &#39;Kansas&#39;, &#39;Kentucky&#39;, &#39;Louisiana&#39;, &#39;Maine&#39;, &#39;Maryland&#39;, &#39;Massachusetts&#39;, &#39;Michigan&#39;, &#39;Minnesota&#39;, &#39;Mississippi&#39;, &#39;Missouri&#39;, &#39;Montana&#39;, &#39;Nebraska&#39;, &#39;Nevada&#39;, &#39;New Hampshire&#39;, &#39;New Jersey&#39;, &#39;New Mexico&#39;, &#39;New York&#39;, &#39;North Carolina&#39;, &#39;North Dakota&#39;, &#39;Ohio&#39;, &#39;Oklahoma&#39;, &#39;Oregon&#39;, &#39;Pennsylvania&#39;, &#39;Puerto Rico&#39;, &#39;Rhode Island&#39;, &#39;South Carolina&#39;, &#39;South Dakota&#39;, &#39;Tennessee&#39;, &#39;Texas&#39;, &#39;Unspecified&#39;, &#39;Utah&#39;, &#39;Vermont&#39;, &#39;Virginia&#39;, &#39;Washington&#39;, &#39;Washington DC&#39;, &#39;West Virginia&#39;, &#39;Wisconsin&#39;, &#39;Wyoming&#39;] . Now we processed our data and we can start training a random forest model! . Modeling . First, we&#39;ll get the split data from our TabularPandas object: . train_xs, train_y = tp.train.xs, tp.train.y valid_xs, valid_y = tp.valid.xs, tp.valid.y . Then, we&#39;ll make a decision tree model as a baseline . dt = DecisionTreeRegressor(min_samples_leaf = 25) dt.fit(train_xs, train_y) . DecisionTreeRegressor(min_samples_leaf=25) . To see how well our model did, we can take the RMSE of our predictions since we took the log beforehand (so our models predict the log of the sale price). We&#39;ll define the functions so that we don&#39;t have to repeat it later: . def r_mse(preds, y): return round(math.sqrt(((preds - y)**2).mean()), 6) def m_rmse(m, preds, y): return r_mse(m.predict(preds), y) . m_rmse(dt, train_xs, train_y), m_rmse(dt, valid_xs, valid_y) . (0.211429, 0.266451) . So, we should be trying to train a model that has a better RMSE than 0.266. . We&#39;ll train our random forest model as an ensemble of 40 decision trees: . def rf(xs, y, n_estimators = 40, max_samples = 200_000, max_features = 0.5, min_samples_leaf = 5, **kwargs): return RandomForestRegressor( n_jobs = -1, # use all CPU cores n_estimators = n_estimators, # number of decision trees max_samples = max_samples, # max number of rows to get max_features = max_features, # how many columns to get (%) min_samples_leaf = min_samples_leaf, # leaf nodes must have at least this many (to prevent overfitting) oob_score = True # track out-of-box error score ).fit(xs, y) . m = rf(train_xs, train_y) . m_rmse(m, train_xs, train_y), m_rmse(m, valid_xs, valid_y) . (0.171233, 0.23292) . And, our out-of-bag error is: . r_mse(m.oob_prediction_, train_y) . 0.210877 . Since it&#39;s smaller than our validation error, our model shouldn&#39;t be overfitting and is instead having other problems. . Interpretation . First, we&#39;ll look at the confidence of each tree: . preds = np.stack([t.predict(valid_xs.values) for t in m.estimators_]) preds.shape # we should have 40 trees . (40, 5754) . preds_std = preds.std(0) preds_std.sort() preds_std . array([0.04779576, 0.05120998, 0.0513565 , ..., 0.5755972 , 0.57621563, 0.57734788]) . The standard deviations of the predictions for each auction ranges from 0.00478 to 0.57735 since the trees for some auctions agree (low standard deviation), while other times they disagree (high standard deviation). In production, you could warn the user to be more wary of the prediction if the standard deviation is above a certain threshold. . To see which features were the most important, we can use the .feature_imporances_ attribute of our RandomForestRegressor model to get each columns&#39; feature importance score. We can pair it up with the column names in a DataFrame and sort it to see which features were the most important. . def feature_importance(df, m): return pd.DataFrame({&#39;Feature&#39;: df.columns, &#39;Importance&#39;: m.feature_importances_}).sort_values(&#39;Importance&#39;, ascending = False) . fi = feature_importance(train_xs, m) . fi[:10] . Feature Importance . 57 YearMade | 0.184860 | . 6 ProductSize | 0.114785 | . 30 Coupler_System | 0.114240 | . 7 fiProductClassDesc | 0.078662 | . 54 ModelID | 0.058189 | . 65 saleElapsed | 0.050378 | . 3 fiSecondaryDesc | 0.038825 | . 12 Enclosure | 0.038811 | . 32 Hydraulics_Flow | 0.036170 | . 1 fiModelDesc | 0.032337 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We&#39;ll try removing the unimportant features and see if it affects the model&#39;s performance. First, we set a threshold (0.005) and only take the columns whose importance is greater than that threshold: . imp = fi[fi.Importance &gt; 0.005].Feature len(imp) . 21 . Then, we make new training and validation sets: . train_xs_imp = train_xs[imp] valid_xs_imp = valid_xs[imp] train_xs_imp . YearMade ProductSize Coupler_System fiProductClassDesc ... Drive_System MachineID Hydraulics Tire_Size . 0 2004 | 0 | 0 | 59 | ... | 0 | 999089 | 1 | 17 | . 1 1996 | 3 | 0 | 62 | ... | 0 | 117657 | 1 | 12 | . 2 2001 | 0 | 1 | 39 | ... | 0 | 434808 | 4 | 0 | . 3 2001 | 4 | 0 | 8 | ... | 0 | 1026470 | 1 | 0 | . 4 2007 | 0 | 1 | 40 | ... | 0 | 1057373 | 4 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 412693 2005 | 5 | 0 | 13 | ... | 0 | 1919201 | 12 | 0 | . 412694 2005 | 5 | 0 | 17 | ... | 0 | 1882122 | 4 | 0 | . 412695 2005 | 5 | 0 | 13 | ... | 0 | 1944213 | 4 | 0 | . 412696 2006 | 5 | 0 | 13 | ... | 0 | 1794518 | 4 | 0 | . 412697 2006 | 5 | 0 | 17 | ... | 0 | 1944743 | 4 | 0 | . 406944 rows × 21 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Finally, we train a new model using the new training set: . m_imp = rf(train_xs_imp, train_y) . m_rmse(m_imp, train_xs_imp, train_y), m_rmse(m_imp, valid_xs_imp, valid_y) . (0.181282, 0.233111) . Although our training metric become a bit worse, our validation metric isn&#39;t far off, so we can train future models using this new training set since we can effectively remove 2/3 of the &quot;unncessary&quot; columns. . Next, we&#39;ll remove the redundant columns. We&#39;ll use cluster_columns which determines feature similarity through rank correlation. . cluster_columns(train_xs_imp) . We&#39;ll see how removing each redundant feature affects the model&#39;s capability by defining a function that trains a quick random forest model and returns its out-of-box error: . def get_oob(df): m = RandomForestRegressor( n_estimators = 40, min_samples_leaf = 15, # higher to have a shorter depth tree max_samples = 50_000, max_features = 0.5, n_jobs = -1, oob_score = True).fit(df, train_y) return m.oob_score_ . The oob_score_, unlike oob_prediction_, returns $R^2$, so a perfect model has a score of 1.0 while a random models has 0.0. As a baseline, we&#39;ll first find the oob_score_ of our current training set: . get_oob(train_xs_imp) . 0.8771422789129072 . Then, we&#39;ll try removing each possibly redundant feature one by one: . {c: get_oob(train_xs_imp.drop(labels = c, axis = &#39;columns&#39;)) for c in (&#39;saleYear&#39;, &#39;saleElapsed&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Coupler_System&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiModelDesc&#39;)} . {&#39;Coupler_System&#39;: 0.8777286429358461, &#39;Grouser_Tracks&#39;: 0.8781106723829636, &#39;Hydraulics_Flow&#39;: 0.8778805337963219, &#39;ProductGroup&#39;: 0.87719310108094, &#39;ProductGroupDesc&#39;: 0.877706764282738, &#39;fiBaseModel&#39;: 0.8755285591209638, &#39;fiModelDesc&#39;: 0.8760491649430575, &#39;saleElapsed&#39;: 0.8726949857217733, &#39;saleYear&#39;: 0.8762485106840037} . Since the out-of-box score didn&#39;t change much, we&#39;ll try leaving just one of them in each group: . to_drop = [&#39;saleYear&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;ProductGroup&#39;, &#39;fiBaseModel&#39;] get_oob(train_xs_imp.drop(to_drop, axis = 1)) . 0.8754873568883906 . The score ultimately only goes down by 0.002 from our baseline, so we&#39;ll drop these labels from our training and validation set. . train_xs_fin = train_xs_imp.drop(to_drop, axis = 1) valid_xs_fin = valid_xs_imp.drop(to_drop, axis = 1) . While we&#39;re at it, we&#39;ll also adjust the years so that the minimum year isn&#39;t at 1000 but instead at 1950 . train_xs_fin.loc[train_xs_fin.YearMade &lt; 1950, &#39;YearMade&#39;] = 1950 valid_xs_fin.loc[valid_xs_fin.YearMade &lt; 1950, &#39;YearMade&#39;] = 1950 . Now, we&#39;ll check that our RMSE didn&#39;t decrease significantly: . m = rf(train_xs_fin, train_y) m_rmse(m, train_xs_fin, train_y), m_rmse(m, valid_xs_fin, valid_y) . (0.182645, 0.234452) . The validation RMSE didn&#39;t decrease much and now we only have 16 columns instead of 66. . len(train_xs_fin.columns) . 16 . With most of the unnecessary features removed, we can revisit feature importance and look at the partial dependence plots of some of the most important features and how our predictions depend on the features: . from sklearn.inspection import PartialDependenceDisplay fig, ax = plt.subplots(figsize = (18, 4)) PartialDependenceDisplay.from_estimator(m, train_xs_fin, [&#39;YearMade&#39;, &#39;Coupler_System&#39;, &#39;ProductSize&#39;], grid_resolution = 20, ax = ax) . &lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7fa97cbd3150&gt; . From these plots, it appears that price has an exponential relationship with year, which makes sense (since products tend to depreciate exponentially over time). But for the other two, the missing data seems to be quite important: . tp.classes[&#39;Coupler_System&#39;], tp.classes[&#39;ProductSize&#39;] . ([&#39;#na#&#39;, &#39;None or Unspecified&#39;, &#39;Yes&#39;], [&#39;#na#&#39;, &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39;]) . Coupler systems, although it&#39;s the second most important feature, makes its most important decision on whether it was filled out or not... and with product sizes, although it makes sense that the price decreases as the size decreases, no size mentioned also plays a significant role. . If we were the ones who created the data set, it would be interesting to see why that might be the case. Maybe only data entered after a certain date contained these values, or maybe these columns aren&#39;t as important. . Next, we can look at how the features influence the prediction. To do so, we can use the treeinterpreter library to see how the features influence the prediction, and then use the waterfallcharts library to draw it. . For the treeinterpreter, we pass the model and the rows of data for which we want predictions. Then, it returns a tuple of three items: . prediction is the predicted value for the dependent variable. | bias is the mean of the dependent variable. | contributions is a list of contributions made by each feature. | . So, prediction = bias - contributions.sum(). And, we&#39;ll plot the contributions using a waterfall chart. . nrows = 1 row = 0 prediction, bias, contributions = treeinterpreter.predict(m, valid_xs_fin[:nrows].values) prediction[row], bias[row], contributions[row].sum() . (array([10.64886316]), 10.104243648162724, 0.5446195099995459) . waterfall(valid_xs_fin.columns, contributions[row], rotation_value = 90, formatting = &#39;{:,.2f}&#39;, net_label = &#39;Total&#39;) . &lt;module &#39;matplotlib.pyplot&#39; from &#39;/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py&#39;&gt; . So, it seems that YearMade, ModelID, fiSecondaryDesc, and fiModelDesc were the most important features for this row&#39;s prediction. . But, it&#39;s a bit weird that ModelID is an important feature. What would happen when we have a new system for ModelID in the future? So, we should try to find out-of-domain data by combining our training and validation sets and train a model to predict whether a row is from the training or validation set. . df_domain = pd.concat([train_xs_fin, valid_xs_fin]) is_valid = np.array([0] * len(train_xs_fin) + [1] * len(valid_xs_fin)) m = rf(df_domain, is_valid) . feature_importance(df_domain, m)[:6] . Feature Importance . 5 saleElapsed | 0.919558 | . 9 SalesID | 0.061309 | . 13 MachineID | 0.014026 | . 0 YearMade | 0.000824 | . 4 ModelID | 0.000767 | . 11 fiModelDescriptor | 0.000733 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; So, the main things that appear to change from the training and validation set are saleElapsed, SalesID,and MachineID. Like before, let&#39;s try removing each of them and see how our model changes. . m = rf(train_xs_fin, train_y) m_rmse(m, valid_xs_fin, valid_y) . 0.23369 . {c: m_rmse(rf(train_xs_fin.drop(c, axis = 1), train_y), valid_xs_fin.drop(c, axis = 1), valid_y) for c in [&#39;saleElapsed&#39;, &#39;SalesID&#39;, &#39;MachineID&#39;, &#39;YearMade&#39;, &#39;ModelID&#39;]} . {&#39;MachineID&#39;: 0.233734, &#39;ModelID&#39;: 0.238536, &#39;SalesID&#39;: 0.231391, &#39;YearMade&#39;: 0.268748, &#39;saleElapsed&#39;: 0.237849} . So, it appears that we can remove MachineID and SalesID without worry (the model actually improved when we removed SalesID). . to_drop = [&#39;MachineID&#39;, &#39;SalesID&#39;] xs_new = train_xs_fin.drop(to_drop, axis = 1) valid_xs_new = valid_xs_fin.drop(to_drop, axis = 1) . And we&#39;ll double check our model&#39;s RMSE didn&#39;t go up significantly: . m = rf(xs_new, train_y) m_rmse(m, valid_xs_new, valid_y) . 0.230939 . Not only did it not go up, it actually went down. So, removing MachineID and SalesID, which probably correlated with YearMade and saleElapsed, made the model better. . For the next blog, I&#39;ll train a deep learning model using the information gained through training and analyzing a random forest model. Then, I&#39;ll use the embeddings learned by the neural network to retrain a random forest model and try to beat the #1 gold-medal-level model on the leaderboards. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/31/Blue-Book.html",
            "relUrl": "/tabular/2022/03/31/Blue-Book.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Adding deep learning to tabular models",
            "content": "Two blogs on the same day! I didn&#39;t want the last one to be too long and I didn&#39;t think I would be this productive to be able to finish the neural network part today. But, here we go. . . Previously, I trained a random forest model for predicting sale price on past auction data for bulldozers. Today, I&#39;m going to be training a deep learning model on the same data set (with some changes based on the last blog). Then, I&#39;ll try using the embeddings learned from that model and train another random forest model and aim for an even better validation RMSE. Finally, I&#39;ll ensemble the results of the deep learning model and the new random forest model and see how much better the predictions become. . So, we&#39;ll first read our data into a pandas DataFrame, . df = pd.read_csv(&#39;TrainAndValid.csv&#39;, low_memory=False) . Then, we apply the initial transforms we did last time: . Set an order for the product sizes. | Log the sale price. | Split the date column into metacolumns. | . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True) df[&#39;SalePrice&#39;] = np.log(df[&#39;SalePrice&#39;]) df = add_datepart(df, &#39;saledate&#39;) . And we remove all the unneeded columns we determined last time: . to_keep_df = load_pickle(&#39;xs_new.pkl&#39;) . to_keep = list(to_keep_df) + [&#39;SalePrice&#39;] . df = df[to_keep] df.head(3) . YearMade ProductSize Coupler_System fiProductClassDesc ... Drive_System Hydraulics Tire_Size SalePrice . 0 2004 | NaN | NaN | Wheel Loader - 110.0 to 120.0 Horsepower | ... | NaN | 2 Valve | None or Unspecified | 11.097410 | . 1 1996 | Medium | NaN | Wheel Loader - 150.0 to 175.0 Horsepower | ... | NaN | 2 Valve | 23.5 | 10.950807 | . 2 2001 | NaN | None or Unspecified | Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity | ... | NaN | Auxiliary | NaN | 9.210340 | . 3 rows × 15 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Next, we have to know which columns should be treated as categorical so that they can be given embeddings. For that, we&#39;ll use cont_cat_split, but we set the max cardinality as 9000 (embedding sizes greater than 10,000 should be used after you tested if there&#39;s better ways to group the variable). . cont, cat = cont_cat_split(df, max_card=9_000, dep_var=&#39;SalePrice&#39;) . Mainly, we want to ensure saleElapsed is in the continuous section since by definition, if a label is in the categorical section, it cannot be extrapolated. . cont . [&#39;saleElapsed&#39;] . Next, let&#39;s see the cardinality of each of the categorical variables (if there&#39;s some that are similar in number, they may be redundant and we can try to remove all but one): . df[cat].nunique() . YearMade 73 ProductSize 6 Coupler_System 2 fiProductClassDesc 74 ModelID 5281 fiSecondaryDesc 177 Enclosure 6 fiModelDesc 5059 ProductGroupDesc 6 fiModelDescriptor 140 Drive_System 4 Hydraulics 12 Tire_Size 17 dtype: int64 . It appears that ModelID and fiModelDesc may be redundant since they&#39;re both pertaining to Model and are of similar cardinality. So, we&#39;ll do what we did before and try removing fiModelDesc and see what happens to the results. . get_oob(xs_new) . 0.8760441852100622 . {c: get_oob(xs_new.drop(c, axis=1)) for c in [&#39;ModelID&#39;, &#39;fiModelDesc&#39;, &#39;fiModelDescriptor&#39;]} . {&#39;ModelID&#39;: 0.8717292042012881, &#39;fiModelDesc&#39;: 0.869256405033346, &#39;fiModelDescriptor&#39;: 0.8748457618690582} . Overall, it seems we can remove fiModelDescriptor without it significantly affecting the model. . cat.remove(&#39;fiModelDescriptor&#39;) . To create DataLoaders for our model, we can use TabularPandas again. However, we have to add the Normalize TabularProc for a neural network since the scale of the variables matter unlike in building a decision tree. . tp = TabularPandas(df, [Categorify, FillMissing, Normalize], cat, cont, splits=splits, y_names=&#39;SalePrice&#39;) . # a vision model since we&#39;re dealing with # dabular data (doesn&#39;t require as much GPU RAM) dls = tp.dataloaders(1024) . Next, we&#39;ll see what range we should have for our predictions. . y, v_y = tp.train.y, tp.valid.y y.min(),y.max(),v_y.min(),v_y.max() . (8.465899467468262, 11.863582611083984, 8.465899467468262, 11.849397659301758) . So, we can set our y_range as (8, 12) (remember that a model tends to do better when we have our upper bound a little higher than the actual maximum). . learn = tabular_learner(dls, layers=[500, 250], y_range=(8, 12), n_out=1, loss_func=F.mse_loss) . As always, we&#39;ll use .lr_find() to find the optimal learning rate: . lr = learn.lr_find().valley . And, we&#39;ll use fit_one_cycle to train our model since we&#39;re not transfer learning: . learn.fit_one_cycle(5, lr) . epoch train_loss valid_loss time . 0 | 0.068500 | 0.075998 | 00:07 | . 1 | 0.052604 | 0.063197 | 00:07 | . 2 | 0.047209 | 0.059763 | 00:07 | . 3 | 0.043221 | 0.061487 | 00:07 | . 4 | 0.039401 | 0.053084 | 00:07 | . preds, targs = learn.get_preds() r_mse(preds, targs) . 0.230399 . Overall, we got a better RMSE than the random forest model, but not by a lot. . Now, we&#39;ll try using the embeddings from the neural network to replace the categorical columns for our random forest: . # code is essentially verbatim from fast.ai&#39;s @danielwbn: # https://forums.fast.ai/t/using-embedding-from-the-neural-network-in-random-forests/80063/10 def transfer_embeds(learn, xs): xs = xs.copy() for i, feature in enumerate(learn.dls.cat_names): emb = learn.embeds[i].cpu() # added .cpu() to learn since tensor below is made on cpu while learn is on cuda new_feat = pd.DataFrame(emb(tensor(xs[feature], dtype=torch.int64)), index=xs.index, columns=[f&#39;{feature}_{j}&#39; for j in range(emb.embedding_dim)]) xs = xs.drop(feature, axis=1) xs = xs.join(new_feat) return xs . xs_with_embs = transfer_embeds(learn, learn.dls.train.xs) valid_xs_with_embs = transfer_embeds(learn, learn.dls.valid.xs) . m = rf(xs_with_embs, train_y) . m_rmse(m, xs_with_embs, train_y), m_rmse(m, valid_xs_with_embs, valid_y) . (0.184022, 0.236021) . The validation RMSE ends up becoming worse! But, take a look at this: . We&#39;ll look at our RMSE when we ensemble the predictions from random forests (with embeddings) and the neural network. However, before we can take the average of the predictions from random forests and the neural network, we have to make them of the same type. To do so, we have to turn our neural network predictions (a rank-2 tensor) into a rank-1 numpy array. We can apply .squeeze() to remove any unit axis (a vector with only 1 thing in it) . rf_preds = m.predict(valid_xs_with_embs) ens_preds = (to_np(preds.squeeze()) + rf_preds) / 2 . r_mse(ens_preds, valid_y) . 0.228773 . And, we now have an RMSE that&#39;s lower than the top leaderboard score of 0.22909, granted we&#39;re doing it on the validation set (since we don&#39;t have access to the SalePrice of Test.csv): . a = pd.read_csv(&#39;Test.csv&#39;, low_memory=False) a.columns, &#39;SalePrice&#39; in a.columns . (Index([&#39;SalesID&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;), False) . So, we&#39;ve finally covered tabular data training with decision trees, random forests, neural networks, and random forests with embeddings. We&#39;ve ensembled the results of the last 2 to get a score that (&quot;technically&quot;) beats the top leaderboard score of the Kaggle competition. . Next, we&#39;ll be moving onto natural language models. But, I might also try experimenting with another tabular data set. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/31/Blue-Book-NN.html",
            "relUrl": "/tabular/2022/03/31/Blue-Book-NN.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "You get a decision tree! And YOU get a decision tree!",
            "content": "Our first method for training structured tabular data is to use ensembles of decision trees. . . Decision trees: a decision tree asks a series of yes/no questions about the data. After each question, the data at that part splits between yes/no. After one or more questions, predictions can be formed by finding the group the data is part of at the bottom of the tree and returning the average value of the targets in that group. . . To train a decision tree, we follow a greedy approach with six steps: . Loop through each column of the data set. | For each column, loop through each possible level of that column. | . Level: for most continuous and some categorical variables, when we say levels, we&#39;re referring to variables that can be ordered. For example, sizes like &quot;Small&quot; &lt; &quot;Medium&quot; &lt; &quot;Large&quot;. For other categorical variables, we refer to the actual values. . . Try splitting the data into two groups, based on whether they&#39;re greater than or less than that value (or equal to or not equal to for other categorical variables). | Find the average prediction for each of those two groups and use your metric to see how close that is to the actual value of each of the items in that group. | After looping through all the possible columns and levels for each column, pick the split point that gave the best prediction. | Now, we have two groups for our data set. Treat each of them as new data sets and repeat from step 1 until each group reaches your minimum size threshold. | With decision trees, you have to be careful with how many leaf nodes you end up with. If you have too many (close to the number of data entries), then your model will overfit. . Overfitting? No problem. . One year before his retirement, Leo Breiman published a paper on &quot;bagging&quot;. Instead of training on the entire training set (or mini-batches), you . randomly choose a subset of the rows of your data, | train a model using this subset, | save the model, and | train more models on different subsets of the data. | Eventually, you end up with a number of models. To make a prediction, you predict using all of the models and take the average. . Each of the models have errors since they&#39;re not trained on the full training set, but since different models have different errors (and these errors aren&#39;t correlated with each other; i.e., they&#39;re independent) the errors end up cancelling out when we take the average. . Seven years later, Breiman also coined &quot;random forests&quot; where you apply bagging to decision trees not only by randomly choosing a subset of the rows of your data, but you also randomly choosing a subset of the columns when choosing a split in each decision tree. . . Random forests: a specific type of an ensemble of decision trees, where bagging is used to combine the results of several decision trees that were trained on random subsets of the rows of the data where each split made on a random subset of the columns of the data. . . Since the errors tend to cancel out, it also means the trees are less susceptible to hyperparameter changes. We can also have as many trees as we want; in fact, the error rate usually decreases as we add more trees. . Interpreting the model . Once we trained our model, if the error rate for the validation set is higher than the training set, we want to make sure it&#39;s from generalization (or extrapolation) problems and not overfitting. . Out-of-bag error allows us to check if we&#39;re overfitting without the need of a validation set. Since each tree in a random forest is trained on a subset of the data, we can form a validation set for each tree as the rows not included in training for that tree. . What makes out-of-bag error different from validation set error is that the data in the former is within the range of the training set, while the validation set is usually outside of the range; this range is most important for time series data since the validation set should contain data that&#39;s in the future compared to the training set. . So, if our out-of-bag error is lower than the validation set error, then the model is not overfitting and is instead having other problems. . In general, we want to interpret in our model: . how confident are we in our predictions for a particular row of data? | for making our predictions on a specific row of data, what were the most important columns, and how did they influence the prediction? | which columns are the most important; and which columns can we ignore (remove them from training)? | which columns are effectively redundant in terms of prediction? | how do predictions vary as we vary the columns (as in, what kind of relationship do the columns have with the predictions)? | . Confidence for a prediction on a particular row of data . When we want to predict for a particular row of data, we pass the data to each tree in our random forest and take the average of the results. To find the relative confidence of the prediction, we can take the standard deviation of the predictions instead of the average. So, if the standard deviation is high, we should be more wary of the prediction since the trees disagree more than if the standard deviation was low. . Feature importance . It&#39;s important to understand how our models are making predictions, not just how accuracte the predictions are. . To find the importance of each column (feature) in our data, we can loop through each tree and recursively explore each branch. At each branch, look at what column was used for that split and how much the model improved at that split. The improvement, which is weighted by the number of rows in that group is added to the importance score for that column. The importance score is summed across all branches of all trees. Then, you can normalize the scores (so that they sum to 1) and sort them in ascending order to see the least important columns, and by descending order to see the most important columns. . The &quot;how&quot; is mostly used in production (and not in model training) to see how the data is leading to predictions. To find how each column influenced the prediction, we take a single row of the data and pass it through each of the decision trees in our random forest. At each split point, record how the prediction changes (increases or decreases) compared to the parent node of the tree and add it to the column&#39;s score. Then, combine the score for each of the columns and you can see how each column increased or decreased the prediction relative to the parent node of the random forest (which is the average of the average of the target in each row in the batch of rows in the batch of trees in the random forest). . Ignoring features . Once you found the importance of each column, you can set a threshold such that you ignore features whose importance scores were lower than that threshold (this is why we normalized the scores). . Try retraining your model with those columns ignored and you can decide to keep the change (if the accuracy hasn&#39;t changed much) or change your threshold (if the accuracy decreased significantly). In any case, it&#39;s nicer to train your model with less unimportant columns since you&#39;ll be able to train future models on the same data set faster. . Redundant features . To find redundant columns, you want to find how similar each column is to another. To do so, you calculate the rank correlation, where all the values in each column are replaced with their rank relative to other values in the same column (think of it like descending argsort, where you give each row in a specific column the index it would have for the column to be sorted in descending order).Then, the correlation is calculated (kind of like the correlation coefficient $r$, but with rank). Columns with similar rank correlations may be synonyms for each other and one (or more) of them could be removed. . When removing redundant columns, retrain the model where you remove only one redundant column at a time. Then, try removing them in groups and eventually altogether. The point of this tedious task is to make sure we&#39;re not significantly reducing the accuracy of the model. And, some columns, although they seem redundant, may not be redundant and would be important to keep in the model. . Although not necessary, you should remove unimportant and redundant columns when possible since it&#39;ll simplify your model. . Relationship between columns and predictions . To find the relationship between a column and prediction, you could guess that we should have a row where we keep all columns constant except for the column in question. . But, we can&#39;t just take the average of the predictions for a specific level of a column since other variables can change. Instead, we replace every single value in the column with a specific level in the validation set, and record the prediction with the new validation set as the input. Then, we do the same for every other level of that column. . With these predictions, we can form a line graph with the levels as the x-axis and the predictions as the y-axis. We call this graph a partial dependence plot. . Sometimes, you trained your model and . your accuraccy is too good to be true, | some features don&#39;t make sense to be predictors, or | the partial dependence plots looks weird. | . If so, your data might have data leakage where the training set contains information that wouldn&#39;t be available in the data you give at inference (i.e., when using the model in practice and/or your validation set). . Data leakage are subtleties that give away the correct answer. For example, if you trained a model to predict the weather and the precipitation was in an available column (and/or it was only filled out on rainy days), you bet your model would predict it was &quot;raining&quot; on &quot;rainy days&quot; if there was any precipitation and &quot;sunny&quot; on &quot;sunny days&quot; otherwise. So, when you interpret the model later, you might see really high accuracy, with precipitation being a high predictor. . In preventing data leakage, train your model first and then look for data leakage (and then clean or reprocess your data); this process is the same with how you would train your model first before performing data cleaning. . We can&#39;t always use random forests . With time series data, you usually want to have a model that can generalize to new data and extrapolate accurately. The downside of random forests is that it can only predict within the range of its training data. So, if the value in the validation set is outside of the range of the training set, the accuracy of the random forest will always be low since it can&#39;t predict values that high. . Why might this be the case? A random forest returns a prediction based on the average of the predictions of its decision trees, where each tree predicts the average of the targets in the rows in a leaf node. So, a random forest can never predict a value that&#39;s outside of the range of the training set. . In a general sense, a random forest can&#39;t generalize to out-of-domain data, so we need to make sure our validation, test, and future data sets contain the same kind of data as our training set. . To test if there&#39;s out-of-(the training set&#39;s)-domain data, we can build a random forest that predicts which row is in the validation or training set. To do so, you can concatenate the validation and training set and label the rows by validation or training. Then, through feature importance, if there&#39;s a particular column that is more prominent in the validation set, there will be a nonuniform distribution of importance scores. . Sometimes, you can remove the columns with high feature importance and improve the accuracy of the model since those columns might be related to another column (hence removing redundant columns). . Removing those columns can also make your model more resilient over time since those columns may be affected by domain shift where the data put into the model is significantly different from the training data. . Boosting instead of bagging . Instead of random forests, which forms an ensemble of decision trees through bagging, we can also make gradient boosted machines which uses boosting instead of bagging. . Bagging takes the average of the predictions from each decision tree. Boosting, on the other hand, adds the predictions of each decision tree. So, you also train your decision trees differently: . train a decision tree that underfits the targets of your training set, | calculate residuals by subtracting the predictions from the targets, | repeat from the beginning, but train your future models with the residuals as the targets, and | continue training more trees until you reach a certain maximum or your validation metric gets worse. | . With boosting, we try to minimize the error by having the residuals become as small as possible by underfitting them. . Unlike random forests, the trees aren&#39;t independent of each other so the more trees we train, the more the overall model will overfit the training set. . Free accuracy boost . In training a model for tabular data, you can get a boost in accuracy by training a random forest model, doing some analysis like feature importance and partial dependence plots to remove redundant columns, and then training a neural network that uses embeddings for the categorical variables/columns. . Then, we retrain our random forest model, but instead of creating levels for the categorical variables, we use the embeddings trained by the neural network. So, instead of using a neural network at inference, you can use an improved random forest model. . The same can be done for gradient boosted machines, and any model that uses categorical variables. Just use the embeddings trained by the neural network. . Conclusion . We covered a machine learning technique called ensembles of decision trees. Here, we mentioned two methods of ensembling: bagging and boosting. . With bagging, you form a random forest that&#39;s quick and easy to train. Random forests are also resistant to hyperparameter changes and since the trees are independent, it&#39;s very difficult to overfit as you increase the number of trees. . With boosting, you form a gradient boosted machine (or gradient boosted decision tree) that are just as fast to train as random forests in theory, but require more hyperparameter tuning and are susceptible to overfitting with the more trees you train since the trees aren&#39;t independent of each other. However, gradient boosted machines tend to have higher accuracy than random forests. . Overall, because of the limitations of decision trees, both random forests and gradient boosted machines can&#39;t extrapolate to out-of-domain data. Therefore, you sometimes have to make a neural network. . Neural networks take the longest to train and require more preprocessing like batch normalization (which also needs to be done at inference). With neural networks, you have to be careful with your hyperparameters since they can lead to overfitting. However, neural networks are great at extrapolating and can have the highest accuracy of the three models. . With neural networks, you can also use ensembles of decision trees to do some of the preprocessing to make them faster to train. And, once you train a neural network, you can use the embeddings trained by the neural networks as the inputs for the categorical variables in another ensemble of decision trees on the same data set. Doing so tends to produce much higher accuracy. . If the task doesn&#39;t require extrpolation (all future predictions are expected to be in the same range as the training set), then you can use the improved ensemble of decision trees since they will be faster at inference compared to neural networks. . Moreover, if the response time at inference isn&#39;t a major problem, you can even form an ensemble of neural networks and an ensemble of decision trees where you take the average of the predictions of each of the models. Taking the theory behind random forests, since the two (or more) models were trained by two (or more) very different algorithms, the errors each make are independent of each other and will cancel each other out, leading to higher accuracy with less chances of overfitting. Still, it won&#39;t make a bad model a good model. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/23/Decision-Trees.html",
            "relUrl": "/tabular/2022/03/23/Decision-Trees.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Training models on tabular data",
            "content": "When we train a model on tabular data, we want to create a model that, given values in some columns, can predict the value in another column. In my collaborative filtering blog, I gave the model users&#39; reviews of other movies as inputs and wanted a prediction of the users&#39; review of another movie. . Preprocessing data . Tabular data have two types of variables: continuous variables (numerical data) and categorical variables (discrete data). In the collaborative filtering model, the users and movies were (high-cardinality) categorial variables. When training a model, we want all our inputs to be continuous variables, so we need a way to turn the categorical variables continuous. . So, you pass your categorical variables through embeddings. An embedding is equivalent to putting a linear layer after every one-hot-encoded input layers. To elaborate: you have inputs that can be indexed by one-hot-encoded vectors. And, an embedding layer takes the relevant inputs from those inputs by indexing while keeping track of the steps taken so that its derivative can be calculated layer. When you pass your one-hot-encoded input layers through an embedding layer, you get continuous numbers, which you can pass through other layers in your neural network. . When we train the model on these embeddings (the inputs), we can interpret the distance between the embeddings afterwards; since the embedding distances were learned based on patterns in the data, they also tend to match up with our intuition. . Since we can form continuous embeddings for our categorical variables, we can treat them like continuous variables when we train our models. So, we could perform probabilistic matrix factorization, or concatenate them with the actual continuous variables and pass them through a neural network. . Below shows how Google trains a model for recommendations on Google Play: . And here we branch . In modern machine learning, there are two main techniques that are widely applicable, each for specific kinds of data: . Ensembles of decision trees (like random forests and gradient boosting machines) for structured data. | Multilayered neural networks optimized with SGD (like shallow and/or deep learning) for unstructured data (like images, audio, and natural language). | Deep learning is almost always superior for unstructured data and tend to give similar results for structured data. But, decision trees train much faster, are simpler to train, and are easier to interpret (like which columns were most important). . However, deep learning is a better choice than decision trees when . there are some high-cardinality categorical variables that are very important (like zip codes); or | there&#39;s some columns that&#39;d be best understood through a neural network like plain text. | . Still, you should try both to see which one works best. Usually, you&#39;ll start with decision trees as a baseline and try to achieve a higher accuracy with a deep learning model if either of those two conditions above applies. . So, in the next two blog posts, I&#39;ll be talking about decision trees and deep learning, respectively, for tabular data. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/17/Tabular.html",
            "relUrl": "/tabular/2022/03/17/Tabular.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "So you want to code collaborative filtering",
            "content": "Hopefully you&#39;ve read my last blog post which explains everything I&#39;m going to be doing in today&#39;s blog. We&#39;re going to be coding a collaborative filtering model in two ways: by probabilistic matrix factorization and then through deep learning. If you&#39;d like to learn how deep learning works, check out my other blog post. . First, we&#39;ll download a subset of the MovieLens dataset, which contains 100,000 of the 25-million recommendation dataset. The main reason being that I&#39;m using the GPUs on Colab and it would take too long to train a model with the full dataset. . from fastai.collab import * from fastai.tabular.all import * path = untar_data(URLs.ML_100k) Path.BASE_PATH = path path.ls() . . 100.15% [4931584/4924029 00:00&lt;00:00] (#23) [Path(&#39;ua.test&#39;),Path(&#39;u.item&#39;),Path(&#39;u2.base&#39;),Path(&#39;u4.test&#39;),Path(&#39;u.user&#39;),Path(&#39;u.genre&#39;),Path(&#39;u.occupation&#39;),Path(&#39;u1.test&#39;),Path(&#39;u5.base&#39;),Path(&#39;u2.test&#39;)...] . . And we can read the README file using the cat command: . !cat {path}/README . SUMMARY &amp; USAGE LICENSE ============================================= MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota. This data set consists of: * 100,000 ratings (1-5) from 943 users on 1682 movies. * Each user has rated at least 20 movies. * Simple demographic info for the users (age, gender, occupation, zip) The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. Detailed descriptions of the data file can be found at the end of this file. Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions: * The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group. * The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information). * The user may not redistribute the data without separate permission. * The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota. If you have any further questions or comments, please contact GroupLens &lt;grouplens-info@cs.umn.edu&gt;. CITATION ============================================== To acknowledge use of the dataset in publications, please cite the following paper: F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872 ACKNOWLEDGEMENTS ============================================== Thanks to Al Borchers for cleaning up this data and writing the accompanying scripts. PUBLISHED WORK THAT HAS USED THIS DATASET ============================================== Herlocker, J., Konstan, J., Borchers, A., Riedl, J.. An Algorithmic Framework for Performing Collaborative Filtering. Proceedings of the 1999 Conference on Research and Development in Information Retrieval. Aug. 1999. FURTHER INFORMATION ABOUT THE GROUPLENS RESEARCH PROJECT ============================================== The GroupLens Research Project is a research group in the Department of Computer Science and Engineering at the University of Minnesota. Members of the GroupLens Research Project are involved in many research projects related to the fields of information filtering, collaborative filtering, and recommender systems. The project is lead by professors John Riedl and Joseph Konstan. The project began to explore automated collaborative filtering in 1992, but is most well known for its world wide trial of an automated collaborative filtering system for Usenet news in 1996. The technology developed in the Usenet trial formed the base for the formation of Net Perceptions, Inc., which was founded by members of GroupLens Research. Since then the project has expanded its scope to research overall information filtering solutions, integrating in content-based methods as well as improving current collaborative filtering technology. Further information on the GroupLens Research project, including research publications, can be found at the following web site: http://www.grouplens.org/ GroupLens Research currently operates a movie recommender based on collaborative filtering: http://www.movielens.org/ DETAILED DESCRIPTIONS OF DATA FILES ============================================== Here are brief descriptions of the data. ml-data.tar.gz -- Compressed tar file. To rebuild the u data files do this: gunzip ml-data.tar.gz tar xvf ml-data.tar mku.sh u.data -- The full u data set, 100000 ratings by 943 users on 1682 items. Each user has rated at least 20 movies. Users and items are numbered consecutively from 1. The data is randomly ordered. This is a tab separated list of user id | item id | rating | timestamp. The time stamps are unix seconds since 1/1/1970 UTC u.info -- The number of users, items, and ratings in the u data set. u.item -- Information about the items (movies); this is a tab separated list of movie id | movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children&#39;s | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western | The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once. The movie ids are the ones used in the u.data data set. u.genre -- A list of the genres. u.user -- Demographic information about the users; this is a tab separated list of user id | age | gender | occupation | zip code The user ids are the ones used in the u.data data set. u.occupation -- A list of the occupations. u1.base -- The data sets u1.base and u1.test through u5.base and u5.test u1.test are 80%/20% splits of the u data into training and test data. u2.base Each of u1, ..., u5 have disjoint test sets; this if for u2.test 5 fold cross validation (where you repeat your experiment u3.base with each training and test set and average the results). u3.test These data sets can be generated from u.data by mku.sh. u4.base u4.test u5.base u5.test ua.base -- The data sets ua.base, ua.test, ub.base, and ub.test ua.test split the u data into a training set and a test set with ub.base exactly 10 ratings per user in the test set. The sets ub.test ua.test and ub.test are disjoint. These data sets can be generated from u.data by mku.sh. allbut.pl -- The script that generates training and test sets where all but n of a users ratings are in the training data. mku.sh -- A shell script to generate all the u data sets from u.data. . . Which tells us that u.data contains the full data set, which is 100,000 ratings by 943 users on 1682 items, where each user rated at least 20 movies. The data is tab separated with column names: user id, item/movie id, rating, and timestamp. So, let&#39;s try reading the csv: . ratings = pd.read_csv( path/&#39;u.data&#39;, delimiter = &#39; t&#39;, header = None, names = [&#39;user&#39;, &#39;movie&#39;, &#39;rating&#39;, &#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We&#39;d like to know the actual movie name instead of the movie ID, so we can also read the u.item file (although it says it&#39;s tab separated, it&#39;s actually pipe separated): . movies = pd.read_csv( path/&#39;u.item&#39;, delimiter = &#39;|&#39;, header = None, usecols = [0, 1], names = [&#39;movie&#39;, &#39;title&#39;], encoding = &#39;latin-1&#39; ) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Then, we can merge the two tables together: . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Then, we build our DataLoaders object, which will have our training and validation DataLoaders (which produces our mini-batches of Datasets). . dls = CollabDataLoaders.from_df(ratings, item_name = &#39;title&#39;, bs = 64) dls.show_batch() . user title rating . 0 542 | My Left Foot (1989) | 4 | . 1 422 | Event Horizon (1997) | 3 | . 2 311 | African Queen, The (1951) | 4 | . 3 595 | Face/Off (1997) | 4 | . 4 617 | Evil Dead II (1987) | 1 | . 5 158 | Jurassic Park (1993) | 5 | . 6 836 | Chasing Amy (1997) | 3 | . 7 474 | Emma (1996) | 3 | . 8 466 | Jackie Chan&#39;s First Strike (1996) | 3 | . 9 554 | Scream (1996) | 3 | . And then we create our model, which will contain our embeddings. We can&#39;t just index into a matrix for a deep learning model since we have to calculate the derivative for each operation we do. Instead, we use one-hot encoding, which is a vector that has a 1 in the places that we want to index in. For example, if we have an array [0, 1, 2, 3] and we want the element in the 2nd index (2), we would matrix multiply [0, 0, 1, 0] to the array&#39;s transpose: $$ begin{bmatrix}0 &amp; 1 &amp; 2 &amp; 3 end{bmatrix}^T begin{bmatrix}0&amp;0&amp;1&amp;0 end{bmatrix}= begin{bmatrix}0 1 2 3 end{bmatrix} begin{bmatrix}0&amp;0&amp;1&amp;0 end{bmatrix}= begin{bmatrix}2 end{bmatrix}$$ . But, storing and using one-hot encoding vectors are pretty time and memory consuming, so we use a special layer in most deep learning libraries (like PyTorch) called embedding. Embedding is mimicking the process of multiplying by a one-hot-encoded matrix, but it just indexes into a matrix using an integer while having its derivative calculated in a way such that it&#39;s identical to what it would&#39;ve been if a matrix multiplication was done with a one-hot-encoded vector. . Optimizers need to be able to get all the parameters from a model, so all embedding does is randomly initialize a matrix and wrap it around the nn.Parameter class which tells PyTorch that it&#39;s a trainable parameter. . When we refer to an embedding, that&#39;s the embedding matrix, which is the thing that&#39;s multiplied by the one-hot-encoded matrix or the thing that&#39;s being indexed into. So, an embedding matrix in this case, is our latent factors (and biases). . When creating a neural network model with PyTorch, we have to inherit from their Module class which contains the essentials; we just have to define __init__ (called dunder init) to initialize our model and forward which is essentially the &quot;predict&quot; step in the model. forward accepts the parameters of a mini-batch and returns a prediction. . n_users = len(dls.classes[&#39;user&#39;]) n_items = len(dls.classes[&#39;title&#39;]) n_factors = 50 . class DotProduct(Module): def __init__(self, n_users, n_items, n_factors, y_range = (0, 5.5)): # User latent factors and biases self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) # Item latent factors and biases self.item_factors = Embedding(n_items, n_factors) self.item_bias = Embedding(n_items, 1) # Range for our predictions self.y_range = y_range def forward(self, x): # Get first column (the users) from input users = self.user_factors(x[:,0]) # Get second column (the titles) from input items = self.item_factors(x[:,1]) # Calculate the dot product dot_prod = (users * items).sum(dim = 1, keepdim = True) # Add biases to the dot product # We add the user biases and the item biases together dot_prod += self.user_bias(x[:,0]) + self.item_bias(x[:,1]) # Return the prediction in the chosen range # Sigmoid is a function that returns a value between 0 and 1 # We can multiply it by (hi - lo) and add lo to get a value # between lo and hi, which is what sigmoid_range does return sigmoid_range(dot_prod, *self.y_range) . Now that we have our model, we can create an object with it and pass it into a Learner and train it. . model = DotProduct(n_users, n_items, n_factors) learn = Learner(dls, model, loss_func = MSELossFlat()) # We also use weight decay since we have bias in our model learn.fit_one_cycle(5, 5e-3, wd = 0.1) . epoch train_loss valid_loss time . 0 | 0.941400 | 0.941900 | 00:09 | . 1 | 0.847874 | 0.877467 | 00:08 | . 2 | 0.719121 | 0.835374 | 00:07 | . 3 | 0.594287 | 0.824023 | 00:07 | . 4 | 0.483335 | 0.824634 | 00:07 | . And, we don&#39;t need to define our own DotProduct class. We can instead use fast.ai&#39;s collab_learner. . learn = collab_learner(dls, n_factors = 50, y_range = (0, 5.5)) learn.fit_one_cycle(5, 5e-3, wd = 0.1) . epoch train_loss valid_loss time . 0 | 0.940803 | 0.954099 | 00:08 | . 1 | 0.846296 | 0.874175 | 00:07 | . 2 | 0.741423 | 0.838990 | 00:07 | . 3 | 0.590897 | 0.822672 | 00:07 | . 4 | 0.492853 | 0.823269 | 00:07 | . And we see the results are similar since the model used by collab_learner is essentially equivalent: . # 50 latent factors for users and items # bias for users and items learn.model . EmbeddingDotBias( (u_weight): Embedding(944, 50) (i_weight): Embedding(1665, 50) (u_bias): Embedding(944, 1) (i_bias): Embedding(1665, 1) ) . To turn our architecture into a deep learning model, we need a neural network. With a neural network, we start with a large matrix that we pass through layers. Instead of taking the dot product, we concatenate the latent factors from the users and the items. So, we also don&#39;t need the same number of latent factors for users as for items. To get the number of latent factors, we can use fast.ai&#39;s get_emb_sz function on our DataLoaders, which will give us recommended latent factors: . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . And, we can rewrite our DotProduct class like so: . class SimpleNet(Module): def __init__(self, user_sz, item_sz, y_range = (0, 5.5), n_acts = 100): # nn.Linear implements bias implicitly, so we # don&#39;t need to define our own bias. self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1] + item_sz[1], n_acts), nn.ReLU(), nn.Linear(n_acts, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim = 1)) return sigmoid_range(x, *self.y_range) . Then, we can put it in a Learner and train our deep learning model: . model = SimpleNet(*embs) learn = Learner(dls, model, loss_func = MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd = 0.1) . epoch train_loss valid_loss time . 0 | 0.942289 | 0.957183 | 00:07 | . 1 | 0.918996 | 0.915120 | 00:07 | . 2 | 0.854367 | 0.902296 | 00:07 | . 3 | 0.820374 | 0.877131 | 00:07 | . 4 | 0.827481 | 0.877810 | 00:07 | . And, like how we didn&#39;t need to define our own DotProduct class and use collab_learner instead, we can also do the same with SimpleNet. . # We just have to enable the use_nn parameter and # give it layers learn = collab_learner(dls, use_nn = True, y_range = (0, 5.5), layers = [100, 50]) learn.fit_one_cycle(5, 5e-3, wd = 0.1) . epoch train_loss valid_loss time . 0 | 1.018050 | 0.979968 | 00:13 | . 1 | 0.906580 | 0.922872 | 00:08 | . 2 | 0.909671 | 0.890887 | 00:08 | . 3 | 0.814160 | 0.870163 | 00:08 | . 4 | 0.802491 | 0.869587 | 00:09 | . Interpreting the results . Now that you&#39;ve trained a model, there&#39;s several ways to interpret your results. . First, we can look at the biases: . # First, take the biases and put them into # a one-dimensional tensor that we can sort item_bias = learn.model.item_bias.weight.squeeze() # argsort returns a list of indexes that would # sort the tensor idxs_bot = item_bias.argsort()[:5] idxs_top = item_bias.argsort(descending = True)[:5] # display the titles of the 5 &quot;worst&quot; movies # and the 5 &quot;best&quot; movies, respectively [dls.classes[&#39;title&#39;][i] for i in idxs_bot],[dls.classes[&#39;title&#39;][i] for i in idxs_top] . ([&#39;Children of the Corn: The Gathering (1996)&#39;, &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, &#39;Crow: City of Angels, The (1996)&#39;, &#39;Beautician and the Beast, The (1997)&#39;, &#39;Robocop 3 (1993)&#39;], [&#39;Titanic (1997)&#39;, &#39;L.A. Confidential (1997)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#34;Schindler&#39;s List (1993)&#34;, &#39;Silence of the Lambs, The (1991)&#39;]) . Then, we can find the distances: . item_factors = learn.model.item_factors.weight idx = dls.classes[&#39;title&#39;].o2i[&#39;Toy Story (1995)&#39;] distances = nn.CosineSimilarity()(item_factors, item_factors[idx][None]) idx = distances.argsort(descending = True)[1:5] dls.classes[&#39;title&#39;][idx] . (#4) [&#39;That Thing You Do! (1996)&#39;,&#39;Abyss, The (1989)&#39;,&#39;Wizard of Oz, The (1939)&#39;,&#39;Aladdin (1992)&#39;] . So, the four movies in the data set that are most similar to Toy Story are the ones above. . In the next few blogs, I&#39;ll be talking more about deep and machine learning with tabular data. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/16/Collaborative-Filtering-Code.html",
            "relUrl": "/tabular/2022/03/16/Collaborative-Filtering-Code.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "So you want to learn collaborative filtering",
            "content": "What is collaborative filtering? . You want to start a movie streaming service. You&#39;re learning machine learning and you want a way to recommend people movies so they watch more and spend more time on your app so you get more ad revenue. How would you solve this problem? . Let&#39;s look at Bingus. He&#39;s a McMaster alumni turned McDonald&#39;s employee. He works an 8 AM to 6 PM shift flipping burgers for minimum wage and gets home to watch some movies. He really likes this one movie called &quot;Cars 3&quot;. He&#39;s watched dozens of times now and he won&#39;t stop. . Then comes Alice. Unlike Bingus, she graduated from Waterloo and landed a CS job earning six figures. She even graduated with the highest GPA of her year and landed that CS job through a return offer from her summer coop. So what? Alice and Bingus both like &quot;Cars 3&quot;. . Since they both like &quot;Cars 3&quot;, we could recommend Alice movies that Bingus also likes and recommend Bingus movies that Alice also likes. . . Collaborative filtering: look at items the current user used or liked, find other users that used or liked similar items, then recommend those items to the current user. . . Collaborative filtering works through latent factors, which are basically the &quot;tags&quot; you would give an item. For example, you could give a movie tags like &quot;science fiction,&quot; &quot;action,&quot; &quot;old,&quot; &quot;horror,&quot; and &quot;romance.&quot; It&#39;s latent because it depends on users for the factors to have meaning. . What&#39;s wild is we never tell our model the latent factors. We just say how many we want and the model learns them on its own. So, we don&#39;t need to know anything about the items (in a descriptive way); we just need the data on the users and the items. . How do you implement collaborative filtering? . Our model will work using stochastic gradient descent. So, we need three things: . random parameters; | a way to calculate our predictions; and | a loss function. | . We first randomly initialize some parameters for our latent factors. Let&#39;s say we want our model to learn 5 latent factors. Then, each user and item would have 5 parameters (or, a 5-dimensional vector). We can use the dot product between the user and the item as our prediction of how likely we would recommend an item to a user. The higher the value, the more likely the user will like that product. . Finally, we need to pick a loss function. Since we&#39;re dealing with regression instead of classification (like digit classification), we can use L1 norm or L2 norm. . . L1 norm (absolute mean difference): take the absolute difference between two values and take the mean. . $$L_1= sum^n_{i=0} frac{|a_i-b_i|}{2}, ,a in A, , b in B$$ . L2 norm (root mean squared error): take the square of the differences between two values and take the mean. Then, square root the result. $$L_2= sqrt{ sum^n_{i=0} frac{(a_i-b_i)^2}{2}}, ,a in A, , b in B$$ . . What&#39;s the difference? L2 norm puts a larger emphasis on small and large changes because of squaring since large * large = larger and small * small = smaller. . Now, we have all we need for stochastic gradient descent: random parameters, a way to calculate our predictions, and our loss function. . At each step, the SGD optimizer calculates the match between each item and user (random parameters) using the dot product (procedure for predictions), and compares it to the actual rating that each user gave to each item (loss function). Then, it calculates the derivative of this value (gradient) and steps the weights by multiplying the calculated derivative by the learning rate and subtracting the weights by that value (descent). . After each epoch, the loss gets better (lower) and the recommendations will also get better. . But something&#39;s missing... . There&#39;s usually a range for ratings on an item. Like &quot;out of 5 stars&quot;. So, you should put a range on your predictions so that they&#39;re in a similar range as the rating system you have for an item. For example, if a movie service has a rating system from 0 to 5, you should have a range from 0 to 5.5. It&#39;s been discovered empirically that having the upper bound a little bit over returns better results. . And, remember how parameters are the weights and biases? We should also have biases attached to each user and each item. Some users may be more positive or negative than others. And, some items may be superior than others. It could also reflect current trends. Nonetheless, adding biases on their own usually leads to overfitting. . So, you also need to add L2 regularization, . . L2 regularization (weight decay): add the sum of all the weights squared to your loss function. . . Why? Because when you compute the gradients, the added sum encourages the weights to be as small as possible. It prevents overfitting because having high parameters lead to sharper changes in the loss function, which can lead to overfitting. So, having smaller parameters encouraged by weight decay decreases that. . However, you don&#39;t apply weight decay by adding the sum of all weights squared to the loss function (it would be inefficient and lead to huge numbers). Instead, add double the parameters to the gradient since the derivative of $x^2$ is $2x$. And, weight decay is just a hyperparameter that you multiply $2x$ by, so what you actually do is gradient += wd * 2 * parameters, which is essentially gradient += wd * parameters (2 is incorporated into wd like how you just have + C for integrals). The end result of adding biases with weight decay is that we make training the model a bit harder, but the model will generalize better in practice. . Interpreting the model . Now, you finished training your model. You have your biases and latent factors (weights) all set. How can you interpret your parameters before putting your model in action? . With biases, you can sort items to see . current trends; and | which items are good (high bias) or bad (low bias). | . Interpreting the latent factors is a bit trickier in that you can&#39;t just model it. But, there is a technique called principal component analysis, which lets you take the most important directions in the latent factors. . There&#39;s a simpler alternative if you just want to compare a few items: you can calculate the &quot;distance&quot; between two items. If two items were very similar, then their latent factors would also be similar. So, their &quot;distance&quot; would be low compared to the distance between a more different item. Ultimately, item similarity in a model is dictated by the similarity of users that like those items. . To calculate to distance, you use Pythagoras&#39; formula: . $$d= sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$$ . except you would do this for how many dimensions there are. For example, the distance between two 50-dimensional embedding (the parameters) would be . $$d= sqrt{(x_{2,1}-x_{1,1})^2+(x_{2,2}-x_{1,2})^2+ dots+(x_{2,50}-x_{1,50})^2}$$ . So, say you have one movie, &quot;Cars 3&quot; and two other movies: &quot;Cars 4&quot; and &quot;Harry Potter&quot;. The distance between &quot;Cars 3&quot; and &quot;Cars 4&quot; might be 50, while it&#39;s 100 for &quot;Cars 3&quot; and &quot;Harry Potter&quot;. Then, since the distance for the former is shorter, you could infer that &quot;Cars 3&quot; is more similar to &quot;Cars 4&quot; than &quot;Cars 3&quot; is to &quot;Harry Potter&quot;. . But wait, there&#39;s a problem . We all know by now that overfitting is a big problem in the training process. But, there&#39;s an equally important problem in practice for collaborative filtering: the bootstrapping problem. . . Bootstrap: to start something with little help. . Bootstrapping problem: what do you do when you have no users (no data) to train your model; and, if you do have previous users, what do you recommend for a new user? Similarly, what do you do when you add a new item? . . Like overfitting, there isn&#39;t a solution that works for everything, but there are some used commonly: . assign the mean/median of all the latent factors to the new user or item; | pick a specific user or item to represent the average user; | survey the new user or item to construct a basic set of latent factors for them. | . However, solutions to the bootstrapping problem leads to another problem: positive feedback loops. A small number of otakus can set the recommendation for the entire user base. You might expect this feedback loop to be an outlier, but it&#39;s actually the norm. For example, even though not a lot of people watch anime, a few people really enjoy &quot;Demon Slayer&quot;; so when the movie came out, it became highly recommended for the general user base. Similarly, &quot;Squid Game&quot; also became popular this way along with many Korean movies like &quot;Parasite&quot; and &quot;Train to Busan&quot;. It&#39;s only when the systems do something about it (like deliberately lower its bias, don&#39;t recommend it anymore, or through time) that the feedback loop stops. . The bias for certain items in the latent factors may be due to representation bias. If you don&#39;t want your entire user base (and your system) to change, then you have to be wary of these feedback loops. Once the bias becomes too high, more people of the same group come along and your user base ends up being that group. . An easy way to prepare for feedback loops is to integrate your model slowly: . first, have people monitor the model and its recommendations; | then, monitor the recommendations over time; and | eventually let the model recommend on its own. | . But what about deep learning? . Our first method isn&#39;t deep learning, but instead called probabilistic matrix factorization. Instead of a neural network, we used the dot product to calculate our predictions. . With deep learning, we need a neural network, which contains all the layers with parameters that we optimize in each epoch. So, we also don&#39;t need the same number of latent factors for items and users since we won&#39;t be using the dot product. . What you&#39;ll find is that deep learning is a bit worse than probabilistic matrix factorization on its own. But, you can add other user and item information, date and time information, and/or any other information that might be relevant to the recommendation. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/12/Collaborative-Filtering.html",
            "relUrl": "/tabular/2022/03/12/Collaborative-Filtering.html",
            "date": " • Mar 12, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "So, what is deep learning?",
            "content": "So what is deep learning? From what I&#39;ve seen so far of the fast.ai course (chapters 1 to 7), deep learning is training a model with labelled data such that it is able to predict labels for unlabelled data in practice. . With a standard algorithm, you get inputs, you give it to the algorithm, and you get an output; you know the code in the algorithm (or at least, you should understand it). It&#39;s similar in a deep learning model: you know how the model is going to be trained (by hyperparameters like loss function, architecture like resnet18, optimizer like SGD, number of epochs, etc.) and you can look into the parameters (what the model learned; e.g., in vision models you can look at the layers and see how the earlier layers learn things like edges, gradients, etc. while the later layers learn more of the complex things like dog faces). . The difference is that you didn&#39;t hard code the parameters - hopefully - like how you hard coded the algorithm (or copied from a library or Stack Overflow). You have to train the model by giving it labelled data. Emphasis on labelled data. The model can&#39;t learn if it doesn&#39;t have anything to learn from. Why? Because you&#39;re creating a model to predict labels so your data should be labelled. So, a lot of deep learning ends up just labelling your data correctly (or labelling your data in general). . Then, there&#39;s overfitting - an obstacle that prevents your model from being useful. Imagine this: your model predicts your training data with near 100% accuracy. Congrats! You should get an award! Oh, wait, your model can only predict the validation set with 90% accuracy. Oh, double wait, your model can only predict the test set with 50% accuracy... WHAT&#39;S HAPPENING? Your model is memorizing (over-fitting) your training data instead of actually learning from it. . So, you should be careful when training your model. There&#39;s so many options you can tune when training a model. From what I&#39;ve seen so far, you usually overfit by . having a large number of epochs, | using too large or small learning rate, | having a poorly split dataset (like having a random split on a time series data), and | making some post-training decisions that you end up overfitting on the validation set, and | not using a pre-trained model (if one exists for your intentions). | . Remember, you want your model to be good at generalizing on previously unseen data... unless you have a dataset that&#39;s so large that there is no previously unseen data. It doesn&#39;t matter how well your model does on your training and validation sets if it doesn&#39;t work with the test set or a random piece of data you give it! . Then, What is a Deep Learning Model? . A deep learning model consists of layers. Remember how I mentioned resnet18? That&#39;s an architecture (pre-trained too, which means it&#39;s been trained on a dataset before and we&#39;re retraining it with our own dataset which saves time and money since we&#39;ll start with a pretty high accuracy). An architecture is the skeleton of a deep learning model. It contains all the layers and parameters. resnet18 as you could assume, contains 18 layers. . A layer is technically composed of two things. The first is a linear layer ($w cdot x+b$, where $w$ are the weights, $x$ is the data or input, and $b$ is the bias; what people call the parameters of a model is the weights and biases) followed by a non-linear layer typically a ReLU (rectified linear unit, which turns all negative numbers to 0 and keeps positive numbers as is). So, you may be wondering: why do we need both? Well, if you only had linear layers, you could combine all of them into a single layer. So, we need some kind of nonlinearity so that we can prevent the linear layers from becoming one big linear layer. . Now, lets say you have an image. We transform the data, for example through resizing and augmenting it (sort of like distorting it, but in a good way), such that it&#39;s $224 times 224$ pixels large. That&#39;s $50176$ pixels. Then, there could be $4$ more categories for red, green, blue, and alpha (transparency). That becomes $200704$ input values for the first layer. With each layer, you want to decrease that value until the last layer, where you&#39;ll have $n$ outputs since you have $n$ different labels. . Then, how does the model actually learn? Every time you pass a piece of data and you get a prediction, that prediction is passed onto a function called the loss function (what you care about is the metric function, the computer cares about the loss function), using that loss function, each parameter takes a step in the opposite direction of the slope of that loss function at that paramter&#39;s value such that by the next prediction, the value of the loss function is smaller. How big is that step? It&#39;s determined by the learning rate you chose. That&#39;s why it&#39;s important to have a good loss function and a reasonable learning rate. . And, you may be saying, &quot;that&#39;s cool and all, but how does that step actually work?&quot; Each number in a layer (weights and biases) are set so that they keep track of what functions are applied to them. So, when you take a &quot;step&quot; (or optimize; hence, &quot;optimizer&quot; like SGD), you first calculate the function of the derivates of the functions applied through chain rule and takes the value of that gradient (so if the derivative is $f&#39;$ and the number is $x$, the value would be $f&#39;(x)$). Then, with the learning rate, you take a step in the opposite direction such that you descend (parameters -= gradient * learning_rate). Now you can guess why they called the process &quot;stochastic gradient descent.&quot; You descend the loss function (which should be easily differentiable and typically have non-zero gradients) based on the gradient and the parameters are usually randomly assigned, hence &quot;stochastic.&quot; . With each epoch (which is one complete pass through the data), the parameters are finetuned for your task, eventually memorizing the training set. You don&#39;t want that, so you train it for as few epochs as you need (to prevent overfitting and to save time). . Remember when I said &quot;you care about the metric function?&quot; That&#39;s what you want to pay attention to after each epoch. Metric functions are typically things like accuracy or error rate (which is 1 - accuracy), which are helpful for us to analyze our model, but terrible for our model to use as loss functions. . With an overfitting model, you usually see two things: the loss for the training set continues to decrease, but the loss for the validation set is suddenly increasing. The opposite is true for the metric: the metric for the training set is increasing, but the metric for the validation set is decreasing. That&#39;s why you typically want a test set. A training set is the set your model sees when it&#39;s training. A validation set is for your eyes only and used to test how well the model is learning. A test set is for no one&#39;s eyes. Only for God&#39;s eyes or whatever you believe in. Once you finished training your model and tuned all the hyperparameters you want, a test set will tell you how well your model actually trained. . Of course, the validation set and training set are going to be useless if you split your data badly. The common example is with a time series dataset. If you split your data randomly, the model can easily predict intermediate values, but why would you want to know about the past? You want to predict the future. So, you could have your training set consist of data not containing the most recent 6 months, for example. And your validation set consists of the remaining 6 months. You could even take the most recent month out and put the first 5 months into your validation set instead and the most recent month into the test set. Whenever you split your data, make sure it&#39;s with the intention that the model will be learning the patterns of the data such that it can generalize to future, never-seen-before data. . That&#39;s it? . Well, that&#39;s basically what I got so far from fast.ai. With Arthur Samuel&#39;s definition of deep learning, you essentially have a model, where you give it labelled data as input, it produces predictions, then those predictions are passed to a loss function, then you optimize the predictions and repeat for $n$ many epochs until you reach a certain point and stop training the model. . There are many parts that you can change in a model with some that you have to change depending on the task, particularly the loss function. . I&#39;ve mainly been focusing on computer vision since that&#39;s what the first 7 chapters were about (apart from chapter 1; I&#39;m also leaving chapter 3 for last, which is about ethics). . Next time you see me, I&#39;ll be talking about tabular data. .",
            "url": "https://geon-youn.github.io/DunGeon/2022/03/07/What-Is-Deep-Learning.html",
            "relUrl": "/2022/03/07/What-Is-Deep-Learning.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "A pet breed classifier",
            "content": "Intro . I remember volunteering at a hackathon and sitting in the award ceremony when I saw a group win in the &quot;fun&quot; category for creating a pet breed classifier. You give it an image and it&#39;ll tell you what breed it thinks it is and how confident it is. It was &quot;fun&quot; because you could override the threshold and allow images that aren&#39;t cats and dogs to be classified as a dog or cat breed. This blog post will show you how you can train your own pet breed classifer and how it isn&#39;t that hard nor time consuming to do so. You don&#39;t need a beefy computer either since you can use Colab&#39;s GPUs. . Training our own pet breed classifier . First, we&#39;ll download the Pet dataset and see what we&#39;re given: . path = untar_data(URLs.PETS) Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;images&#39;),Path(&#39;annotations&#39;)] . (path/&#39;images&#39;).ls() . (#7393) [Path(&#39;images/english_setter_69.jpg&#39;),Path(&#39;images/scottish_terrier_120.jpg&#39;),Path(&#39;images/basset_hound_113.jpg&#39;),Path(&#39;images/miniature_pinscher_87.jpg&#39;),Path(&#39;images/pomeranian_1.jpg&#39;),Path(&#39;images/Persian_68.jpg&#39;),Path(&#39;images/japanese_chin_39.jpg&#39;),Path(&#39;images/english_setter_107.jpg&#39;),Path(&#39;images/Birman_128.jpg&#39;),Path(&#39;images/staffordshire_bull_terrier_26.jpg&#39;)...] . In this dataset, there are two subfolders: images and annotations. images contains the images of the pet breeds (and their labels) while annotations contains the location of the pet in each image if you wanted to do localization. . The images are structured like so: the name of the pet breed with spaces turned into underscores, followed by a number. The name is capitalized if the pet is a cat. We can get the name of the pet breed by using regular expressions: . fname = (path/&#39;images&#39;).ls()[0] fname, fname.name . (Path(&#39;images/english_setter_69.jpg&#39;), &#39;english_setter_69.jpg&#39;) . # () = extract what&#39;s in the parentheses -&gt; .+ # .+ = any character appearing one or more times # _ = followed by an underscore # d+ = followed by any digit appearing one or more times # .jpg$ = with a .jpg extension at the end of the string re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;english_setter&#39;] . This time, we&#39;ll be using a DataBlock to create our DataLoaders . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = partial(get_image_files, folders = &#39;images&#39;), splitter = RandomSplitter(), get_y = using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms = Resize(460), batch_tfms = aug_transforms(size = 224, min_scale = 0.75)) dls = pets.dataloaders(path) . In our pets DataBlock, we give it the following parameters: . blocks = (ImageBlock, CategoryBlock): our independent variable is an image and our dependent variable is a category. | get_items = partial(get_image_files, folders = &#39;images&#39;): we are getting our images recursively in the images folder. If you&#39;ve used functional programming before, partial is like currying; we give a function some of its parameters and it returns another function that accepts the rest of its parameters, except partial allows us to specify which parameters we want to give. | splitter = RandomSplitter(): randomly splits our data into training and validation sets with a default 80:20 split. We can also specify a seed if we want to test how tuning our hyperparameters affects the final accuracy. | . The final two parameters are part of &quot;presizing&quot;: . item_tfms = Resize(460): picks a random area of an image (using its max width or height, whichever is smallest) and resizes it to 460x460. This process happens for all images in the dataset. | batch_tfms = aug_transforms(size = 224, min_scale = 0.75): take a random portion of the image which is at least 75% of it and resize to 224x224. This process happens for all images in a batch (like the batch we get when we call dls.one_batch()). | . We first resize an image to a much larger size than our actual size for training so that we can avoid the data destruction done by data augmentation. The larger size allows tranformation of the data without creating empty areas. . . We can check if our DataLoaders is created successfully by using the .show_batch() feature: . dls.show_batch(nrows = 1, ncols = 4) . We can then do some Googling to make sure our images are labelled correctly. . Fastai also allows us to debug our DataBlock in case we make an error. It attemps to create a batch from the source: . pets.summary(path) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_179.jpg applying PILBase.create gives PILImage mode=RGB size=500x334 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_179.jpg applying partial gives great_pyrenees applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(21) Final sample: (PILImage mode=RGB size=500x334, TensorCategory(21)) Collecting items from /root/.fastai/data/oxford-iiit-pet Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=500x334, TensorCategory(21)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(21)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(21)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) . . Now, let&#39;s get to training our model. This time, we&#39;ll be fine tuning a pretrained model. This process is called transfer learning, where we take a pretrained model and retrain it on our data so that it can perform well for our task. We randomize the head (last layer) of our model, freeze the parameters of the earlier layers and train our model for one epoch. Then, we unfreeze the model and update the later layers of the model with a higher learning rate than the earlier layers. . The pretrained model we will be using is resnet34, which was trained on the ImageNet dataset with 34 layers: . learner = cnn_learner(dls, resnet34, metrics = accuracy) . lrs = learner.lr_find() . learner.fit_one_cycle(3, lr_max = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 1.542125 | 0.296727 | 0.900541 | 01:14 | . 1 | 0.618474 | 0.227452 | 0.924222 | 01:13 | . 2 | 0.401809 | 0.214500 | 0.932341 | 01:12 | . learner.unfreeze() lrs = learner.lr_find() . learner.fit_one_cycle(6, lr_max = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 0.340459 | 0.213287 | 0.928281 | 01:16 | . 1 | 0.341917 | 0.233392 | 0.921516 | 01:16 | . 2 | 0.277254 | 0.187060 | 0.939107 | 01:16 | . 3 | 0.191343 | 0.192029 | 0.938430 | 01:16 | . 4 | 0.156336 | 0.178532 | 0.941813 | 01:16 | . 5 | 0.123608 | 0.174198 | 0.939107 | 01:16 | . When we use a pretrained model, fastai automatically freezes the early layers.We then train the head (last layer) of the model for 3 epochs so that it can get a sense of our objective. Then, we unfreeze the model and train all the layers for 6 more epochs. After training for a total of 9 epochs, we now have a model that can predict pet breeds accuractely 94% of the time. We can use fastai&#39;s confusion matrix to see where our model is having problems: . interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix(figsize = (12, 12), dpi = 60) . interp.most_confused(5) . [(&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 6), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 5), (&#39;chihuahua&#39;, &#39;miniature_pinscher&#39;, 5)] . Using the .most_confused feature, it seems like most of the errors come from the pet breeds being very similar. We should be careful however, that we aren&#39;t overfitting on our validation set through changing hyperparameters. We can see that our training loss is always going down, but our validation loss fluctuates from going down and sometimes up. . And that&#39;s all there is to training a pet breed classifier. You could improve the accuracy by exploring deeper models like resnet50 which has 50 layers; training for more epochs (whether before unfreezing or after or both); using discriminative learning rates (giving lower learning rates or early laters using split(lr1, lr2) in the lr_max key-word argument in fit_one_cycle). . Using our own pet breed classifier . First, let&#39;s save the model using .export(): . learner.export() . Then, let&#39;s load the .pkl file: . learn = load_learner(&#39;export.pkl&#39;) . Create some basic UI: . def pretty(name: str) -&gt; str: return name.replace(&#39;_&#39;, &#39; &#39;).lower() . def classify(a): if not btn_upload.data: lbl_pred.value = &#39;Please upload an image.&#39; return img = PILImage.create(btn_upload.data[-1]) pred, pred_idx, probs = learn.predict(img) out_pl.clear_output() with out_pl: display(img.to_thumb(128, 128)) lbl_pred.value = f&#39;Looks like a {pretty(pred)} to me. I &#39;m {probs[pred_idx] * 100:.02f}% confident!&#39; btn_upload = widgets.FileUpload() lbl_pred = widgets.Label() out_pl = widgets.Output() btn_run = widgets.Button(description = &#39;Classify&#39;) btn_run.on_click(classify) VBox([ widgets.Label(&#39;Upload a pet!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . And there we have it! You can make it prettier and go win a hackathon. . However, a bit of a downside with deep learning is that it can only predict what it has been trained on. So, drawings of pets, night-time images of pets, and breeds that weren&#39;t included in the training set won&#39;t be accurately labelled. . We could solve the last case by turning this problem into a multi-label classification problem. Then, if we aren&#39;t confident that we have one of the known breeds, we can just say we don&#39;t know this breed. . Siamese pair . When I was watching the fastai lectures, I heard Jeremy talking about &quot;siamese pairs&quot; where you give the model two images and it will tell you if they are of the same breed. Now that we have a model, let&#39;s make it! . def pair(a): if not up1.data or not up2.data: lbl.value = &#39;Please upload images.&#39; return im1 = PILImage.create(up1.data[-1]) im2 = PILImage.create(up2.data[-1]) pred1, x, _ = learn.predict(im1) pred2, y, _ = learn.predict(im2) out1.clear_output() out2.clear_output() with out1: display(im1.to_thumb(128, 128)) with out2: display(im2.to_thumb(128, 128)) if x == y: lbl.value = f&#39;Wow, they &#39;re both {pretty(pred1)}(s)!&#39; else: lbl.value = f&#39;The first one seems to be {pretty(pred1)} while the second one is a(n) {pretty(pred2)}. I &#39;m not an expert, but they seem to be of different breeds, chief.&#39; up1 = widgets.FileUpload() up2 = widgets.FileUpload() lbl = widgets.Label() out1 = widgets.Output() out2 = widgets.Output() run = widgets.Button(description = &#39;Classify&#39;) run.on_click(pair) VBox([ widgets.Label(&quot;Siamese Pairs&quot;), HBox([up1, up2]), run, HBox([out1, out2]), lbl ]) . You can now test out these cells here! .",
            "url": "https://geon-youn.github.io/DunGeon/vision/2022/02/21/Pet-Breeds.html",
            "relUrl": "/vision/2022/02/21/Pet-Breeds.html",
            "date": " • Feb 21, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Pixel Similarity vs. Basic Neural Net on the MNIST data set",
            "content": "Pixel similarity . We will take the average of each digit to get its &quot;perfect&quot; version. Then, we compare an image to each of those perfect numbers and see which one is the most similar. . First, we&#39;ll download the MNIST dataset: . path = untar_data(URLs.MNIST) . The dataset is separated into training and testing subfolders, where in those folders, there are separate folders for each digit: . Path.BASE_PATH = path . path.ls(),(path/&#39;training&#39;).ls() . ((#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)], (#10) [Path(&#39;training/7&#39;),Path(&#39;training/8&#39;),Path(&#39;training/1&#39;),Path(&#39;training/6&#39;),Path(&#39;training/3&#39;),Path(&#39;training/2&#39;),Path(&#39;training/5&#39;),Path(&#39;training/4&#39;),Path(&#39;training/0&#39;),Path(&#39;training/9&#39;)]) . We&#39;ll store the path of each image in an array, where the ith row contains the path for the ith digit: . nums = [(path/&#39;training&#39;/f&#39;{x}&#39;).ls().sorted() for x in range(10)] . im3_path = nums[3][0] im3 = Image.open(im3_path) im3 . Then, we&#39;ll open the images, put every image of the same digit into their own tensor and store them as a list of tensors: . nums_tens = [torch.stack([tensor(Image.open(j)) for j in nums[i]]) for i in range(10)] nums_tens = [nums_tens[i].float()/255 for i in range(10)] . We can then take the mean of one of the tensors to get its &quot;perfect&quot; version. Here is how it looks like for a 3: . stacked_threes = nums_tens[3].mean(0) show_image(stacked_threes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e3d3acd0&gt; . And to compare, here is just one of those threes: . a_3 = nums_tens[3][0] show_image(a_3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e36f1750&gt; . Next, we&#39;ll create a function that compares two tensors through absolute mean difference: . def mnist_distance(x1, x2): return (x1 - x2).abs().mean((-1, -2)) . Now we can compare one of the threes with its &quot;perfect&quot; version. The number doesn&#39;t really mean anything until we compare it with another number: . mnist_distance(a_3, stacked_threes) . tensor(0.1074) . So, we&#39;ll take the average seven and take the L1 norm (absolute mean difference) and compare that number with the number we just got (0.1074) . stacked_sevens = nums_tens[7].mean(0) show_image(stacked_sevens) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e0401510&gt; . mnist_distance(a_3, stacked_sevens) . tensor(0.1441) . As you can see, the distance between the 3 and the average 3 is smaller than the distance between the 3 and the average 7. So, it is more three than seven. We&#39;ll extend this approach by comparing an image with the average for each digit and say it is the digit it is the most similar to (its L1 norm with that average digit is the smallest). . We&#39;ll create the average number for each digit: . stacked_nums = [nums_tens[i].mean(0) for i in range(10)] show_image(stacked_nums[4]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e2b5e150&gt; . We can compare our 3 to each average digit: . L(mnist_distance(a_3, stacked_nums[i]) for i in range(10)) . (#10) [tensor(0.1750),tensor(0.1153),tensor(0.1501),tensor(0.1074),tensor(0.1635),tensor(0.1326),tensor(0.1579),tensor(0.1441),tensor(0.1345),tensor(0.1402)] . As you can see, it is most similar to the average three. . Now we&#39;ll import the validation set and put them into a list of tensors: . valid_nums = [(path/&#39;testing&#39;/f&#39;{i}&#39;).ls().sorted() for i in range(10)] . valid_nums_tens = [torch.stack([tensor(Image.open(j)) for j in valid_nums[i]]) for i in range(10)] valid_nums_tens = [valid_nums_tens[i].float()/255 for i in range(10)] . We&#39;ll create a function that returns the accuracy of our whole process: . def is_num(x1, x2s, x): # Get the distance between the number and the average digit for each digit vals = [mnist_distance(x1, x2s[i]) for i in range(10)] # Turn the tensors into floats so that we can perform the `min` function vals_2 = [[vals[i][j].item() for i in range(10)] for j in range(len(x1))] # Get a list of tensors that contain a bool value, where it&#39;s true when # the minimum distance is equal to the digit the given number is supposed # to be vals_3 = [tensor(vals_2[i].index(min(vals_2[i])) == x) for i in range(len(x1))] # Return how often our model is correct return tensor(vals_3).float().mean(0) . nums_accuracy = tensor([is_num(valid_nums_tens[i], stacked_nums, i) for i in range(10)]) nums_accuracy, nums_accuracy.mean(0) . (tensor([0.8153, 0.9982, 0.4234, 0.6089, 0.6680, 0.3262, 0.7871, 0.7646, 0.4425, 0.7760]), tensor(0.6610)) . Our model has an overall accuracy of 66.1%! Better than a random guess of 10%, but certainly not good. It is particularly good at guessing if a number is a 1, but particularly bad for 2s, 5s and 8s. . Now that we have a baseline, we can try how good we can get a simple model &quot;from scratch.&quot; . Basic neural network . For our &quot;from scratch&quot; learner, we&#39;ll have 2 layers, where each layer contains a linear layer and a ReLU (rectified linear unit, where all negative numbers become 0). . For our loss function, we will be using cross-entropy loss since we have multiple categories. . First, we&#39;ll make our training and validation datasets and dataloaders. Then, we&#39;ll initialize parameters, figure out how to make predictions, calculate the loss (cross-entropy), calculate the gradients, and then step (using the provided SGD optimizer). . Let&#39;s first redownload the MNIST dataset: . path = untar_data(URLs.MNIST) . Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)] . Then, we&#39;ll save the training and validation images into separate variables: . nums = [(path/&#39;training&#39;/f&#39;{x}&#39;).ls().sorted() for x in range(10)] . nums_tens = [torch.stack([tensor(Image.open(j)) for j in nums[i]]) for i in range(10)] nums_tens = [nums_tens[i].float()/255 for i in range(10)] . valid_nums = [(path/&#39;testing&#39;/f&#39;{i}&#39;).ls().sorted() for i in range(10)] . valid_nums_tens = [torch.stack([tensor(Image.open(j)) for j in valid_nums[i]]) for i in range(10)] valid_nums_tens = [valid_nums_tens[i].float()/255 for i in range(10)] . Next, we&#39;ll create our dataset from our training set. A dataset is a list of tuples, which contains the independent variable and its label (dependent variable) like so: (independent, dependent). . train_x = torch.cat(nums_tens).view(-1, 28*28) . It took a while for me to realize what the .view() function was doing, but what it does is pretty simple. We give it however many values we want (that makes sense) to change the shape of our tensor. Here we give it -1, 28*28 which will turn our rank-3 tensor (n-images of 28 by 28) into a rank-2 tensor (n-images of 28*28). -1 makes it so that we don&#39;t have to specify how many images there are and 28*28 means we want to compress our previous 28 by 28 grid into a 28*28 vector. It&#39;s like turning a 2D array into a 1D array: . # -1 makes it so that we don&#39;t have to know how many images there are nums_tens[0].view(-1, 28*28).shape, nums_tens[0].view(5923, 28*28).shape . (torch.Size([5923, 784]), torch.Size([5923, 784])) . # before we called .view(), our tensor was originally 28x28, but afterwards, it is 28*28 (784) nums_tens[0].size(), nums_tens[0].view(-1, 28*28).shape . (torch.Size([5923, 28, 28]), torch.Size([5923, 784])) . We&#39;ll form our labels by having as many tensors containing the digit&#39;s digit as there are of that digit: . train_y = torch.cat([tensor([i] * len(nums_tens[i])) for i in range(10)]) . train_x.shape, train_y.shape . (torch.Size([60000, 784]), torch.Size([60000])) . # when we take a random 3, we can index into the labels at the same spot and see # that we can 3 as its label show_image(nums_tens[3][200]), train_y[len(nums_tens[0]) + len(nums_tens[1]) + len(nums_tens[2]) +200] . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e2aa69d0&gt;, tensor(3)) . Like I said before, a dataset is just a list of tuples containing our independent and dependent variables: . dset = list(zip(train_x, train_y)) x, y = dset[0] x.shape, y . (torch.Size([784]), tensor(0)) . And we can see that given a label 0, our image is indeed a zero: . # we have to reshape our image from a 784 long vector into a 28*28 matrix show_image(x.view(28, 28)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1d9324110&gt; . Now we&#39;ll make the dataset for our validation set: . valid_x = torch.cat(valid_nums_tens).view(-1, 28*28) valid_y = torch.cat([tensor([i] * len(valid_nums_tens[i])) for i in range(10)]) valid_dset = list(zip(valid_x, valid_y)) . Next, we&#39;ll create DataLoaders for our training and validation sets. A DataLoader takes a dataset and each time we use it, it will give a portion of the dataset. We can then work on a portion of the dataset instead of just 1 tuple or the entire set. We can also toggle whether we want our given portion to be randomize (we wouldn&#39;t want to get all 0s, then 1s, then 2s, ... we want a mix): . dl = DataLoader(dset, batch_size = 128, shuffle = True) valid_dl = DataLoader(valid_dset, batch_size = 128, shuffle = True) xb, yb = first(dl) xb.shape, yb.shape . (torch.Size([128, 784]), torch.Size([128])) . Then, we&#39;ll create our DataLoaders. A DataLoaders is like the dataset of a DataLoader: it just contains our training and validation DataLoaders: . dls = DataLoaders(dl, valid_dl) . Our simple neural network uses PyTorch&#39;s nn.Sequential which takes modules and uses the GPU to handle the operations: . simple_net = nn.Sequential( # Our first layer takes in 28*28 inputs and outputs 250 nn.Linear(28 * 28, 250), nn.ReLU(), # Our second layer takes in 250 inputs and outputs 50 nn.Linear(250, 50), nn.ReLU(), # Our final layer takes in 50 inputs and outputs 10 # (its confidence for our image to be each digit) nn.Linear(50, 10) ) . We use cross-entropy loss so that we can turn our 10 outputs into numbers that are from 0 to 1 and sum to 1 like probabilities (through softmax). But, that&#39;s just the first part. We then take the negative log (-log(p)) of those probabilities to give emphasis on the higher probabilities. . We&#39;ll use the given Learner class from fastai (which handles epochs) with the SGD optimizer (stochastic gradient descent, which handles calculating gradients and stepping into lower loss) and use the accuracy metric (the number we care about). . learn = Learner(dls, simple_net, opt_func = SGD, loss_func = F.cross_entropy, metrics = accuracy) . We&#39;ll use the learning rate finder to select a good learning rate for us: . lrs = learn.lr_find() lrs . SuggestedLRs(valley=0.04786301031708717) . learn.fit(20, lr = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 0.454874 | 0.400172 | 0.889500 | 00:02 | . 1 | 0.322119 | 0.292965 | 0.916900 | 00:02 | . 2 | 0.264725 | 0.245524 | 0.929800 | 00:02 | . 3 | 0.228520 | 0.216811 | 0.938200 | 00:02 | . 4 | 0.196191 | 0.187768 | 0.945100 | 00:02 | . 5 | 0.181299 | 0.170362 | 0.950000 | 00:02 | . 6 | 0.151747 | 0.152564 | 0.954600 | 00:02 | . 7 | 0.143972 | 0.141233 | 0.957100 | 00:02 | . 8 | 0.125890 | 0.130209 | 0.961300 | 00:02 | . 9 | 0.119570 | 0.117828 | 0.964100 | 00:02 | . 10 | 0.112297 | 0.120144 | 0.963900 | 00:02 | . 11 | 0.096248 | 0.108321 | 0.967100 | 00:02 | . 12 | 0.085236 | 0.100657 | 0.970100 | 00:02 | . 13 | 0.082346 | 0.094316 | 0.970200 | 00:02 | . 14 | 0.077736 | 0.090774 | 0.972000 | 00:02 | . 15 | 0.075661 | 0.093964 | 0.971500 | 00:02 | . 16 | 0.064168 | 0.087111 | 0.973000 | 00:02 | . 17 | 0.062180 | 0.080836 | 0.975900 | 00:02 | . 18 | 0.061466 | 0.077446 | 0.977300 | 00:02 | . 19 | 0.052969 | 0.078910 | 0.975300 | 00:02 | . And we see that our final accuracy is 97.5%! Certainly better than the 66.1% we got from our pixel similarity approach. . To compare, here&#39;s the results using fastai&#39;s provided cnn_learner which uses a pretrained model with 18 layers: . dls2 = ImageDataLoaders.from_folder(path, train=&#39;training&#39;, valid=&#39;testing&#39;) learn2 = cnn_learner(dls2, resnet18, loss_func=F.cross_entropy, metrics=accuracy) learn2.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.124300 | 0.056642 | 0.982900 | 02:04 | . Our model is not bad considering it&#39;s less than 1% in accuracy different from a pretrained model. . We could even make our model better by training for more epochs until the validation loss becomes worse or by using a deeper model. .",
            "url": "https://geon-youn.github.io/DunGeon/vision/2022/02/20/MNIST.html",
            "relUrl": "/vision/2022/02/20/MNIST.html",
            "date": " • Feb 20, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! . My name is Geon and I am an 18 year old Korean-Canadian. I graduated Westdale S.S. in 2021 and am currently attending McMaster University for computer science. I started fast.ai when I was in 12th grade and here I am continuing to learn about deep learning and now I am starting a blog. .",
          "url": "https://geon-youn.github.io/DunGeon/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://geon-youn.github.io/DunGeon/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}