{
  
    
        "post0": {
            "title": "A Classifier on Pet Breeds",
            "content": "Training Our Own Pet Breed Classifier . First, we&#39;ll download the Pet dataset and see what we&#39;re given: . path = untar_data(URLs.PETS) Path.BASE_PATH = path path.ls() . . 100.00% [811712512/811706944 00:17&lt;00:00] (#2) [Path(&#39;images&#39;),Path(&#39;annotations&#39;)] . (path/&#39;images&#39;).ls() . (#7393) [Path(&#39;images/english_setter_69.jpg&#39;),Path(&#39;images/scottish_terrier_120.jpg&#39;),Path(&#39;images/basset_hound_113.jpg&#39;),Path(&#39;images/miniature_pinscher_87.jpg&#39;),Path(&#39;images/pomeranian_1.jpg&#39;),Path(&#39;images/Persian_68.jpg&#39;),Path(&#39;images/japanese_chin_39.jpg&#39;),Path(&#39;images/english_setter_107.jpg&#39;),Path(&#39;images/Birman_128.jpg&#39;),Path(&#39;images/staffordshire_bull_terrier_26.jpg&#39;)...] . In this dataset, there are two subfolders: images and annotations. images contains the images of the pet breeds (and their labels) while annotations contains the location of the pet in each image if you wanted to do localization. . The images are structured like so: the name of the pet breed with spaces turned into underscores, followed by a number. The name is capitalized if the pet is a cat. We can get the name of the pet breed by using regular expressions: . fname = (path/&#39;images&#39;).ls()[0] fname, fname.name . (Path(&#39;images/english_setter_69.jpg&#39;), &#39;english_setter_69.jpg&#39;) . # () = extract what&#39;s in the parentheses -&gt; .+ # .+ = any character appearing one or more times # _ = followed by an underscore # d+ = followed by any digit appearing one or more times # .jpg$ = with a .jpg extension at the end of the string re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;english_setter&#39;] . This time, we&#39;ll be using a DataBlock to create our DataLoaders . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = partial(get_image_files, folders = &#39;images&#39;), splitter = RandomSplitter(), get_y = using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms = Resize(460), batch_tfms = aug_transforms(size = 224, min_scale = 0.75)) dls = pets.dataloaders(path) . In our pets DataBlock, we give it the following parameters: . blocks = (ImageBlock, CategoryBlock): our independent variable is an image and our dependent variable is a category. | get_items = partial(get_image_files, folders = &#39;images&#39;): we are getting our images recursively in the images folder. If you&#39;ve used functional programming before, partial is like currying; we give a function some of its parameters and it returns another function that accepts the rest of its parameters, except partial allows us to specify which parameters we want to give. | splitter = RandomSplitter(): randomly splits our data into training and validation sets with a default 80:20 split. We can also specify a seed if we want to test how tuning our hyperparameters affects the final accuracy. | . The final two parameters are part of &quot;presizing&quot;: . item_tfms = Resize(460): picks a random area of an image (using its max width or height, whichever is smallest) and resizes it to 460x460. This process happens for all images in the dataset. | batch_tfms = aug_transforms(size = 224, min_scale = 0.75): take a random portion of the image which is at least 75% of it and resize to 224x224. This process happens for all images in a batch (like the batch we get when we call dls.one_batch()). | . We first resize an image to a much larger size than our actual size for training so that we can avoid the data destruction done by data augmentation. The larger size allows tranformation of the data without creating empty areas. . . We can check if our DataLoaders is created successfully by using the .show_batch() feature: . dls.show_batch(nrows = 1, ncols = 4) . We can then do some Googling to make sure our images are labelled correctly. . Fastai also allows us to debug our DataBlock in case we make an error. It attemps to create a batch from the source: . pets.summary(path) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_179.jpg applying PILBase.create gives PILImage mode=RGB size=500x334 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_179.jpg applying partial gives great_pyrenees applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(21) Final sample: (PILImage mode=RGB size=500x334, TensorCategory(21)) Collecting items from /root/.fastai/data/oxford-iiit-pet Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=500x334, TensorCategory(21)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(21)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(21)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) . . Now, let&#39;s get to training our model. This time, we&#39;ll be fine tuning a pretrained model. This process is called transfer learning, where we take a pretrained model and retrain it on our data so that it can perform well for our task. We randomize the head (last layer) of our model, freeze the parameters of the earlier layers and train our model for one epoch. Then, we unfreeze the model and update the later layers of the model with a higher learning rate than the earlier layers. . The pretrained model we will be using is resnet34, which was trained on the ImageNet dataset with 34 layers: . learner = cnn_learner(dls, resnet34, metrics = accuracy) . lrs = learner.lr_find() . learner.fit_one_cycle(3, lr_max = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 1.542125 | 0.296727 | 0.900541 | 01:14 | . 1 | 0.618474 | 0.227452 | 0.924222 | 01:13 | . 2 | 0.401809 | 0.214500 | 0.932341 | 01:12 | . learner.unfreeze() lrs = learner.lr_find() . learner.fit_one_cycle(6, lr_max = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 0.340459 | 0.213287 | 0.928281 | 01:16 | . 1 | 0.341917 | 0.233392 | 0.921516 | 01:16 | . 2 | 0.277254 | 0.187060 | 0.939107 | 01:16 | . 3 | 0.191343 | 0.192029 | 0.938430 | 01:16 | . 4 | 0.156336 | 0.178532 | 0.941813 | 01:16 | . 5 | 0.123608 | 0.174198 | 0.939107 | 01:16 | . After fine tuning for 2 epochs, we now have a model that can predict pet breeds accuractely 94% of the time. We can use fastai&#39;s confusion matrix to see where our model is having problems: . interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix(figsize = (12, 12), dpi = 60) . interp.most_confused(5) . [(&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 6), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 5), (&#39;chihuahua&#39;, &#39;miniature_pinscher&#39;, 5)] . Using the .most_confused feature, it seems like most of the errors come from the pet breeds being very similar. We should be careful however, that we aren&#39;t overfitting on our validation set through changing hyperparameters. We can see that our training loss is always going down, but our validation loss fluctuates from going down and sometimes up. . And that&#39;s all there is to training a pet breed classifier. You could improve the accuracy by exploring deeper models like resnet50 which has 50 layers; training for more epochs (whether before unfreezing or after or both); using discriminative learning rates (giving lower learning rates or early laters using split(lr1, lr2) in the lr_max key-word argument in fit_one_cycle). . Using Our Own Pet Breed Classifier . First, let&#39;s save the model using .export(): . learner.export() . Then, let&#39;s load the .pkl file: . learn = load_learner(&#39;export.pkl&#39;) . Create some basic UI: . def pretty(name: str) -&gt; str: return name.replace(&#39;_&#39;, &#39; &#39;).lower() . def classify(a): img = PILImage.create(btn_upload.data[-1]) pred, pred_idx, probs = learn.predict(img) out_pl.clear_output() with out_pl: display(img.to_thumb(128, 128)) lbl_pred.value = f&#39;Looks like a {pretty(pred)} to me. I &#39;m {probs[pred_idx] * 100:.02f}% confident!&#39; btn_upload = widgets.FileUpload() lbl_pred = widgets.Label() out_pl = widgets.Output() btn_run = widgets.Button(description = &#39;Classify&#39;) btn_run.on_click(classify) VBox([ widgets.Label(&#39;Upload a pet!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . And there we have it! You can make it prettier and go win a hackathon. . However, a bit of a downside with deep learning is that it can only predict what it has been trained on. So, drawings of pets, night-time images of pets, and breeds that weren&#39;t included in the training set won&#39;t be accurately labelled. . We could solve the last case by turning this problem into a multi-label classification problem. Then, if we aren&#39;t confident that we have one of the known breeds, we can just say we don&#39;t know this breed. . Siamese Pair . When I was watching the fastai lectures, I heard Jeremy talking about &quot;siamese pairs&quot; where you give the model two images and it will tell you if they are of the same breed. Now that we have a model, let&#39;s make it! . def pair(a): if not up1.data or not up2.data: lbl.value = &#39;Please upload images.&#39; return im1 = PILImage.create(up1.data[-1]) im2 = PILImage.create(up2.data[-1]) pred1, x, _ = learn.predict(im1) pred2, y, _ = learn.predict(im2) out1.clear_output() out2.clear_output() with out1: display(im1.to_thumb(128, 128)) with out2: display(im2.to_thumb(128, 128)) if x == y: lbl.value = f&#39;Wow, they &#39;re both {pretty(pred1)}(s)!&#39; else: lbl.value = f&#39;The first one seems to be {pretty(pred1)} while the second one is a(n) {pretty(pred2)}. I &#39;m not an expert, but they seem to be of different breeds, chief.&#39; up1 = widgets.FileUpload() up2 = widgets.FileUpload() lbl = widgets.Label() out1 = widgets.Output() out2 = widgets.Output() run = widgets.Button(description = &#39;Classify&#39;) run.on_click(pair) VBox([ widgets.Label(&quot;Siamese Pairs&quot;), HBox([up1, up2]), run, HBox([out1, out2]), lbl ]) .",
            "url": "https://geon-youn.github.io/DunGeon/2022/02/21/Pets.html",
            "relUrl": "/2022/02/21/Pets.html",
            "date": " • Feb 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Pixel Similarity vs. Basic Neural Net on the MNIST Dataset",
            "content": "Pixel Similarity . We will take the average of each digit to get its &quot;perfect&quot; version. Then, we compare an image to each of those perfect numbers and see which one is the most similar. . First, we&#39;ll download the MNIST dataset: . path = untar_data(URLs.MNIST) . The dataset is separated into training and testing subfolders, where in those folders, there are separate folders for each digit: . Path.BASE_PATH = path . path.ls(),(path/&#39;training&#39;).ls() . ((#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)], (#10) [Path(&#39;training/7&#39;),Path(&#39;training/8&#39;),Path(&#39;training/1&#39;),Path(&#39;training/6&#39;),Path(&#39;training/3&#39;),Path(&#39;training/2&#39;),Path(&#39;training/5&#39;),Path(&#39;training/4&#39;),Path(&#39;training/0&#39;),Path(&#39;training/9&#39;)]) . We&#39;ll store the path of each image in an array, where the ith row contains the path for the ith digit: . nums = [(path/&#39;training&#39;/f&#39;{x}&#39;).ls().sorted() for x in range(10)] . im3_path = nums[3][0] im3 = Image.open(im3_path) im3 . Then, we&#39;ll open the images, put every image of the same digit into their own tensor and store them as a list of tensors: . nums_tens = [torch.stack([tensor(Image.open(j)) for j in nums[i]]) for i in range(10)] nums_tens = [nums_tens[i].float()/255 for i in range(10)] . We can then take the mean of one of the tensors to get its &quot;perfect&quot; version. Here is how it looks like for a 3: . stacked_threes = nums_tens[3].mean(0) show_image(stacked_threes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e3d3acd0&gt; . And to compare, here is just one of those threes: . a_3 = nums_tens[3][0] show_image(a_3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e36f1750&gt; . Next, we&#39;ll create a function that compares two tensors through absolute mean difference: . def mnist_distance(x1, x2): return (x1 - x2).abs().mean((-1, -2)) . Now we can compare one of the threes with its &quot;perfect&quot; version. The number doesn&#39;t really mean anything until we compare it with another number: . mnist_distance(a_3, stacked_threes) . tensor(0.1074) . So, we&#39;ll take the average seven and take the L1 norm (absolute mean difference) and compare that number with the number we just got (0.1074) . stacked_sevens = nums_tens[7].mean(0) show_image(stacked_sevens) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e0401510&gt; . mnist_distance(a_3, stacked_sevens) . tensor(0.1441) . As you can see, the distance between the 3 and the average 3 is smaller than the distance between the 3 and the average 7. So, it is more three than seven. We&#39;ll extend this approach by comparing an image with the average for each digit and say it is the digit it is the most similar to (its L1 norm with that average digit is the smallest). . We&#39;ll create the average number for each digit: . stacked_nums = [nums_tens[i].mean(0) for i in range(10)] show_image(stacked_nums[4]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e2b5e150&gt; . We can compare our 3 to each average digit: . L(mnist_distance(a_3, stacked_nums[i]) for i in range(10)) . (#10) [tensor(0.1750),tensor(0.1153),tensor(0.1501),tensor(0.1074),tensor(0.1635),tensor(0.1326),tensor(0.1579),tensor(0.1441),tensor(0.1345),tensor(0.1402)] . As you can see, it is most similar to the average three. . Now we&#39;ll import the validation set and put them into a list of tensors: . valid_nums = [(path/&#39;testing&#39;/f&#39;{i}&#39;).ls().sorted() for i in range(10)] . valid_nums_tens = [torch.stack([tensor(Image.open(j)) for j in valid_nums[i]]) for i in range(10)] valid_nums_tens = [valid_nums_tens[i].float()/255 for i in range(10)] . We&#39;ll create a function that returns the accuracy of our whole process: . def is_num(x1, x2s, x): # Get the distance between the number and the average digit for each digit vals = [mnist_distance(x1, x2s[i]) for i in range(10)] # Turn the tensors into floats so that we can perform the `min` function vals_2 = [[vals[i][j].item() for i in range(10)] for j in range(len(x1))] # Get a list of tensors that contain a bool value, where it&#39;s true when # the minimum distance is equal to the digit the given number is supposed # to be vals_3 = [tensor(vals_2[i].index(min(vals_2[i])) == x) for i in range(len(x1))] # Return how often our model is correct return tensor(vals_3).float().mean(0) . nums_accuracy = tensor([is_num(valid_nums_tens[i], stacked_nums, i) for i in range(10)]) nums_accuracy, nums_accuracy.mean(0) . (tensor([0.8153, 0.9982, 0.4234, 0.6089, 0.6680, 0.3262, 0.7871, 0.7646, 0.4425, 0.7760]), tensor(0.6610)) . Our model has an overall accuracy of 66.1%! Better than a random guess of 10%, but certainly not good. It is particularly good at guessing if a number is a 1, but particularly bad for 2s, 5s and 8s. . Now that we have a baseline, we can try how good we can get a simple model &quot;from scratch.&quot; . Basic Neural Net . For our &quot;from scratch&quot; learner, we&#39;ll have 2 layers, where each layer contains a linear layer and a ReLU (rectified linear unit, where all negative numbers become 0). . For our loss function, we will be using cross-entropy loss since we have multiple categories. . First, we&#39;ll make our training and validation datasets and dataloaders. Then, we&#39;ll initialize parameters, figure out how to make predictions, calculate the loss (cross-entropy), calculate the gradients, and then step (using the provided SGD optimizer). . Let&#39;s first redownload the MNIST dataset: . path = untar_data(URLs.MNIST) . Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)] . Then, we&#39;ll save the training and validation images into separate variables: . nums = [(path/&#39;training&#39;/f&#39;{x}&#39;).ls().sorted() for x in range(10)] . nums_tens = [torch.stack([tensor(Image.open(j)) for j in nums[i]]) for i in range(10)] nums_tens = [nums_tens[i].float()/255 for i in range(10)] . valid_nums = [(path/&#39;testing&#39;/f&#39;{i}&#39;).ls().sorted() for i in range(10)] . valid_nums_tens = [torch.stack([tensor(Image.open(j)) for j in valid_nums[i]]) for i in range(10)] valid_nums_tens = [valid_nums_tens[i].float()/255 for i in range(10)] . Next, we&#39;ll create our dataset from our training set. A dataset is a list of tuples, which contains the independent variable and its label (dependent variable) like so: (independent, dependent). . train_x = torch.cat(nums_tens).view(-1, 28*28) . It took a while for me to realize what the .view() function was doing, but what it does is pretty simple. We give it however many values we want (that makes sense) to change the shape of our tensor. Here we give it -1, 28*28 which will turn our rank-3 tensor (n-images of 28 by 28) into a rank-2 tensor (n-images of 28*28). -1 makes it so that we don&#39;t have to specify how many images there are and 28*28 means we want to compress our previous 28 by 28 grid into a 28*28 vector. It&#39;s like turning a 2D array into a 1D array: . # -1 makes it so that we don&#39;t have to know how many images there are nums_tens[0].view(-1, 28*28).shape, nums_tens[0].view(5923, 28*28).shape . (torch.Size([5923, 784]), torch.Size([5923, 784])) . # before we called .view(), our tensor was originally 28x28, but afterwards, it is 28*28 (784) nums_tens[0].size(), nums_tens[0].view(-1, 28*28).shape . (torch.Size([5923, 28, 28]), torch.Size([5923, 784])) . We&#39;ll form our labels by having as many tensors containing the digit&#39;s digit as there are of that digit: . train_y = torch.cat([tensor([i] * len(nums_tens[i])) for i in range(10)]) . train_x.shape, train_y.shape . (torch.Size([60000, 784]), torch.Size([60000])) . # when we take a random 3, we can index into the labels at the same spot and see # that we can 3 as its label show_image(nums_tens[3][200]), train_y[len(nums_tens[0]) + len(nums_tens[1]) + len(nums_tens[2]) +200] . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e2aa69d0&gt;, tensor(3)) . Like I said before, a dataset is just a list of tuples containing our independent and dependent variables: . dset = list(zip(train_x, train_y)) x, y = dset[0] x.shape, y . (torch.Size([784]), tensor(0)) . And we can see that given a label 0, our image is indeed a zero: . # we have to reshape our image from a 784 long vector into a 28*28 matrix show_image(x.view(28, 28)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1d9324110&gt; . Now we&#39;ll make the dataset for our validation set: . valid_x = torch.cat(valid_nums_tens).view(-1, 28*28) valid_y = torch.cat([tensor([i] * len(valid_nums_tens[i])) for i in range(10)]) valid_dset = list(zip(valid_x, valid_y)) . Next, we&#39;ll create DataLoaders for our training and validation sets. A DataLoader takes a dataset and each time we use it, it will give a portion of the dataset. We can then work on a portion of the dataset instead of just 1 tuple or the entire set. We can also toggle whether we want our given portion to be randomize (we wouldn&#39;t want to get all 0s, then 1s, then 2s, ... we want a mix): . dl = DataLoader(dset, batch_size = 128, shuffle = True) valid_dl = DataLoader(valid_dset, batch_size = 128, shuffle = True) xb, yb = first(dl) xb.shape, yb.shape . (torch.Size([128, 784]), torch.Size([128])) . Then, we&#39;ll create our DataLoaders. A DataLoaders is like the dataset of a DataLoader: it just contains our training and validation DataLoaders: . dls = DataLoaders(dl, valid_dl) . Our simple neural network uses PyTorch&#39;s nn.Sequential which takes modules and uses the GPU to handle the operations: . simple_net = nn.Sequential( # Our first layer takes in 28*28 inputs and outputs 250 nn.Linear(28 * 28, 250), nn.ReLU(), # Our second layer takes in 250 inputs and outputs 50 nn.Linear(250, 50), nn.ReLU(), # Our final layer takes in 50 inputs and outputs 10 # (its confidence for our image to be each digit) nn.Linear(50, 10) ) . We use cross-entropy loss so that we can turn our 10 outputs into numbers that are from 0 to 1 and sum to 1 like probabilities (through softmax). But, that&#39;s just the first part. We then take the negative log (-log(p)) of those probabilities to give emphasis on the higher probabilities. . We&#39;ll use the given Learner class from fastai (which handles epochs) with the SGD optimizer (stochastic gradient descent, which handles calculating gradients and stepping into lower loss) and use the accuracy metric (the number we care about). . learn = Learner(dls, simple_net, opt_func = SGD, loss_func = F.cross_entropy, metrics = accuracy) . We&#39;ll use the learning rate finder to select a good learning rate for us: . lrs = learn.lr_find() lrs . SuggestedLRs(valley=0.04786301031708717) . learn.fit(20, lr = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 0.454874 | 0.400172 | 0.889500 | 00:02 | . 1 | 0.322119 | 0.292965 | 0.916900 | 00:02 | . 2 | 0.264725 | 0.245524 | 0.929800 | 00:02 | . 3 | 0.228520 | 0.216811 | 0.938200 | 00:02 | . 4 | 0.196191 | 0.187768 | 0.945100 | 00:02 | . 5 | 0.181299 | 0.170362 | 0.950000 | 00:02 | . 6 | 0.151747 | 0.152564 | 0.954600 | 00:02 | . 7 | 0.143972 | 0.141233 | 0.957100 | 00:02 | . 8 | 0.125890 | 0.130209 | 0.961300 | 00:02 | . 9 | 0.119570 | 0.117828 | 0.964100 | 00:02 | . 10 | 0.112297 | 0.120144 | 0.963900 | 00:02 | . 11 | 0.096248 | 0.108321 | 0.967100 | 00:02 | . 12 | 0.085236 | 0.100657 | 0.970100 | 00:02 | . 13 | 0.082346 | 0.094316 | 0.970200 | 00:02 | . 14 | 0.077736 | 0.090774 | 0.972000 | 00:02 | . 15 | 0.075661 | 0.093964 | 0.971500 | 00:02 | . 16 | 0.064168 | 0.087111 | 0.973000 | 00:02 | . 17 | 0.062180 | 0.080836 | 0.975900 | 00:02 | . 18 | 0.061466 | 0.077446 | 0.977300 | 00:02 | . 19 | 0.052969 | 0.078910 | 0.975300 | 00:02 | . And we see that our final accuracy is 97.5%! Certainly better than the 66.1% we got from our pixel similarity approach. . To compare, here&#39;s the results using fastai&#39;s provided cnn_learner which uses a pretrained model with 18 layers: . dls2 = ImageDataLoaders.from_folder(path, train=&#39;training&#39;, valid=&#39;testing&#39;) learn2 = cnn_learner(dls2, resnet18, loss_func=F.cross_entropy, metrics=accuracy) learn2.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.124300 | 0.056642 | 0.982900 | 02:04 | . Our model is not bad considering it&#39;s less than 1% in accuracy different from a pretrained model. . We could even make our model better by training for more epochs until the validation loss becomes worse or by using a deeper model. .",
            "url": "https://geon-youn.github.io/DunGeon/2022/02/20/MNIST.html",
            "relUrl": "/2022/02/20/MNIST.html",
            "date": " • Feb 20, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! . My name is Geon and I am an 18 year old Korean-Canadian. I graduated Westdale S.S. in 2021 and am currently attending McMaster University for computer science. I started fast.ai when I was in 12th grade and here I am continuing to learn about deep learning and now I am starting a blog. .",
          "url": "https://geon-youn.github.io/DunGeon/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://geon-youn.github.io/DunGeon/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}