{
  
    
        "post0": {
            "title": "Predicting the Sentiment of Movie Reviews with NLP",
            "content": "Downloading the data set . If you didn&#39;t know, I work on Colab; I don&#39;t own a GPU that I can use to train models on my own. Whenever I download data sets, I end up having to redownload them when I come back on the next day. This time, before we get into the main topic, I want to show how I&#39;ve been able to download the data sets into my Google drive so that I don&#39;t have to redownload them every time. It&#39;s also helpful since I can save the pickles to the same folder instead of saving them on my laptop and having to upload them later. . It&#39;s really simple too: you mount onto your drive and then cd to your desired folder. When you download data sets, you download them into that folder. But, if you use untar_data, you have to move the files to the folder since it only saves to ~/.fastai/.... . !mkdir -p &#39;/content/gdrive/MyDrive/data_sets/&#39; %cd &#39;/content/gdrive/MyDrive/data_sets/&#39; . /content/gdrive/MyDrive/data_sets . There&#39;s also a difference between ! and % in Colab: ! is like a &quot;no side-effect&quot; way to do command-line commands. !mkdir ... will make a directory, and !cd ... will change directories, except it doesn&#39;t, because that would be a side effect. So, to actually change the current directory that Colab is in, you have to use %. You can think of % as being able to change the variables stored in Colab; hypothetically %cd path makes Colab&#39;s CURRENT_PATH = path. . from fastai.text.all import * . path = untar_data(URLs.IMDB_SAMPLE); path . . 100.28% [573440/571827 00:00&lt;00:00] Path(&#39;/root/.fastai/data/imdb_sample&#39;) . !mv {str(path)} &#39;/content/gdrive/MyDrive/data_sets/imdb_sample&#39; . path = Path(&#39;/content/gdrive/MyDrive/data_sets/imdb_sample&#39;) . !ls {str(path)} . texts.csv . Preparing the data for the language model . To get the data from the data set, we&#39;ll use fastai&#39;s get_text_files as opposed to get_image_files that we did for computer vision problems. By specifying a list for folders, get_text_files will only look for text files in those folders. . df = pd.read_csv(path/&#39;texts.csv&#39;, low_memory=False) df.head(3) . label text is_valid . 0 negative | Un-bleeping-believable! Meg Ryan doesn&#39;t even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff! | False | . 1 positive | This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som... | False | . 2 negative | Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li... | False | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Then, we&#39;ll go straight into putting it into a DataBlock from which we can make our DataLoaders: . dls_lm = DataBlock( blocks=TextBlock.from_df([&#39;text&#39;], is_lm=True), get_items=ColReader(&#39;text&#39;), splitter=RandomSplitter(0.1)).dataloaders(df, path=path) . When we show a batch, you can see how the independent variable contains one more token before the dependent variable, but has one less token at the end. Fastai also has special tokens that it makes through TextBlock (which uses the Tokenizer class): . xxbos: beginning of stream. | xxeos: end of stream. | xxmaj: next token is capitalized. | xxunk: unknown (we ignore tokens that don&#39;t appear enough to minimize the size of the embedding matrix). | xxup: next token is all uppercase. | xxrep followed by a number n: the next actual token repeats n times. | . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos xxmaj even though the book was n&#39;t strictly accurate to the real situation it described it still carried a sense of xxmaj japan . i find it hard to believe that anyone who was involved in making this film had ever been to japan as it did n&#39;t feel xxmaj japanese in the slightest . xxmaj almost everything about it was terrible . i will admit the actors were generally quite | xxmaj even though the book was n&#39;t strictly accurate to the real situation it described it still carried a sense of xxmaj japan . i find it hard to believe that anyone who was involved in making this film had ever been to japan as it did n&#39;t feel xxmaj japanese in the slightest . xxmaj almost everything about it was terrible . i will admit the actors were generally quite good | . 1 ; &quot; the xxmaj return &quot; ( xxunk xxmaj xxunk , 2003 ) ; &quot; little xxmaj xxunk &quot; ( james xxmaj xxunk ) ; xxmaj xxunk , &quot; xxunk and xxmaj sons &quot; ; and , of course , xxmaj xxunk , &quot; the xxmaj brothers xxmaj xxunk . &quot; n n xxmaj credits in xxmaj english indicate intended international xxunk , meaning that the excuse can not be used that you | &quot; the xxmaj return &quot; ( xxunk xxmaj xxunk , 2003 ) ; &quot; little xxmaj xxunk &quot; ( james xxmaj xxunk ) ; xxmaj xxunk , &quot; xxunk and xxmaj sons &quot; ; and , of course , xxmaj xxunk , &quot; the xxmaj brothers xxmaj xxunk . &quot; n n xxmaj credits in xxmaj english indicate intended international xxunk , meaning that the excuse can not be used that you have | . In general, Tokenizer does a lot of extra things on top of tokenizing the texts: . defaults.text_proc_rules . [&lt;function fastai.text.core.fix_html&gt;, &lt;function fastai.text.core.replace_rep&gt;, &lt;function fastai.text.core.replace_wrep&gt;, &lt;function fastai.text.core.spec_add_spaces&gt;, &lt;function fastai.text.core.rm_useless_spaces&gt;, &lt;function fastai.text.core.replace_all_caps&gt;, &lt;function fastai.text.core.replace_maj&gt;, &lt;function fastai.text.core.lowercase&gt;] . Where . fix_html: replace special HTML characters with a readable version. | replace_rep: replace characters repeated three or more times with xxrep n c where xxrep is the special token, n is the number of times it repeats, and c is the character. | replace_wrep: replace words repeated three or more times with xxwrep n w where w is the word. | spec_add_spaces: add spaces around special characters like / and # so they get split into separate tokens. | rm_useless_spaces: remove repeated spaces. | replace_all_caps: lowercase a word in all caps and add a xxup before it. | replace_maj: lowercase a capitalized word and add a xxmaj before it. | lowercase: lowercase all words and add a xxbox to the beginning and/or xxeos to the end of the string. | . To show most of it in action: . tok = WordTokenizer() tkn = Tokenizer(tok) list(tkn(&#39;amp; &amp;copy; #a&quot; &lt;br /&gt; www.google.ca/INDEX.html wow wow wow&#39;)) . [&#39;xxbos&#39;, &#39;&amp;&#39;, &#39;©&#39;, &#39;#&#39;, &#39;a&#39;, &#39;&#34;&#39;, &#39; n &#39;, &#39;xxrep&#39;, &#39;3&#39;, &#39;w&#39;, &#39;.google.ca&#39;, &#39;/&#39;, &#39;index.html&#39;, &#39;xxwrep&#39;, &#39;3&#39;, &#39;wow&#39;] . Fine-tuning the language model . We&#39;ll be fine-tuning a language model pretrained using text from Wikepedia. This model uses a recurrent neural network using the AWD-LSTM architecture. . learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]) . Since pretrained is set to True for our language_model_learner by default, all the layers except the head are frozen. So, we&#39;ll first train the head of the model for one epoch: . lr = learn.lr_find().valley . learn.fit_one_cycle(1, lr) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.127933 | 3.944165 | 0.272763 | 51.633183 | 00:57 | . Then, we&#39;ll unfreeze all the layers and train all the layers for another epoch: . lr = learn.lr_find().valley . learn.unfreeze() learn.fit_one_cycle(1, lr) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.904036 | 3.806237 | 0.281909 | 44.980846 | 01:04 | . We&#39;ll save the model as an encoder, which is the body of the model (the model not including the task-specific final layer(s)): . learn.save_encoder(&#39;lm&#39;) . learn.predict(&quot;I liked the book better&quot;, 25, temperature=0.75) . &#34;i liked the book better , i had more movie time , so i was surprised , but i do n&#39;t like it . While i essentially liked this&#34; . Fine-tuning the language model for a classifier . Unlike language models, which are self-supervised, we have to provide our DataLoaders for our classifier with the labels: . dls_c = DataBlock( blocks =(TextBlock.from_df(&#39;text&#39;, vocab=dls_lm.vocab), CategoryBlock), get_x =ColReader(&#39;text&#39;), get_y =ColReader(&#39;label&#39;), splitter =ColSplitter() ).dataloaders(df, path=path) . This time, we have to specify our blocks differently such that our independent variable is a TextBlock and our dependent variable is a CategoryBlock. So, when show a part of a batch, it&#39;ll look different from a language model&#39;s DataLoaders: . dls_c.show_batch(max_n=2) . text category . 0 xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review n n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it &#39;s warm and xxunk , but you &#39;re not sure if it feels right . xxmaj try as i might , no matter how warm and xxunk xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n&#39;t quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director &#39;s part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n&#39;t fool me . xxmaj raising xxmaj victor xxmaj vargas is | negative | . 1 xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there &#39;s just no getting around that , and it &#39;s hard to actually put one &#39;s feeling for this film into words . xxmaj it &#39;s not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot … which is highly believable to xxunk . xxmaj it &#39;s easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n&#39;t often get from other romantic comedies | positive | . Next, we&#39;ll define a Learner by using the text_classifier_learner class and load the encoder. . learn = text_classifier_learner(dls_c, AWD_LSTM, drop_mult=0.5, metrics=accuracy) . learn = learn.load_encoder(&#39;lm&#39;) . When fine-tuning a language model for a text classifier, it&#39;s recommended to use discriminative learning rates and gradual unfreezing. So, we&#39;ll first fine-tune it for 1 epoch: . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.587637 | 0.625740 | 0.655000 | 00:16 | . Then unfreeze the last 2 layers: . den = 2.6**4 learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/den, 1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.445948 | 0.522101 | 0.715000 | 00:19 | . Unfreeze up to the last 3 layers: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/den, 5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.306446 | 0.478002 | 0.780000 | 00:28 | . And unfreeze the entire model: . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/den, 1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.208012 | 0.417342 | 0.795000 | 00:36 | . 1 | 0.193315 | 0.410684 | 0.805000 | 00:36 | . And train for a few more epochs: . learn.fit_one_cycle(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.162276 | 0.398424 | 0.820000 | 00:36 | . 1 | 0.146796 | 0.461882 | 0.790000 | 00:36 | . 2 | 0.114655 | 0.511482 | 0.835000 | 00:36 | . 3 | 0.092193 | 0.437872 | 0.825000 | 00:35 | . 4 | 0.074616 | 0.423975 | 0.835000 | 00:36 | . And we&#39;re done! We can try predicting with a few of Morbius&#39; reviews: . [learn.predict(i) for i in [ &quot;So many choices... just seem halfhearted. They&#39;re not terrible, but they&#39;re also not interesting.&quot;, &quot;Doesn&#39;t deliver a lot of high notes, but it&#39;s not unwatchable (especially if you enjoy the vampire genre). &quot;Morbius &quot; is not as great as you&#39;d hoped, but not as bad as you feared.&quot;, &quot;Overall, this is an entertaining movie. It sucks you in with the action and standout scenes between some of the characters. But, while Morbius has the makings of a great anti-hero revival, it wavers with the execution.&quot;, &quot;Like the Venom films and unlike the MCU movies Morbius eschews the mythmaking of the Avengers for darkly comic horror.&quot;, &quot;Sadly, director Daniel Espinosa&#39;s action horror is itself drained of atmosphere, shocks and drama.&quot; ]] . [(&#39;negative&#39;, TensorText(0), TensorText([0.9590, 0.0410])), (&#39;negative&#39;, TensorText(0), TensorText([0.8723, 0.1277])), (&#39;positive&#39;, TensorText(1), TensorText([0.0055, 0.9945])), (&#39;positive&#39;, TensorText(1), TensorText([0.0279, 0.9721])), (&#39;negative&#39;, TensorText(0), TensorText([0.5976, 0.4024]))] . Conclusion . In this blog, I applied the ULMFiT approach to NLP. I fine-tuned the pretrained language model that was trained on Wikepedia pages with the smaller version of the IMDB data set to create a language model that can generate text relevant to movie reviews. Then, I used the encoder from that model to train a movie review sentiment classifier with an accuracy of 83.5%. For a model that was only trained with 1000 reviews, it&#39;s not that bad! . For next steps, I&#39;d like to train a model using the full IMDB data set, but I don&#39;t have the necessary hardware to do it in a reasonable time. When I tried training a Learner on the full data set, the estimated time for the first epoch was two and a half hours. Maybe in the future when I pay for a GPU server or own my own setup. .",
            "url": "https://geon-youn.github.io/DunGeon/nlp/2022/04/08/Movie-Review-Sentiment.html",
            "relUrl": "/nlp/2022/04/08/Movie-Review-Sentiment.html",
            "date": " • Apr 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "So you wanna learn natural language processing",
            "content": "How to train your NLP model (ULMFit) . With NLP (natural language processing), we can use transfer learning to train an NLP model using a pretrained language model. By using a pretrained language model, you spend less time, data, and money. But, unlike computer vision, we don&#39;t need a model that was trained on similar data. . A language model is a model that was trained to predict the next word given a sequence of text. We train these models through self-supervised learning, where we don&#39;t give any labels, just a lot of text. The model can automatically (thus self-supervised) create the labels from the text and be trained on it to develop an understanding of the text&#39;s language. . So, you have a pretrained language model, but it isn&#39;t the best idea to train an NLP model right away. The language model probably doesn&#39;t know the vocabulary and style (like grammar, formality, etc.) of your problem domain. So, you first fine-tune the model on the corpus of your problem domain, then fine-tune that new model to train your NLP model. . With this method, we can fine-tune the language model on both the text from the training and validation (and maybe test) sets which will make the new language model very good at predicting the next word for your problem domain. . This process of having a language model, fine-tuning it on your corpus, and then fine-tuning that for an NLP model is called the Universal Language Model Fine-tuning (ULMFit) approach. . Training your NLP (classifier) model . In training an NLP model, we first have to fine-tune our language model. To do so, we need to preprocess the text such that it&#39;s ready to be put into a model. . Preprocessing the text . Text is a categorical variable. And, to put it inside of a neural network, we have to assign them an embedding matrix. This process is the first layer of the neural network: turning a cateogrical variable continuous through an embedding matrix assignment. . With regular categorical variables, we: . Make a list of all possible levels of that categorical variable (which we call the vocab). | Replace each level with its index in the vocab. | Make an embedding matrix for this categorical variable where each row corresponds to a level. | Use this embedding matrix as the first layer of a neural network. This layer takes the index $i$ created in step 2 and returns the $i$-th row in the matrix. | For text, the first step is a little different since there&#39;re many ways we can define levels for text. It also works differently for different languages. This process is called tokenization, where each item in the vocab is called a token. The second step, where we assign numbers to each token is called numericalization. . When we use a pretrained model, our new vocab will contain words that were in the old model, but also some that weren&#39;t. We&#39;ll keep the corresponding row in the embedding matrix for words that exist in the pretrained model and initialize a random vector in rows corresponding to new words. . In tokenizing a given corpus, there are three main methods: . Words: Split a sentence at every space and puntuation (like apostrophes) to create words and contractions. | Subwords: Split words into subwords based on the most commonly occurring substrings. | Characters: Split a sentence into characters. | When do we use which? Word tokenizers assume that spaces are special separators in a sentence. While this is usually correct for English, other languages like Chinese and Japanese that don&#39;t really have spaces are better off with subword and character tokenizers. And, when spaces are special, but the languages uses many subwords like in Turkish and Hungarian, it would be better to use subword tokenizers than word tokenizers. Lastly, when a language has many characters (unlike 26 in English) like Chinese, it may be better to use character tokenizers. . You want to be careful to not have too many items in your vocab. For subword, you have the positive that there&#39;ll be fewer tokens in each sentence, and thus have faster training, less memory, and less state for the model to remember. But in general, a larger vocab leads to larger embedding matrices, which require more data to learn, take longer and require more GPU memory to train. . Once we have our vocab, we can convert every token in the corpus into a number that represents its index in the vocab. . Then, we have to make our independent and dependent variables for our DataSet object (which is just a wrapper class for a tuple (independent, dependent)). . For a language model, we want it to be able to predict the next word in a sequence of words. So, given a sequence of words, we want our independent variable to be from the first word of the sequence to the second last word. Then, our dependent variable will be from the second word of the sequence to the last word. . We&#39;d also be dividing the text into small pieces while maintaining order (otherwise our model would just predict random words instead of the next word in the sequence). . At every epoch, we shuffle our collection of documents and concatenate them into a stream of tokens. Then, we cut that stream into a batch of fixed-size consecutive mini-streams. The model then reads the mini-streams in order and learns to predict the next word for each independent variable. . Unlike with images, the key thing in NLP is that we randomize the documents (blocks of text) but we always have to maintain order of the words in each document. . Fine-tuning the language model . When we&#39;re fine-tuning the pretrained language model, we use a recurrent neural network (unlike convolutional neural network for vision) and use the AWD-LSTM architecture. For our loss function, we use cross-entropy loss since (almost all) NLP problems are classification problems where the different categories are the words in the vocab. Finally, for our metrics we&#39;ll use accuracy (since cross-entropy is difficult to interpret and speaks more for the confidence of our model than its accuracy) and perplexity (which is the exponent of the loss). . If we don&#39;t want to train a text classifier and instead just wanted a language model, we stop here and end up with a text generator. If you add some randomness (so you don&#39;t get the same prediction twice), you can generate many different kinds of text given the first few words. . Otherwise, you&#39;ll use this fine-tuned language model to train a text classifier. . Fine-tuning the text classifier . Unlike with a language model, when making our DataLoaders, our independent variable will be the text, while our dependent variable must be supplied. And, when trying to make a mini-batch, the tensors have to be the same shape. So, we sort the text by token length before each epoch and for every mini-batch, pad every text to be the same token length as the text with the largest token length in the mini-batch. By &quot;pad&quot;, we add a special padding token that&#39;ll be ignored by the model. . Then, we fine-tune our fine-tuned language model by training it with discriminative learning rates and gradual unfreezing. . In the end, you have a language model that can generate text related to your problem domain and a text classifier that can classify text in your problem domain with certain labels. . Conclusion . With NLP, there&#39;s a lot of fine-tuning. By ULMFit, you start with a pretrained language model that could have been trained on a really big data set like Wikipedia, you fine-tune it on your own text to have a language model that can generate text really well for your problem domain, then you fine-tune that language model to train a text classifier. .",
            "url": "https://geon-youn.github.io/DunGeon/nlp/2022/04/05/Natural-Language-Processing.html",
            "relUrl": "/nlp/2022/04/05/Natural-Language-Processing.html",
            "date": " • Apr 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Just The Basics - Strata 2013 | predicting spam emails",
            "content": "Hello! I wanted to move onto natrual language processing by next week, but I also wanted some more practice with tabular data sets. And, what better way than to use a data set that&#39;s related to natural language: emails. . This Kaggle competition ran 9 years ago and is one of the &quot;getting started&quot; data sets. This data set contains &quot;100 features extracted from a corpus of emails. Some of the emails are spam and some are normal. [Our] task is to make a spam detector.&quot; . I&#39;ve already downloaded the data set and we can first import them: . xs = pd.read_csv(&#39;train.csv&#39;, low_memory=False) y = pd.read_csv(&#39;train_labels.csv&#39;, low_memory=False) test = pd.read_csv(&#39;test.csv&#39;, low_memory=False) . Then, we&#39;ll define functions for our random forest trainer and the metric that this competition requires, which is the area under the ROC curve. . def rf(xs, y, n_estimators=40, min_samples_leaf=5, max_samples=300, max_features=0.5): return RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_samples=max_samples, max_features=max_features, oob_score=True, n_jobs=-1).fit(xs, y) . def a_uc(preds, y): fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=1) return metrics.auc(fpr, tpr) def m_auc(m, xs, y): return a_uc(m.predict(xs), y) . In making our TabularPandas, we&#39;ll merge the independent and dependent variables. But, unlike before we&#39;ll be using a randomized split since this isn&#39;t a time series. . df_merged = pd.concat((xs, y), axis=1).copy() . procs = [Categorify, FillMissing, Normalize] . cont, cat = cont_cat_split(df_merged, dep_var=&#39;0&#39;) . tp = TabularPandas(df_merged, procs, cat_names=cat, cont_names=cont, y_names=&#39;0&#39;, splits=RandomSplitter()(xs)) . t_xs, v_xs, t_y, v_y = tp.train.xs, tp.valid.xs, tp.train.y, tp.valid.y . I&#39;m not sure how to use TabularPandas that well since there&#39;s no categorical columns in this data set, but it seems like TabularPandas requires it. If there aren&#39;t any, then it duplicates all the columns, so we remove them here: . t_xs = t_xs.drop(tp.train.x_names[0:100], axis=1) . v_xs = v_xs.drop(tp.valid.x_names[0:100], axis=1) . Then, we train a decision tree as a baseline: . dt = DecisionTreeRegressor(min_samples_leaf=40).fit(t_xs, t_y) . m_auc(dt, t_xs, t_y), m_auc(dt, v_xs, v_y) . (0.9117870857001292, 0.9124478856462179) . Surprisingly, the baseline is already at 0.912. I&#39;m also not sure if this metric is equivalent to accuracy in terms of saying it&#39;s 91.2% accurate, so I&#39;ll just refer to it as 0.912. . Next, let&#39;s train a random forest model: . m = rf(t_xs, t_y, min_samples_leaf=10, n_estimators=120) . m_auc(m, t_xs, t_y), m_auc(m, v_xs, v_y) . (0.9765178460830635, 0.9401429422275164) . And then, we&#39;ll train a neural network: . dls = tp.dataloaders(128) . Again, we&#39;re dropping the duplicated columns from TabularPandas: . dls.train.xs = dls.train.xs.drop(columns=dls.train.x_names[0:100]) dls.valid.xs = dls.valid.xs.drop(columns=dls.valid.x_names[0:100]) . learn = tabular_learner(dls, y_range=(0, 1.1), n_out=1) . lr = learn.lr_find().valley . learn.fit_one_cycle(20, lr) . epoch train_loss valid_loss time . 0 | 0.289919 | 0.264292 | 00:00 | . 1 | 0.280569 | 0.260043 | 00:00 | . 2 | 0.264891 | 0.248970 | 00:00 | . 3 | 0.243126 | 0.230470 | 00:00 | . 4 | 0.218251 | 0.207044 | 00:00 | . 5 | 0.195376 | 0.184551 | 00:00 | . 6 | 0.175897 | 0.165676 | 00:00 | . 7 | 0.158557 | 0.149466 | 00:00 | . 8 | 0.143941 | 0.136603 | 00:00 | . 9 | 0.131354 | 0.127658 | 00:00 | . 10 | 0.119663 | 0.120048 | 00:00 | . 11 | 0.110008 | 0.114571 | 00:00 | . 12 | 0.101210 | 0.111422 | 00:00 | . 13 | 0.093147 | 0.109775 | 00:00 | . 14 | 0.086211 | 0.108900 | 00:00 | . 15 | 0.080164 | 0.107172 | 00:00 | . 16 | 0.074857 | 0.106758 | 00:00 | . 17 | 0.069664 | 0.106035 | 00:00 | . 18 | 0.065428 | 0.105261 | 00:00 | . 19 | 0.061637 | 0.104235 | 00:00 | . . preds, targs = learn.get_preds() . a_uc(preds, targs) . 0.9252531268612271 . Finally, we ensemble the predictions from our baseline (since it&#39;s not far off from the neural network&#39;s ROC AUC), random forest model, and neural network: . rf_preds = m.predict(v_xs) ens_preds = (to_np(preds.squeeze()) + rf_preds + dt.predict(v_xs)) / 3 . a_uc(ens_preds, v_y) . 0.953543776057177 . From a fairly quick model with no really &quot;complex&quot; components added (that I would probably learn in the second part of fast.ai), we&#39;re able to come 36th in the leaderboards (which is top 75%). . Anyway, this post is just a quick recap on tabular model training using a relatively small data set. We didn&#39;t need to preprocess the data since TabularPandas handles the missing values for us. We trained a baseline, then trained a random forest and a neural network model, then ensembled the predictions. Perhaps I could have analyzed the columns and remove some of the unimportant features. Unlike last time, there aren&#39;t any categorical columns so I didn&#39;t use the embeddings to train another random forest model. . Maybe in the future I&#39;ll revisit this data set and aim to get a higher score. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/04/01/Predicting-Spam.html",
            "relUrl": "/tabular/2022/04/01/Predicting-Spam.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Using auction data to predict sale price",
            "content": "To practice the theory from my previous post, I&#39;m going to be training a random forest model on a data set used by a previous Kaggle competition: &quot;Blue Book for Bulldozers&quot;. . The contest took place 9 years ago and was sponsored by Fast Iron. Here is its description: . The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it&#39;s usage, equipment type, and configuaration. The data is sourced from auction result postings and includes information on usage and equipment configurations. . Fast Iron is creating a &quot;blue book for bull dozers,&quot; for customers to value what their heavy equipment fleet is worth at auction. . Preprocessing the data set . First, we download the data set using the Kaggle API (to see how I did this, open the blog in Colab). Post download, we have these files: . !ls . bluebook-for-bulldozers.zip Test.csv Train.zip &#39;Data Dictionary.xlsx&#39; tp.pkl Valid.7z kaggle.json Train.7z Valid.csv Machine_Appendix.csv TrainAndValid.7z ValidSolution.csv median_benchmark.csv TrainAndValid.csv Valid.zip random_forest_benchmark_test.csv TrainAndValid.zip . Then, we can import the full data set into a Pandas DataFrame object: . # When low_memory = True (default), Pandas only looks at a # few rows to see what type the data in a specific column # is. If we want to minimize errors and don&#39;t run into a # low memory issue, we should set low_memory = False. df = pd.read_csv(&#39;TrainAndValid.csv&#39;, low_memory = False) . This data set contains 53 different features: . df.columns, len(df.columns) . (Index([&#39;SalesID&#39;, &#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;), 53) . According to the &quot;Data&quot; tab in the Kaggle competition page, the key features are in the data set are: . SalesID: the uniue identifier of the sale. | MachineID: the unique identifier of a machine. A machine can be sold multiple times. | SalePrice: what the machine sold for at auction (only provided in train.csv). | saledate: the date of the sale. | . After we read the data, we should handle the orderable categorical variables (called ordinal). We can do so by explicitly setting an order and telling Pandas that we want this column to be ordered by this new rule. The only feature for which this applies is the ProductSize column: . df[&#39;ProductSize&#39;].unique() . array([nan, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;], dtype=object) . Currently, it&#39;s in a random order, but we can give it a specific order: . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered = True) . df[&#39;ProductSize&#39;].unique() . [NaN, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;] Categories (6, object): [&#39;Large&#39; &lt; &#39;Large / Medium&#39; &lt; &#39;Medium&#39; &lt; &#39;Small&#39; &lt; &#39;Mini&#39; &lt; &#39;Compact&#39;] . Next, we have to use the correct metric for the competition. For regression problems, we usually use RMSE, but the Kaggle competition usually states which metric to use. For this competition, we use RMSLE (root mean squared log error, which is the same as RMSE, but the predictions are log&#39;d before RMSE). So, as a little preprocessing, we log the SalePrice column (and we can just use RMSE later and handle smaller prediction values while training): . df[&#39;SalePrice&#39;] = np.log(df[&#39;SalePrice&#39;]) . RMSLE is better to use than RMSE when you want to get a more accurate prediction. RMSLE is also larger for when you underestimate vs. when you overestimate. To learn more about RMSLE, read this blog post. . We should also handle dates. Currently, the dates are just... the dates. . df[&#39;saledate&#39;].head() . 0 11/16/2006 0:00 1 3/26/2004 0:00 2 2/26/2004 0:00 3 5/19/2011 0:00 4 7/23/2009 0:00 Name: saledate, dtype: object . Dates are pretty special since some dates are more important than others. There&#39;s holidays, Fridays, Mondays, weekends, end of quarter, end of year, etc. So, we use fastai&#39;s add_datepart function that splits the date column into metadata columns. . df = add_datepart(df, &#39;saledate&#39;) . Evidently, fastai has generated many metadata columns and we now have 13 date related features: . L([i for i in df if i.startswith(&#39;sale&#39;)]), len(df.columns) . ((#13) [&#39;saleYear&#39;,&#39;saleMonth&#39;,&#39;saleWeek&#39;,&#39;saleDay&#39;,&#39;saleDayofweek&#39;,&#39;saleDayofyear&#39;,&#39;saleIs_month_end&#39;,&#39;saleIs_month_start&#39;,&#39;saleIs_quarter_end&#39;,&#39;saleIs_quarter_start&#39;...], 65) . Next, we need to transform our data such that it has no strings or missing values. To do so, we use fastai&#39;s TabularPandas by passing in the DataFrame, TabularProcs, which features are categorical and which are continuous, what the dependent variable is, and how we&#39;re splitting the data. . A TabularProc is a special kind of Transform that performs the transform when it&#39;s called instead of lazily as it&#39;s accessed, and returns the exact same object after modifying it in place. . Categorify sets a number to each level in categorical columns, effectively making them continuous, with each assigned number having no meaning (the model learns the meanings on its own). If we defined an order beforehand like we did with ProductSize, then it takes that order instead of assigning a random ordering. . FillMissing takes missing values in continuous columns and assigns the average value of that continuous column. . procs = [Categorify, FillMissing] . We can use fastai&#39;s cont_cat_split that returns a tuple: (continuous labels, categorical labels). When we define dep_var = &#39;SalePrice&#39;, we tell cont_cat_split to ignore that column. The 1 is for the cardinality. Any column with cardinality greater than 1 will be assigned as continuous. . cont, cat = cont_cat_split(df, 1, dep_var = &#39;SalePrice&#39;) L(cont), L(cat) . ((#14) [&#39;SalesID&#39;,&#39;MachineID&#39;,&#39;ModelID&#39;,&#39;datasource&#39;,&#39;auctioneerID&#39;,&#39;YearMade&#39;,&#39;MachineHoursCurrentMeter&#39;,&#39;saleYear&#39;,&#39;saleMonth&#39;,&#39;saleWeek&#39;...], (#50) [&#39;UsageBand&#39;,&#39;fiModelDesc&#39;,&#39;fiBaseModel&#39;,&#39;fiSecondaryDesc&#39;,&#39;fiModelSeries&#39;,&#39;fiModelDescriptor&#39;,&#39;ProductSize&#39;,&#39;fiProductClassDesc&#39;,&#39;state&#39;,&#39;ProductGroup&#39;...]) . To split our data as training and validation, we want to construct it so that our model can generalize to unseen data. Since this is a time series data set, we want our training set to occur before our validation set. So, we can split it according to time (TrainAndValid.csv goes up to April 2012, with Test.csv being six months after May 2012. We&#39;ll split our data such that the training is six months before April 2012: before November 2011): . # Forms a copy of our current DataFrame, but has # a True where the condition is true. The pipe # &#39;|&#39; acts as &#39;union&#39; for the sets. So, our # condition is that it is before 2011 or November cond = (df[&#39;saleYear&#39;] &lt; 2011) | (df[&#39;saleMonth&#39;] &lt; 11) # The first element of the tuple produced by # np.where contains the indices, i, where cond[i] # was True train_idx = np.where( cond)[0] # Applying ~ to cond turns all True values to # False and False to True valid_idx = np.where(~cond)[0] splits = (list(train_idx), list(valid_idx)) . Now, we can put all of them inside of a TabularPandas: . tp = TabularPandas(df, procs, cat, cont, y_names = &#39;SalePrice&#39;, splits = splits) . When we display the data, they don&#39;t seem changed: . tp.show(3) . UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start auctioneerID_na MachineHoursCurrentMeter_na SalesID MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleElapsed SalePrice . 0 Low | 521D | 521 | D | #na# | #na# | #na# | Wheel Loader - 110.0 to 120.0 Horsepower | Alabama | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | False | False | 1139246 | 999089 | 3157 | 121 | 3.0 | 2004 | 68.0 | 2006 | 11 | 46 | 16 | 3 | 320 | 1.163635e+09 | 11.097410 | . 1 Low | 950FII | 950 | F | II | #na# | Medium | Wheel Loader - 150.0 to 175.0 Horsepower | North Carolina | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | 23.5 | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | False | False | 1139248 | 117657 | 77 | 121 | 3.0 | 1996 | 4640.0 | 2004 | 3 | 13 | 26 | 4 | 86 | 1.080259e+09 | 10.950807 | . 2 High | 226 | 226 | #na# | #na# | #na# | #na# | Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity | New York | SSL | Skid Steer Loaders | #na# | OROPS | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Auxiliary | #na# | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | None or Unspecified | Standard | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | False | False | False | False | False | False | False | False | 1139249 | 434808 | 7009 | 121 | 3.0 | 2001 | 2838.0 | 2004 | 2 | 9 | 26 | 3 | 57 | 1.077754e+09 | 9.210340 | . But when we display them as items (what our model&#39;s going to see), all the categorical variables are changed to continuous variables: . tp.items.head(3) . SalesID SalePrice MachineID ModelID ... saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na . 0 1139246 | 11.097410 | 999089 | 3157 | ... | 1 | 1.163635e+09 | 1 | 1 | . 1 1139248 | 10.950807 | 117657 | 77 | ... | 1 | 1.080259e+09 | 1 | 1 | . 2 1139249 | 9.210340 | 434808 | 7009 | ... | 1 | 1.077754e+09 | 1 | 1 | . 3 rows × 67 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; And, like how we defined an order for ProductSize, the order is maintained: . tp.classes[&#39;ProductSize&#39;] . [&#39;#na#&#39;, &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39;] . But for a feature whose order we didn&#39;t define, they&#39;re ordered alphabetically: . tp.classes[&#39;state&#39;] . [&#39;#na#&#39;, &#39;Alabama&#39;, &#39;Alaska&#39;, &#39;Arizona&#39;, &#39;Arkansas&#39;, &#39;California&#39;, &#39;Colorado&#39;, &#39;Connecticut&#39;, &#39;Delaware&#39;, &#39;Florida&#39;, &#39;Georgia&#39;, &#39;Hawaii&#39;, &#39;Idaho&#39;, &#39;Illinois&#39;, &#39;Indiana&#39;, &#39;Iowa&#39;, &#39;Kansas&#39;, &#39;Kentucky&#39;, &#39;Louisiana&#39;, &#39;Maine&#39;, &#39;Maryland&#39;, &#39;Massachusetts&#39;, &#39;Michigan&#39;, &#39;Minnesota&#39;, &#39;Mississippi&#39;, &#39;Missouri&#39;, &#39;Montana&#39;, &#39;Nebraska&#39;, &#39;Nevada&#39;, &#39;New Hampshire&#39;, &#39;New Jersey&#39;, &#39;New Mexico&#39;, &#39;New York&#39;, &#39;North Carolina&#39;, &#39;North Dakota&#39;, &#39;Ohio&#39;, &#39;Oklahoma&#39;, &#39;Oregon&#39;, &#39;Pennsylvania&#39;, &#39;Puerto Rico&#39;, &#39;Rhode Island&#39;, &#39;South Carolina&#39;, &#39;South Dakota&#39;, &#39;Tennessee&#39;, &#39;Texas&#39;, &#39;Unspecified&#39;, &#39;Utah&#39;, &#39;Vermont&#39;, &#39;Virginia&#39;, &#39;Washington&#39;, &#39;Washington DC&#39;, &#39;West Virginia&#39;, &#39;Wisconsin&#39;, &#39;Wyoming&#39;] . Now we processed our data and we can start training a random forest model! . Modeling . First, we&#39;ll get the split data from our TabularPandas object: . train_xs, train_y = tp.train.xs, tp.train.y valid_xs, valid_y = tp.valid.xs, tp.valid.y . Then, we&#39;ll make a decision tree model as a baseline . dt = DecisionTreeRegressor(min_samples_leaf = 25) dt.fit(train_xs, train_y) . DecisionTreeRegressor(min_samples_leaf=25) . To see how well our model did, we can take the RMSE of our predictions since we took the log beforehand (so our models predict the log of the sale price). We&#39;ll define the functions so that we don&#39;t have to repeat it later: . def r_mse(preds, y): return round(math.sqrt(((preds - y)**2).mean()), 6) def m_rmse(m, preds, y): return r_mse(m.predict(preds), y) . m_rmse(dt, train_xs, train_y), m_rmse(dt, valid_xs, valid_y) . (0.211429, 0.266451) . So, we should be trying to train a model that has a better RMSE than 0.266. . We&#39;ll train our random forest model as an ensemble of 40 decision trees: . def rf(xs, y, n_estimators = 40, max_samples = 200_000, max_features = 0.5, min_samples_leaf = 5, **kwargs): return RandomForestRegressor( n_jobs = -1, # use all CPU cores n_estimators = n_estimators, # number of decision trees max_samples = max_samples, # max number of rows to get max_features = max_features, # how many columns to get (%) min_samples_leaf = min_samples_leaf, # leaf nodes must have at least this many (to prevent overfitting) oob_score = True # track out-of-box error score ).fit(xs, y) . m = rf(train_xs, train_y) . m_rmse(m, train_xs, train_y), m_rmse(m, valid_xs, valid_y) . (0.171233, 0.23292) . And, our out-of-bag error is: . r_mse(m.oob_prediction_, train_y) . 0.210877 . Since it&#39;s smaller than our validation error, our model shouldn&#39;t be overfitting and is instead having other problems. . Interpretation . First, we&#39;ll look at the confidence of each tree: . preds = np.stack([t.predict(valid_xs.values) for t in m.estimators_]) preds.shape # we should have 40 trees . (40, 5754) . preds_std = preds.std(0) preds_std.sort() preds_std . array([0.04779576, 0.05120998, 0.0513565 , ..., 0.5755972 , 0.57621563, 0.57734788]) . The standard deviations of the predictions for each auction ranges from 0.00478 to 0.57735 since the trees for some auctions agree (low standard deviation), while other times they disagree (high standard deviation). In production, you could warn the user to be more wary of the prediction if the standard deviation is above a certain threshold. . To see which features were the most important, we can use the .feature_imporances_ attribute of our RandomForestRegressor model to get each columns&#39; feature importance score. We can pair it up with the column names in a DataFrame and sort it to see which features were the most important. . def feature_importance(df, m): return pd.DataFrame({&#39;Feature&#39;: df.columns, &#39;Importance&#39;: m.feature_importances_}).sort_values(&#39;Importance&#39;, ascending = False) . fi = feature_importance(train_xs, m) . fi[:10] . Feature Importance . 57 YearMade | 0.184860 | . 6 ProductSize | 0.114785 | . 30 Coupler_System | 0.114240 | . 7 fiProductClassDesc | 0.078662 | . 54 ModelID | 0.058189 | . 65 saleElapsed | 0.050378 | . 3 fiSecondaryDesc | 0.038825 | . 12 Enclosure | 0.038811 | . 32 Hydraulics_Flow | 0.036170 | . 1 fiModelDesc | 0.032337 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We&#39;ll try removing the unimportant features and see if it affects the model&#39;s performance. First, we set a threshold (0.005) and only take the columns whose importance is greater than that threshold: . imp = fi[fi.Importance &gt; 0.005].Feature len(imp) . 21 . Then, we make new training and validation sets: . train_xs_imp = train_xs[imp] valid_xs_imp = valid_xs[imp] train_xs_imp . YearMade ProductSize Coupler_System fiProductClassDesc ... Drive_System MachineID Hydraulics Tire_Size . 0 2004 | 0 | 0 | 59 | ... | 0 | 999089 | 1 | 17 | . 1 1996 | 3 | 0 | 62 | ... | 0 | 117657 | 1 | 12 | . 2 2001 | 0 | 1 | 39 | ... | 0 | 434808 | 4 | 0 | . 3 2001 | 4 | 0 | 8 | ... | 0 | 1026470 | 1 | 0 | . 4 2007 | 0 | 1 | 40 | ... | 0 | 1057373 | 4 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 412693 2005 | 5 | 0 | 13 | ... | 0 | 1919201 | 12 | 0 | . 412694 2005 | 5 | 0 | 17 | ... | 0 | 1882122 | 4 | 0 | . 412695 2005 | 5 | 0 | 13 | ... | 0 | 1944213 | 4 | 0 | . 412696 2006 | 5 | 0 | 13 | ... | 0 | 1794518 | 4 | 0 | . 412697 2006 | 5 | 0 | 17 | ... | 0 | 1944743 | 4 | 0 | . 406944 rows × 21 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Finally, we train a new model using the new training set: . m_imp = rf(train_xs_imp, train_y) . m_rmse(m_imp, train_xs_imp, train_y), m_rmse(m_imp, valid_xs_imp, valid_y) . (0.181282, 0.233111) . Although our training metric become a bit worse, our validation metric isn&#39;t far off, so we can train future models using this new training set since we can effectively remove 2/3 of the &quot;unncessary&quot; columns. . Next, we&#39;ll remove the redundant columns. We&#39;ll use cluster_columns which determines feature similarity through rank correlation. . cluster_columns(train_xs_imp) . We&#39;ll see how removing each redundant feature affects the model&#39;s capability by defining a function that trains a quick random forest model and returns its out-of-box error: . def get_oob(df): m = RandomForestRegressor( n_estimators = 40, min_samples_leaf = 15, # higher to have a shorter depth tree max_samples = 50_000, max_features = 0.5, n_jobs = -1, oob_score = True).fit(df, train_y) return m.oob_score_ . The oob_score_, unlike oob_prediction_, returns $R^2$, so a perfect model has a score of 1.0 while a random models has 0.0. As a baseline, we&#39;ll first find the oob_score_ of our current training set: . get_oob(train_xs_imp) . 0.8771422789129072 . Then, we&#39;ll try removing each possibly redundant feature one by one: . {c: get_oob(train_xs_imp.drop(labels = c, axis = &#39;columns&#39;)) for c in (&#39;saleYear&#39;, &#39;saleElapsed&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Coupler_System&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiModelDesc&#39;)} . {&#39;Coupler_System&#39;: 0.8777286429358461, &#39;Grouser_Tracks&#39;: 0.8781106723829636, &#39;Hydraulics_Flow&#39;: 0.8778805337963219, &#39;ProductGroup&#39;: 0.87719310108094, &#39;ProductGroupDesc&#39;: 0.877706764282738, &#39;fiBaseModel&#39;: 0.8755285591209638, &#39;fiModelDesc&#39;: 0.8760491649430575, &#39;saleElapsed&#39;: 0.8726949857217733, &#39;saleYear&#39;: 0.8762485106840037} . Since the out-of-box score didn&#39;t change much, we&#39;ll try leaving just one of them in each group: . to_drop = [&#39;saleYear&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;ProductGroup&#39;, &#39;fiBaseModel&#39;] get_oob(train_xs_imp.drop(to_drop, axis = 1)) . 0.8754873568883906 . The score ultimately only goes down by 0.002 from our baseline, so we&#39;ll drop these labels from our training and validation set. . train_xs_fin = train_xs_imp.drop(to_drop, axis = 1) valid_xs_fin = valid_xs_imp.drop(to_drop, axis = 1) . While we&#39;re at it, we&#39;ll also adjust the years so that the minimum year isn&#39;t at 1000 but instead at 1950 . train_xs_fin.loc[train_xs_fin.YearMade &lt; 1950, &#39;YearMade&#39;] = 1950 valid_xs_fin.loc[valid_xs_fin.YearMade &lt; 1950, &#39;YearMade&#39;] = 1950 . Now, we&#39;ll check that our RMSE didn&#39;t decrease significantly: . m = rf(train_xs_fin, train_y) m_rmse(m, train_xs_fin, train_y), m_rmse(m, valid_xs_fin, valid_y) . (0.182645, 0.234452) . The validation RMSE didn&#39;t decrease much and now we only have 16 columns instead of 66. . len(train_xs_fin.columns) . 16 . With most of the unnecessary features removed, we can revisit feature importance and look at the partial dependence plots of some of the most important features and how our predictions depend on the features: . from sklearn.inspection import PartialDependenceDisplay fig, ax = plt.subplots(figsize = (18, 4)) PartialDependenceDisplay.from_estimator(m, train_xs_fin, [&#39;YearMade&#39;, &#39;Coupler_System&#39;, &#39;ProductSize&#39;], grid_resolution = 20, ax = ax) . &lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7fa97cbd3150&gt; . From these plots, it appears that price has an exponential relationship with year, which makes sense (since products tend to depreciate exponentially over time). But for the other two, the missing data seems to be quite important: . tp.classes[&#39;Coupler_System&#39;], tp.classes[&#39;ProductSize&#39;] . ([&#39;#na#&#39;, &#39;None or Unspecified&#39;, &#39;Yes&#39;], [&#39;#na#&#39;, &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39;]) . Coupler systems, although it&#39;s the second most important feature, makes its most important decision on whether it was filled out or not... and with product sizes, although it makes sense that the price decreases as the size decreases, no size mentioned also plays a significant role. . If we were the ones who created the data set, it would be interesting to see why that might be the case. Maybe only data entered after a certain date contained these values, or maybe these columns aren&#39;t as important. . Next, we can look at how the features influence the prediction. To do so, we can use the treeinterpreter library to see how the features influence the prediction, and then use the waterfallcharts library to draw it. . For the treeinterpreter, we pass the model and the rows of data for which we want predictions. Then, it returns a tuple of three items: . prediction is the predicted value for the dependent variable. | bias is the mean of the dependent variable. | contributions is a list of contributions made by each feature. | . So, prediction = bias - contributions.sum(). And, we&#39;ll plot the contributions using a waterfall chart. . nrows = 1 row = 0 prediction, bias, contributions = treeinterpreter.predict(m, valid_xs_fin[:nrows].values) prediction[row], bias[row], contributions[row].sum() . (array([10.64886316]), 10.104243648162724, 0.5446195099995459) . waterfall(valid_xs_fin.columns, contributions[row], rotation_value = 90, formatting = &#39;{:,.2f}&#39;, net_label = &#39;Total&#39;) . &lt;module &#39;matplotlib.pyplot&#39; from &#39;/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py&#39;&gt; . So, it seems that YearMade, ModelID, fiSecondaryDesc, and fiModelDesc were the most important features for this row&#39;s prediction. . But, it&#39;s a bit weird that ModelID is an important feature. What would happen when we have a new system for ModelID in the future? So, we should try to find out-of-domain data by combining our training and validation sets and train a model to predict whether a row is from the training or validation set. . df_domain = pd.concat([train_xs_fin, valid_xs_fin]) is_valid = np.array([0] * len(train_xs_fin) + [1] * len(valid_xs_fin)) m = rf(df_domain, is_valid) . feature_importance(df_domain, m)[:6] . Feature Importance . 5 saleElapsed | 0.919558 | . 9 SalesID | 0.061309 | . 13 MachineID | 0.014026 | . 0 YearMade | 0.000824 | . 4 ModelID | 0.000767 | . 11 fiModelDescriptor | 0.000733 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; So, the main things that appear to change from the training and validation set are saleElapsed, SalesID,and MachineID. Like before, let&#39;s try removing each of them and see how our model changes. . m = rf(train_xs_fin, train_y) m_rmse(m, valid_xs_fin, valid_y) . 0.23369 . {c: m_rmse(rf(train_xs_fin.drop(c, axis = 1), train_y), valid_xs_fin.drop(c, axis = 1), valid_y) for c in [&#39;saleElapsed&#39;, &#39;SalesID&#39;, &#39;MachineID&#39;, &#39;YearMade&#39;, &#39;ModelID&#39;]} . {&#39;MachineID&#39;: 0.233734, &#39;ModelID&#39;: 0.238536, &#39;SalesID&#39;: 0.231391, &#39;YearMade&#39;: 0.268748, &#39;saleElapsed&#39;: 0.237849} . So, it appears that we can remove MachineID and SalesID without worry (the model actually improved when we removed SalesID). . to_drop = [&#39;MachineID&#39;, &#39;SalesID&#39;] xs_new = train_xs_fin.drop(to_drop, axis = 1) valid_xs_new = valid_xs_fin.drop(to_drop, axis = 1) . And we&#39;ll double check our model&#39;s RMSE didn&#39;t go up significantly: . m = rf(xs_new, train_y) m_rmse(m, valid_xs_new, valid_y) . 0.230939 . Not only did it not go up, it actually went down. So, removing MachineID and SalesID, which probably correlated with YearMade and saleElapsed, made the model better. . For the next blog, I&#39;ll train a deep learning model using the information gained through training and analyzing a random forest model. Then, I&#39;ll use the embeddings learned by the neural network to retrain a random forest model and try to beat the #1 gold-medal-level model on the leaderboards. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/31/Blue-Book.html",
            "relUrl": "/tabular/2022/03/31/Blue-Book.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Adding deep learning to tabular models",
            "content": "Two blogs on the same day! I didn&#39;t want the last one to be too long and I didn&#39;t think I would be this productive to be able to finish the neural network part today. But, here we go. . . Previously, I trained a random forest model for predicting sale price on past auction data for bulldozers. Today, I&#39;m going to be training a deep learning model on the same data set (with some changes based on the last blog). Then, I&#39;ll try using the embeddings learned from that model and train another random forest model and aim for an even better validation RMSE. Finally, I&#39;ll ensemble the results of the deep learning model and the new random forest model and see how much better the predictions become. . So, we&#39;ll first read our data into a pandas DataFrame, . df = pd.read_csv(&#39;TrainAndValid.csv&#39;, low_memory=False) . Then, we apply the initial transforms we did last time: . Set an order for the product sizes. | Log the sale price. | Split the date column into metacolumns. | . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True) df[&#39;SalePrice&#39;] = np.log(df[&#39;SalePrice&#39;]) df = add_datepart(df, &#39;saledate&#39;) . And we remove all the unneeded columns we determined last time: . to_keep_df = load_pickle(&#39;xs_new.pkl&#39;) . to_keep = list(to_keep_df) + [&#39;SalePrice&#39;] . df = df[to_keep] df.head(3) . YearMade ProductSize Coupler_System fiProductClassDesc ... Drive_System Hydraulics Tire_Size SalePrice . 0 2004 | NaN | NaN | Wheel Loader - 110.0 to 120.0 Horsepower | ... | NaN | 2 Valve | None or Unspecified | 11.097410 | . 1 1996 | Medium | NaN | Wheel Loader - 150.0 to 175.0 Horsepower | ... | NaN | 2 Valve | 23.5 | 10.950807 | . 2 2001 | NaN | None or Unspecified | Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity | ... | NaN | Auxiliary | NaN | 9.210340 | . 3 rows × 15 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Next, we have to know which columns should be treated as categorical so that they can be given embeddings. For that, we&#39;ll use cont_cat_split, but we set the max cardinality as 9000 (embedding sizes greater than 10,000 should be used after you tested if there&#39;s better ways to group the variable). . cont, cat = cont_cat_split(df, max_card=9_000, dep_var=&#39;SalePrice&#39;) . Mainly, we want to ensure saleElapsed is in the continuous section since by definition, if a label is in the categorical section, it cannot be extrapolated. . cont . [&#39;saleElapsed&#39;] . Next, let&#39;s see the cardinality of each of the categorical variables (if there&#39;s some that are similar in number, they may be redundant and we can try to remove all but one): . df[cat].nunique() . YearMade 73 ProductSize 6 Coupler_System 2 fiProductClassDesc 74 ModelID 5281 fiSecondaryDesc 177 Enclosure 6 fiModelDesc 5059 ProductGroupDesc 6 fiModelDescriptor 140 Drive_System 4 Hydraulics 12 Tire_Size 17 dtype: int64 . It appears that ModelID and fiModelDesc may be redundant since they&#39;re both pertaining to Model and are of similar cardinality. So, we&#39;ll do what we did before and try removing fiModelDesc and see what happens to the results. . get_oob(xs_new) . 0.8760441852100622 . {c: get_oob(xs_new.drop(c, axis=1)) for c in [&#39;ModelID&#39;, &#39;fiModelDesc&#39;, &#39;fiModelDescriptor&#39;]} . {&#39;ModelID&#39;: 0.8717292042012881, &#39;fiModelDesc&#39;: 0.869256405033346, &#39;fiModelDescriptor&#39;: 0.8748457618690582} . Overall, it seems we can remove fiModelDescriptor without it significantly affecting the model. . cat.remove(&#39;fiModelDescriptor&#39;) . To create DataLoaders for our model, we can use TabularPandas again. However, we have to add the Normalize TabularProc for a neural network since the scale of the variables matter unlike in building a decision tree. . tp = TabularPandas(df, [Categorify, FillMissing, Normalize], cat, cont, splits=splits, y_names=&#39;SalePrice&#39;) . # a vision model since we&#39;re dealing with # dabular data (doesn&#39;t require as much GPU RAM) dls = tp.dataloaders(1024) . Next, we&#39;ll see what range we should have for our predictions. . y, v_y = tp.train.y, tp.valid.y y.min(),y.max(),v_y.min(),v_y.max() . (8.465899467468262, 11.863582611083984, 8.465899467468262, 11.849397659301758) . So, we can set our y_range as (8, 12) (remember that a model tends to do better when we have our upper bound a little higher than the actual maximum). . learn = tabular_learner(dls, layers=[500, 250], y_range=(8, 12), n_out=1, loss_func=F.mse_loss) . As always, we&#39;ll use .lr_find() to find the optimal learning rate: . lr = learn.lr_find().valley . And, we&#39;ll use fit_one_cycle to train our model since we&#39;re not transfer learning: . learn.fit_one_cycle(5, lr) . epoch train_loss valid_loss time . 0 | 0.068500 | 0.075998 | 00:07 | . 1 | 0.052604 | 0.063197 | 00:07 | . 2 | 0.047209 | 0.059763 | 00:07 | . 3 | 0.043221 | 0.061487 | 00:07 | . 4 | 0.039401 | 0.053084 | 00:07 | . preds, targs = learn.get_preds() r_mse(preds, targs) . 0.230399 . Overall, we got a better RMSE than the random forest model, but not by a lot. . Now, we&#39;ll try using the embeddings from the neural network to replace the categorical columns for our random forest: . # code is essentially verbatim from fast.ai&#39;s @danielwbn: # https://forums.fast.ai/t/using-embedding-from-the-neural-network-in-random-forests/80063/10 def transfer_embeds(learn, xs): xs = xs.copy() for i, feature in enumerate(learn.dls.cat_names): emb = learn.embeds[i].cpu() # added .cpu() to learn since tensor below is made on cpu while learn is on cuda new_feat = pd.DataFrame(emb(tensor(xs[feature], dtype=torch.int64)), index=xs.index, columns=[f&#39;{feature}_{j}&#39; for j in range(emb.embedding_dim)]) xs = xs.drop(feature, axis=1) xs = xs.join(new_feat) return xs . xs_with_embs = transfer_embeds(learn, learn.dls.train.xs) valid_xs_with_embs = transfer_embeds(learn, learn.dls.valid.xs) . m = rf(xs_with_embs, train_y) . m_rmse(m, xs_with_embs, train_y), m_rmse(m, valid_xs_with_embs, valid_y) . (0.184022, 0.236021) . The validation RMSE ends up becoming worse! But, take a look at this: . We&#39;ll look at our RMSE when we ensemble the predictions from random forests (with embeddings) and the neural network. However, before we can take the average of the predictions from random forests and the neural network, we have to make them of the same type. To do so, we have to turn our neural network predictions (a rank-2 tensor) into a rank-1 numpy array. We can apply .squeeze() to remove any unit axis (a vector with only 1 thing in it) . rf_preds = m.predict(valid_xs_with_embs) ens_preds = (to_np(preds.squeeze()) + rf_preds) / 2 . r_mse(ens_preds, valid_y) . 0.228773 . And, we now have an RMSE that&#39;s lower than the top leaderboard score of 0.22909, granted we&#39;re doing it on the validation set (since we don&#39;t have access to the SalePrice of Test.csv): . a = pd.read_csv(&#39;Test.csv&#39;, low_memory=False) a.columns, &#39;SalePrice&#39; in a.columns . (Index([&#39;SalesID&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;), False) . So, we&#39;ve finally covered tabular data training with decision trees, random forests, neural networks, and random forests with embeddings. We&#39;ve ensembled the results of the last 2 to get a score that (&quot;technically&quot;) beats the top leaderboard score of the Kaggle competition. . Next, we&#39;ll be moving onto natural language models. But, I might also try experimenting with another tabular data set. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/31/Blue-Book-NN.html",
            "relUrl": "/tabular/2022/03/31/Blue-Book-NN.html",
            "date": " • Mar 31, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "You get a decision tree! And YOU get a decision tree!",
            "content": "Our first method for training structured tabular data is to use ensembles of decision trees. . . Decision trees: a decision tree asks a series of yes/no questions about the data. After each question, the data at that part splits between yes/no. After one or more questions, predictions can be formed by finding the group the data is part of at the bottom of the tree and returning the average value of the targets in that group. . . To train a decision tree, we follow a greedy approach with six steps: . Loop through each column of the data set. | For each column, loop through each possible level of that column. | . Level: for most continuous and some categorical variables, when we say levels, we&#39;re referring to variables that can be ordered. For example, sizes like &quot;Small&quot; &lt; &quot;Medium&quot; &lt; &quot;Large&quot;. For other categorical variables, we refer to the actual values. . . Try splitting the data into two groups, based on whether they&#39;re greater than or less than that value (or equal to or not equal to for other categorical variables). | Find the average prediction for each of those two groups and use your metric to see how close that is to the actual value of each of the items in that group. | After looping through all the possible columns and levels for each column, pick the split point that gave the best prediction. | Now, we have two groups for our data set. Treat each of them as new data sets and repeat from step 1 until each group reaches your minimum size threshold. | With decision trees, you have to be careful with how many leaf nodes you end up with. If you have too many (close to the number of data entries), then your model will overfit. . Overfitting? No problem. . One year before his retirement, Leo Breiman published a paper on &quot;bagging&quot;. Instead of training on the entire training set (or mini-batches), you . randomly choose a subset of the rows of your data, | train a model using this subset, | save the model, and | train more models on different subsets of the data. | Eventually, you end up with a number of models. To make a prediction, you predict using all of the models and take the average. . Each of the models have errors since they&#39;re not trained on the full training set, but since different models have different errors (and these errors aren&#39;t correlated with each other; i.e., they&#39;re independent) the errors end up cancelling out when we take the average. . Seven years later, Breiman also coined &quot;random forests&quot; where you apply bagging to decision trees not only by randomly choosing a subset of the rows of your data, but you also randomly choosing a subset of the columns when choosing a split in each decision tree. . . Random forests: a specific type of an ensemble of decision trees, where bagging is used to combine the results of several decision trees that were trained on random subsets of the rows of the data where each split made on a random subset of the columns of the data. . . Since the errors tend to cancel out, it also means the trees are less susceptible to hyperparameter changes. We can also have as many trees as we want; in fact, the error rate usually decreases as we add more trees. . Interpreting the model . Once we trained our model, if the error rate for the validation set is higher than the training set, we want to make sure it&#39;s from generalization (or extrapolation) problems and not overfitting. . Out-of-bag error allows us to check if we&#39;re overfitting without the need of a validation set. Since each tree in a random forest is trained on a subset of the data, we can form a validation set for each tree as the rows not included in training for that tree. . What makes out-of-bag error different from validation set error is that the data in the former is within the range of the training set, while the validation set is usually outside of the range; this range is most important for time series data since the validation set should contain data that&#39;s in the future compared to the training set. . So, if our out-of-bag error is lower than the validation set error, then the model is not overfitting and is instead having other problems. . In general, we want to interpret in our model: . how confident are we in our predictions for a particular row of data? | for making our predictions on a specific row of data, what were the most important columns, and how did they influence the prediction? | which columns are the most important; and which columns can we ignore (remove them from training)? | which columns are effectively redundant in terms of prediction? | how do predictions vary as we vary the columns (as in, what kind of relationship do the columns have with the predictions)? | . Confidence for a prediction on a particular row of data . When we want to predict for a particular row of data, we pass the data to each tree in our random forest and take the average of the results. To find the relative confidence of the prediction, we can take the standard deviation of the predictions instead of the average. So, if the standard deviation is high, we should be more wary of the prediction since the trees disagree more than if the standard deviation was low. . Feature importance . It&#39;s important to understand how our models are making predictions, not just how accuracte the predictions are. . To find the importance of each column (feature) in our data, we can loop through each tree and recursively explore each branch. At each branch, look at what column was used for that split and how much the model improved at that split. The improvement, which is weighted by the number of rows in that group is added to the importance score for that column. The importance score is summed across all branches of all trees. Then, you can normalize the scores (so that they sum to 1) and sort them in ascending order to see the least important columns, and by descending order to see the most important columns. . The &quot;how&quot; is mostly used in production (and not in model training) to see how the data is leading to predictions. To find how each column influenced the prediction, we take a single row of the data and pass it through each of the decision trees in our random forest. At each split point, record how the prediction changes (increases or decreases) compared to the parent node of the tree and add it to the column&#39;s score. Then, combine the score for each of the columns and you can see how each column increased or decreased the prediction relative to the parent node of the random forest (which is the average of the average of the target in each row in the batch of rows in the batch of trees in the random forest). . Ignoring features . Once you found the importance of each column, you can set a threshold such that you ignore features whose importance scores were lower than that threshold (this is why we normalized the scores). . Try retraining your model with those columns ignored and you can decide to keep the change (if the accuracy hasn&#39;t changed much) or change your threshold (if the accuracy decreased significantly). In any case, it&#39;s nicer to train your model with less unimportant columns since you&#39;ll be able to train future models on the same data set faster. . Redundant features . To find redundant columns, you want to find how similar each column is to another. To do so, you calculate the rank correlation, where all the values in each column are replaced with their rank relative to other values in the same column (think of it like descending argsort, where you give each row in a specific column the index it would have for the column to be sorted in descending order).Then, the correlation is calculated (kind of like the correlation coefficient $r$, but with rank). Columns with similar rank correlations may be synonyms for each other and one (or more) of them could be removed. . When removing redundant columns, retrain the model where you remove only one redundant column at a time. Then, try removing them in groups and eventually altogether. The point of this tedious task is to make sure we&#39;re not significantly reducing the accuracy of the model. And, some columns, although they seem redundant, may not be redundant and would be important to keep in the model. . Although not necessary, you should remove unimportant and redundant columns when possible since it&#39;ll simplify your model. . Relationship between columns and predictions . To find the relationship between a column and prediction, you could guess that we should have a row where we keep all columns constant except for the column in question. . But, we can&#39;t just take the average of the predictions for a specific level of a column since other variables can change. Instead, we replace every single value in the column with a specific level in the validation set, and record the prediction with the new validation set as the input. Then, we do the same for every other level of that column. . With these predictions, we can form a line graph with the levels as the x-axis and the predictions as the y-axis. We call this graph a partial dependence plot. . Sometimes, you trained your model and . your accuraccy is too good to be true, | some features don&#39;t make sense to be predictors, or | the partial dependence plots looks weird. | . If so, your data might have data leakage where the training set contains information that wouldn&#39;t be available in the data you give at inference (i.e., when using the model in practice and/or your validation set). . Data leakage are subtleties that give away the correct answer. For example, if you trained a model to predict the weather and the precipitation was in an available column (and/or it was only filled out on rainy days), you bet your model would predict it was &quot;raining&quot; on &quot;rainy days&quot; if there was any precipitation and &quot;sunny&quot; on &quot;sunny days&quot; otherwise. So, when you interpret the model later, you might see really high accuracy, with precipitation being a high predictor. . In preventing data leakage, train your model first and then look for data leakage (and then clean or reprocess your data); this process is the same with how you would train your model first before performing data cleaning. . We can&#39;t always use random forests . With time series data, you usually want to have a model that can generalize to new data and extrapolate accurately. The downside of random forests is that it can only predict within the range of its training data. So, if the value in the validation set is outside of the range of the training set, the accuracy of the random forest will always be low since it can&#39;t predict values that high. . Why might this be the case? A random forest returns a prediction based on the average of the predictions of its decision trees, where each tree predicts the average of the targets in the rows in a leaf node. So, a random forest can never predict a value that&#39;s outside of the range of the training set. . In a general sense, a random forest can&#39;t generalize to out-of-domain data, so we need to make sure our validation, test, and future data sets contain the same kind of data as our training set. . To test if there&#39;s out-of-(the training set&#39;s)-domain data, we can build a random forest that predicts which row is in the validation or training set. To do so, you can concatenate the validation and training set and label the rows by validation or training. Then, through feature importance, if there&#39;s a particular column that is more prominent in the validation set, there will be a nonuniform distribution of importance scores. . Sometimes, you can remove the columns with high feature importance and improve the accuracy of the model since those columns might be related to another column (hence removing redundant columns). . Removing those columns can also make your model more resilient over time since those columns may be affected by domain shift where the data put into the model is significantly different from the training data. . Boosting instead of bagging . Instead of random forests, which forms an ensemble of decision trees through bagging, we can also make gradient boosted machines which uses boosting instead of bagging. . Bagging takes the average of the predictions from each decision tree. Boosting, on the other hand, adds the predictions of each decision tree. So, you also train your decision trees differently: . train a decision tree that underfits the targets of your training set, | calculate residuals by subtracting the predictions from the targets, | repeat from the beginning, but train your future models with the residuals as the targets, and | continue training more trees until you reach a certain maximum or your validation metric gets worse. | . With boosting, we try to minimize the error by having the residuals become as small as possible by underfitting them. . Unlike random forests, the trees aren&#39;t independent of each other so the more trees we train, the more the overall model will overfit the training set. . Free accuracy boost . In training a model for tabular data, you can get a boost in accuracy by training a random forest model, doing some analysis like feature importance and partial dependence plots to remove redundant columns, and then training a neural network that uses embeddings for the categorical variables/columns. . Then, we retrain our random forest model, but instead of creating levels for the categorical variables, we use the embeddings trained by the neural network. So, instead of using a neural network at inference, you can use an improved random forest model. . The same can be done for gradient boosted machines, and any model that uses categorical variables. Just use the embeddings trained by the neural network. . Conclusion . We covered a machine learning technique called ensembles of decision trees. Here, we mentioned two methods of ensembling: bagging and boosting. . With bagging, you form a random forest that&#39;s quick and easy to train. Random forests are also resistant to hyperparameter changes and since the trees are independent, it&#39;s very difficult to overfit as you increase the number of trees. . With boosting, you form a gradient boosted machine (or gradient boosted decision tree) that are just as fast to train as random forests in theory, but require more hyperparameter tuning and are susceptible to overfitting with the more trees you train since the trees aren&#39;t independent of each other. However, gradient boosted machines tend to have higher accuracy than random forests. . Overall, because of the limitations of decision trees, both random forests and gradient boosted machines can&#39;t extrapolate to out-of-domain data. Therefore, you sometimes have to make a neural network. . Neural networks take the longest to train and require more preprocessing like batch normalization (which also needs to be done at inference). With neural networks, you have to be careful with your hyperparameters since they can lead to overfitting. However, neural networks are great at extrapolating and can have the highest accuracy of the three models. . With neural networks, you can also use ensembles of decision trees to do some of the preprocessing to make them faster to train. And, once you train a neural network, you can use the embeddings trained by the neural networks as the inputs for the categorical variables in another ensemble of decision trees on the same data set. Doing so tends to produce much higher accuracy. . If the task doesn&#39;t require extrpolation (all future predictions are expected to be in the same range as the training set), then you can use the improved ensemble of decision trees since they will be faster at inference compared to neural networks. . Moreover, if the response time at inference isn&#39;t a major problem, you can even form an ensemble of neural networks and an ensemble of decision trees where you take the average of the predictions of each of the models. Taking the theory behind random forests, since the two (or more) models were trained by two (or more) very different algorithms, the errors each make are independent of each other and will cancel each other out, leading to higher accuracy with less chances of overfitting. Still, it won&#39;t make a bad model a good model. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/23/Decision-Trees.html",
            "relUrl": "/tabular/2022/03/23/Decision-Trees.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Training models on tabular data",
            "content": "When we train a model on tabular data, we want to create a model that, given values in some columns, can predict the value in another column. In my collaborative filtering blog, I gave the model users&#39; reviews of other movies as inputs and wanted a prediction of the users&#39; review of another movie. . Preprocessing data . Tabular data have two types of variables: continuous variables (numerical data) and categorical variables (discrete data). In the collaborative filtering model, the users and movies were (high-cardinality) categorial variables. When training a model, we want all our inputs to be continuous variables, so we need a way to turn the categorical variables continuous. . So, you pass your categorical variables through embeddings. An embedding is equivalent to putting a linear layer after every one-hot-encoded input layers. To elaborate: you have inputs that can be indexed by one-hot-encoded vectors. And, an embedding layer takes the relevant inputs from those inputs by indexing while keeping track of the steps taken so that its derivative can be calculated layer. When you pass your one-hot-encoded input layers through an embedding layer, you get continuous numbers, which you can pass through other layers in your neural network. . When we train the model on these embeddings (the inputs), we can interpret the distance between the embeddings afterwards; since the embedding distances were learned based on patterns in the data, they also tend to match up with our intuition. . Since we can form continuous embeddings for our categorical variables, we can treat them like continuous variables when we train our models. So, we could perform probabilistic matrix factorization, or concatenate them with the actual continuous variables and pass them through a neural network. . Below shows how Google trains a model for recommendations on Google Play: . And here we branch . In modern machine learning, there are two main techniques that are widely applicable, each for specific kinds of data: . Ensembles of decision trees (like random forests and gradient boosting machines) for structured data. | Multilayered neural networks optimized with SGD (like shallow and/or deep learning) for unstructured data (like images, audio, and natural language). | Deep learning is almost always superior for unstructured data and tend to give similar results for structured data. But, decision trees train much faster, are simpler to train, and are easier to interpret (like which columns were most important). . However, deep learning is a better choice than decision trees when . there are some high-cardinality categorical variables that are very important (like zip codes); or | there&#39;s some columns that&#39;d be best understood through a neural network like plain text. | . Still, you should try both to see which one works best. Usually, you&#39;ll start with decision trees as a baseline and try to achieve a higher accuracy with a deep learning model if either of those two conditions above applies. . So, in the next two blog posts, I&#39;ll be talking about decision trees and deep learning, respectively, for tabular data. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/17/Tabular.html",
            "relUrl": "/tabular/2022/03/17/Tabular.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "So you want to code collaborative filtering",
            "content": "Hopefully you&#39;ve read my last blog post which explains everything I&#39;m going to be doing in today&#39;s blog. We&#39;re going to be coding a collaborative filtering model in two ways: by probabilistic matrix factorization and then through deep learning. If you&#39;d like to learn how deep learning works, check out my other blog post. . First, we&#39;ll download a subset of the MovieLens dataset, which contains 100,000 of the 25-million recommendation dataset. The main reason being that I&#39;m using the GPUs on Colab and it would take too long to train a model with the full dataset. . from fastai.collab import * from fastai.tabular.all import * path = untar_data(URLs.ML_100k) Path.BASE_PATH = path path.ls() . . 100.15% [4931584/4924029 00:00&lt;00:00] (#23) [Path(&#39;ua.test&#39;),Path(&#39;u.item&#39;),Path(&#39;u2.base&#39;),Path(&#39;u4.test&#39;),Path(&#39;u.user&#39;),Path(&#39;u.genre&#39;),Path(&#39;u.occupation&#39;),Path(&#39;u1.test&#39;),Path(&#39;u5.base&#39;),Path(&#39;u2.test&#39;)...] . . And we can read the README file using the cat command: . !cat {path}/README . SUMMARY &amp; USAGE LICENSE ============================================= MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota. This data set consists of: * 100,000 ratings (1-5) from 943 users on 1682 movies. * Each user has rated at least 20 movies. * Simple demographic info for the users (age, gender, occupation, zip) The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. Detailed descriptions of the data file can be found at the end of this file. Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions: * The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group. * The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information). * The user may not redistribute the data without separate permission. * The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota. If you have any further questions or comments, please contact GroupLens &lt;grouplens-info@cs.umn.edu&gt;. CITATION ============================================== To acknowledge use of the dataset in publications, please cite the following paper: F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872 ACKNOWLEDGEMENTS ============================================== Thanks to Al Borchers for cleaning up this data and writing the accompanying scripts. PUBLISHED WORK THAT HAS USED THIS DATASET ============================================== Herlocker, J., Konstan, J., Borchers, A., Riedl, J.. An Algorithmic Framework for Performing Collaborative Filtering. Proceedings of the 1999 Conference on Research and Development in Information Retrieval. Aug. 1999. FURTHER INFORMATION ABOUT THE GROUPLENS RESEARCH PROJECT ============================================== The GroupLens Research Project is a research group in the Department of Computer Science and Engineering at the University of Minnesota. Members of the GroupLens Research Project are involved in many research projects related to the fields of information filtering, collaborative filtering, and recommender systems. The project is lead by professors John Riedl and Joseph Konstan. The project began to explore automated collaborative filtering in 1992, but is most well known for its world wide trial of an automated collaborative filtering system for Usenet news in 1996. The technology developed in the Usenet trial formed the base for the formation of Net Perceptions, Inc., which was founded by members of GroupLens Research. Since then the project has expanded its scope to research overall information filtering solutions, integrating in content-based methods as well as improving current collaborative filtering technology. Further information on the GroupLens Research project, including research publications, can be found at the following web site: http://www.grouplens.org/ GroupLens Research currently operates a movie recommender based on collaborative filtering: http://www.movielens.org/ DETAILED DESCRIPTIONS OF DATA FILES ============================================== Here are brief descriptions of the data. ml-data.tar.gz -- Compressed tar file. To rebuild the u data files do this: gunzip ml-data.tar.gz tar xvf ml-data.tar mku.sh u.data -- The full u data set, 100000 ratings by 943 users on 1682 items. Each user has rated at least 20 movies. Users and items are numbered consecutively from 1. The data is randomly ordered. This is a tab separated list of user id | item id | rating | timestamp. The time stamps are unix seconds since 1/1/1970 UTC u.info -- The number of users, items, and ratings in the u data set. u.item -- Information about the items (movies); this is a tab separated list of movie id | movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children&#39;s | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western | The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once. The movie ids are the ones used in the u.data data set. u.genre -- A list of the genres. u.user -- Demographic information about the users; this is a tab separated list of user id | age | gender | occupation | zip code The user ids are the ones used in the u.data data set. u.occupation -- A list of the occupations. u1.base -- The data sets u1.base and u1.test through u5.base and u5.test u1.test are 80%/20% splits of the u data into training and test data. u2.base Each of u1, ..., u5 have disjoint test sets; this if for u2.test 5 fold cross validation (where you repeat your experiment u3.base with each training and test set and average the results). u3.test These data sets can be generated from u.data by mku.sh. u4.base u4.test u5.base u5.test ua.base -- The data sets ua.base, ua.test, ub.base, and ub.test ua.test split the u data into a training set and a test set with ub.base exactly 10 ratings per user in the test set. The sets ub.test ua.test and ub.test are disjoint. These data sets can be generated from u.data by mku.sh. allbut.pl -- The script that generates training and test sets where all but n of a users ratings are in the training data. mku.sh -- A shell script to generate all the u data sets from u.data. . . Which tells us that u.data contains the full data set, which is 100,000 ratings by 943 users on 1682 items, where each user rated at least 20 movies. The data is tab separated with column names: user id, item/movie id, rating, and timestamp. So, let&#39;s try reading the csv: . ratings = pd.read_csv( path/&#39;u.data&#39;, delimiter = &#39; t&#39;, header = None, names = [&#39;user&#39;, &#39;movie&#39;, &#39;rating&#39;, &#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We&#39;d like to know the actual movie name instead of the movie ID, so we can also read the u.item file (although it says it&#39;s tab separated, it&#39;s actually pipe separated): . movies = pd.read_csv( path/&#39;u.item&#39;, delimiter = &#39;|&#39;, header = None, usecols = [0, 1], names = [&#39;movie&#39;, &#39;title&#39;], encoding = &#39;latin-1&#39; ) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Then, we can merge the two tables together: . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Then, we build our DataLoaders object, which will have our training and validation DataLoaders (which produces our mini-batches of Datasets). . dls = CollabDataLoaders.from_df(ratings, item_name = &#39;title&#39;, bs = 64) dls.show_batch() . user title rating . 0 542 | My Left Foot (1989) | 4 | . 1 422 | Event Horizon (1997) | 3 | . 2 311 | African Queen, The (1951) | 4 | . 3 595 | Face/Off (1997) | 4 | . 4 617 | Evil Dead II (1987) | 1 | . 5 158 | Jurassic Park (1993) | 5 | . 6 836 | Chasing Amy (1997) | 3 | . 7 474 | Emma (1996) | 3 | . 8 466 | Jackie Chan&#39;s First Strike (1996) | 3 | . 9 554 | Scream (1996) | 3 | . And then we create our model, which will contain our embeddings. We can&#39;t just index into a matrix for a deep learning model since we have to calculate the derivative for each operation we do. Instead, we use one-hot encoding, which is a vector that has a 1 in the places that we want to index in. For example, if we have an array [0, 1, 2, 3] and we want the element in the 2nd index (2), we would matrix multiply [0, 0, 1, 0] to the array&#39;s transpose: $$ begin{bmatrix}0 &amp; 1 &amp; 2 &amp; 3 end{bmatrix}^T begin{bmatrix}0&amp;0&amp;1&amp;0 end{bmatrix}= begin{bmatrix}0 1 2 3 end{bmatrix} begin{bmatrix}0&amp;0&amp;1&amp;0 end{bmatrix}= begin{bmatrix}2 end{bmatrix}$$ . But, storing and using one-hot encoding vectors are pretty time and memory consuming, so we use a special layer in most deep learning libraries (like PyTorch) called embedding. Embedding is mimicking the process of multiplying by a one-hot-encoded matrix, but it just indexes into a matrix using an integer while having its derivative calculated in a way such that it&#39;s identical to what it would&#39;ve been if a matrix multiplication was done with a one-hot-encoded vector. . Optimizers need to be able to get all the parameters from a model, so all embedding does is randomly initialize a matrix and wrap it around the nn.Parameter class which tells PyTorch that it&#39;s a trainable parameter. . When we refer to an embedding, that&#39;s the embedding matrix, which is the thing that&#39;s multiplied by the one-hot-encoded matrix or the thing that&#39;s being indexed into. So, an embedding matrix in this case, is our latent factors (and biases). . When creating a neural network model with PyTorch, we have to inherit from their Module class which contains the essentials; we just have to define __init__ (called dunder init) to initialize our model and forward which is essentially the &quot;predict&quot; step in the model. forward accepts the parameters of a mini-batch and returns a prediction. . n_users = len(dls.classes[&#39;user&#39;]) n_items = len(dls.classes[&#39;title&#39;]) n_factors = 50 . class DotProduct(Module): def __init__(self, n_users, n_items, n_factors, y_range = (0, 5.5)): # User latent factors and biases self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) # Item latent factors and biases self.item_factors = Embedding(n_items, n_factors) self.item_bias = Embedding(n_items, 1) # Range for our predictions self.y_range = y_range def forward(self, x): # Get first column (the users) from input users = self.user_factors(x[:,0]) # Get second column (the titles) from input items = self.item_factors(x[:,1]) # Calculate the dot product dot_prod = (users * items).sum(dim = 1, keepdim = True) # Add biases to the dot product # We add the user biases and the item biases together dot_prod += self.user_bias(x[:,0]) + self.item_bias(x[:,1]) # Return the prediction in the chosen range # Sigmoid is a function that returns a value between 0 and 1 # We can multiply it by (hi - lo) and add lo to get a value # between lo and hi, which is what sigmoid_range does return sigmoid_range(dot_prod, *self.y_range) . Now that we have our model, we can create an object with it and pass it into a Learner and train it. . model = DotProduct(n_users, n_items, n_factors) learn = Learner(dls, model, loss_func = MSELossFlat()) # We also use weight decay since we have bias in our model learn.fit_one_cycle(5, 5e-3, wd = 0.1) . epoch train_loss valid_loss time . 0 | 0.941400 | 0.941900 | 00:09 | . 1 | 0.847874 | 0.877467 | 00:08 | . 2 | 0.719121 | 0.835374 | 00:07 | . 3 | 0.594287 | 0.824023 | 00:07 | . 4 | 0.483335 | 0.824634 | 00:07 | . And, we don&#39;t need to define our own DotProduct class. We can instead use fast.ai&#39;s collab_learner. . learn = collab_learner(dls, n_factors = 50, y_range = (0, 5.5)) learn.fit_one_cycle(5, 5e-3, wd = 0.1) . epoch train_loss valid_loss time . 0 | 0.940803 | 0.954099 | 00:08 | . 1 | 0.846296 | 0.874175 | 00:07 | . 2 | 0.741423 | 0.838990 | 00:07 | . 3 | 0.590897 | 0.822672 | 00:07 | . 4 | 0.492853 | 0.823269 | 00:07 | . And we see the results are similar since the model used by collab_learner is essentially equivalent: . # 50 latent factors for users and items # bias for users and items learn.model . EmbeddingDotBias( (u_weight): Embedding(944, 50) (i_weight): Embedding(1665, 50) (u_bias): Embedding(944, 1) (i_bias): Embedding(1665, 1) ) . To turn our architecture into a deep learning model, we need a neural network. With a neural network, we start with a large matrix that we pass through layers. Instead of taking the dot product, we concatenate the latent factors from the users and the items. So, we also don&#39;t need the same number of latent factors for users as for items. To get the number of latent factors, we can use fast.ai&#39;s get_emb_sz function on our DataLoaders, which will give us recommended latent factors: . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . And, we can rewrite our DotProduct class like so: . class SimpleNet(Module): def __init__(self, user_sz, item_sz, y_range = (0, 5.5), n_acts = 100): # nn.Linear implements bias implicitly, so we # don&#39;t need to define our own bias. self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1] + item_sz[1], n_acts), nn.ReLU(), nn.Linear(n_acts, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim = 1)) return sigmoid_range(x, *self.y_range) . Then, we can put it in a Learner and train our deep learning model: . model = SimpleNet(*embs) learn = Learner(dls, model, loss_func = MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd = 0.1) . epoch train_loss valid_loss time . 0 | 0.942289 | 0.957183 | 00:07 | . 1 | 0.918996 | 0.915120 | 00:07 | . 2 | 0.854367 | 0.902296 | 00:07 | . 3 | 0.820374 | 0.877131 | 00:07 | . 4 | 0.827481 | 0.877810 | 00:07 | . And, like how we didn&#39;t need to define our own DotProduct class and use collab_learner instead, we can also do the same with SimpleNet. . # We just have to enable the use_nn parameter and # give it layers learn = collab_learner(dls, use_nn = True, y_range = (0, 5.5), layers = [100, 50]) learn.fit_one_cycle(5, 5e-3, wd = 0.1) . epoch train_loss valid_loss time . 0 | 1.018050 | 0.979968 | 00:13 | . 1 | 0.906580 | 0.922872 | 00:08 | . 2 | 0.909671 | 0.890887 | 00:08 | . 3 | 0.814160 | 0.870163 | 00:08 | . 4 | 0.802491 | 0.869587 | 00:09 | . Interpreting the results . Now that you&#39;ve trained a model, there&#39;s several ways to interpret your results. . First, we can look at the biases: . # First, take the biases and put them into # a one-dimensional tensor that we can sort item_bias = learn.model.item_bias.weight.squeeze() # argsort returns a list of indexes that would # sort the tensor idxs_bot = item_bias.argsort()[:5] idxs_top = item_bias.argsort(descending = True)[:5] # display the titles of the 5 &quot;worst&quot; movies # and the 5 &quot;best&quot; movies, respectively [dls.classes[&#39;title&#39;][i] for i in idxs_bot],[dls.classes[&#39;title&#39;][i] for i in idxs_top] . ([&#39;Children of the Corn: The Gathering (1996)&#39;, &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, &#39;Crow: City of Angels, The (1996)&#39;, &#39;Beautician and the Beast, The (1997)&#39;, &#39;Robocop 3 (1993)&#39;], [&#39;Titanic (1997)&#39;, &#39;L.A. Confidential (1997)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#34;Schindler&#39;s List (1993)&#34;, &#39;Silence of the Lambs, The (1991)&#39;]) . Then, we can find the distances: . item_factors = learn.model.item_factors.weight idx = dls.classes[&#39;title&#39;].o2i[&#39;Toy Story (1995)&#39;] distances = nn.CosineSimilarity()(item_factors, item_factors[idx][None]) idx = distances.argsort(descending = True)[1:5] dls.classes[&#39;title&#39;][idx] . (#4) [&#39;That Thing You Do! (1996)&#39;,&#39;Abyss, The (1989)&#39;,&#39;Wizard of Oz, The (1939)&#39;,&#39;Aladdin (1992)&#39;] . So, the four movies in the data set that are most similar to Toy Story are the ones above. . In the next few blogs, I&#39;ll be talking more about deep and machine learning with tabular data. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/16/Collaborative-Filtering-Code.html",
            "relUrl": "/tabular/2022/03/16/Collaborative-Filtering-Code.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "So you want to learn collaborative filtering",
            "content": "What is collaborative filtering? . You want to start a movie streaming service. You&#39;re learning machine learning and you want a way to recommend people movies so they watch more and spend more time on your app so you get more ad revenue. How would you solve this problem? . Let&#39;s look at Bingus. He&#39;s a McMaster alumni turned McDonald&#39;s employee. He works an 8 AM to 6 PM shift flipping burgers for minimum wage and gets home to watch some movies. He really likes this one movie called &quot;Cars 3&quot;. He&#39;s watched dozens of times now and he won&#39;t stop. . Then comes Alice. Unlike Bingus, she graduated from Waterloo and landed a CS job earning six figures. She even graduated with the highest GPA of her year and landed that CS job through a return offer from her summer coop. So what? Alice and Bingus both like &quot;Cars 3&quot;. . Since they both like &quot;Cars 3&quot;, we could recommend Alice movies that Bingus also likes and recommend Bingus movies that Alice also likes. . . Collaborative filtering: look at items the current user used or liked, find other users that used or liked similar items, then recommend those items to the current user. . . Collaborative filtering works through latent factors, which are basically the &quot;tags&quot; you would give an item. For example, you could give a movie tags like &quot;science fiction,&quot; &quot;action,&quot; &quot;old,&quot; &quot;horror,&quot; and &quot;romance.&quot; It&#39;s latent because it depends on users for the factors to have meaning. . What&#39;s wild is we never tell our model the latent factors. We just say how many we want and the model learns them on its own. So, we don&#39;t need to know anything about the items (in a descriptive way); we just need the data on the users and the items. . How do you implement collaborative filtering? . Our model will work using stochastic gradient descent. So, we need three things: . random parameters; | a way to calculate our predictions; and | a loss function. | . We first randomly initialize some parameters for our latent factors. Let&#39;s say we want our model to learn 5 latent factors. Then, each user and item would have 5 parameters (or, a 5-dimensional vector). We can use the dot product between the user and the item as our prediction of how likely we would recommend an item to a user. The higher the value, the more likely the user will like that product. . Finally, we need to pick a loss function. Since we&#39;re dealing with regression instead of classification (like digit classification), we can use L1 norm or L2 norm. . . L1 norm (absolute mean difference): take the absolute difference between two values and take the mean. . $$L_1= sum^n_{i=0} frac{|a_i-b_i|}{2}, ,a in A, , b in B$$ . L2 norm (root mean squared error): take the square of the differences between two values and take the mean. Then, square root the result. $$L_2= sqrt{ sum^n_{i=0} frac{(a_i-b_i)^2}{2}}, ,a in A, , b in B$$ . . What&#39;s the difference? L2 norm puts a larger emphasis on small and large changes because of squaring since large * large = larger and small * small = smaller. . Now, we have all we need for stochastic gradient descent: random parameters, a way to calculate our predictions, and our loss function. . At each step, the SGD optimizer calculates the match between each item and user (random parameters) using the dot product (procedure for predictions), and compares it to the actual rating that each user gave to each item (loss function). Then, it calculates the derivative of this value (gradient) and steps the weights by multiplying the calculated derivative by the learning rate and subtracting the weights by that value (descent). . After each epoch, the loss gets better (lower) and the recommendations will also get better. . But something&#39;s missing... . There&#39;s usually a range for ratings on an item. Like &quot;out of 5 stars&quot;. So, you should put a range on your predictions so that they&#39;re in a similar range as the rating system you have for an item. For example, if a movie service has a rating system from 0 to 5, you should have a range from 0 to 5.5. It&#39;s been discovered empirically that having the upper bound a little bit over returns better results. . And, remember how parameters are the weights and biases? We should also have biases attached to each user and each item. Some users may be more positive or negative than others. And, some items may be superior than others. It could also reflect current trends. Nonetheless, adding biases on their own usually leads to overfitting. . So, you also need to add L2 regularization, . . L2 regularization (weight decay): add the sum of all the weights squared to your loss function. . . Why? Because when you compute the gradients, the added sum encourages the weights to be as small as possible. It prevents overfitting because having high parameters lead to sharper changes in the loss function, which can lead to overfitting. So, having smaller parameters encouraged by weight decay decreases that. . However, you don&#39;t apply weight decay by adding the sum of all weights squared to the loss function (it would be inefficient and lead to huge numbers). Instead, add double the parameters to the gradient since the derivative of $x^2$ is $2x$. And, weight decay is just a hyperparameter that you multiply $2x$ by, so what you actually do is gradient += wd * 2 * parameters, which is essentially gradient += wd * parameters (2 is incorporated into wd like how you just have + C for integrals). The end result of adding biases with weight decay is that we make training the model a bit harder, but the model will generalize better in practice. . Interpreting the model . Now, you finished training your model. You have your biases and latent factors (weights) all set. How can you interpret your parameters before putting your model in action? . With biases, you can sort items to see . current trends; and | which items are good (high bias) or bad (low bias). | . Interpreting the latent factors is a bit trickier in that you can&#39;t just model it. But, there is a technique called principal component analysis, which lets you take the most important directions in the latent factors. . There&#39;s a simpler alternative if you just want to compare a few items: you can calculate the &quot;distance&quot; between two items. If two items were very similar, then their latent factors would also be similar. So, their &quot;distance&quot; would be low compared to the distance between a more different item. Ultimately, item similarity in a model is dictated by the similarity of users that like those items. . To calculate to distance, you use Pythagoras&#39; formula: . $$d= sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$$ . except you would do this for how many dimensions there are. For example, the distance between two 50-dimensional embedding (the parameters) would be . $$d= sqrt{(x_{2,1}-x_{1,1})^2+(x_{2,2}-x_{1,2})^2+ dots+(x_{2,50}-x_{1,50})^2}$$ . So, say you have one movie, &quot;Cars 3&quot; and two other movies: &quot;Cars 4&quot; and &quot;Harry Potter&quot;. The distance between &quot;Cars 3&quot; and &quot;Cars 4&quot; might be 50, while it&#39;s 100 for &quot;Cars 3&quot; and &quot;Harry Potter&quot;. Then, since the distance for the former is shorter, you could infer that &quot;Cars 3&quot; is more similar to &quot;Cars 4&quot; than &quot;Cars 3&quot; is to &quot;Harry Potter&quot;. . But wait, there&#39;s a problem . We all know by now that overfitting is a big problem in the training process. But, there&#39;s an equally important problem in practice for collaborative filtering: the bootstrapping problem. . . Bootstrap: to start something with little help. . Bootstrapping problem: what do you do when you have no users (no data) to train your model; and, if you do have previous users, what do you recommend for a new user? Similarly, what do you do when you add a new item? . . Like overfitting, there isn&#39;t a solution that works for everything, but there are some used commonly: . assign the mean/median of all the latent factors to the new user or item; | pick a specific user or item to represent the average user; | survey the new user or item to construct a basic set of latent factors for them. | . However, solutions to the bootstrapping problem leads to another problem: positive feedback loops. A small number of otakus can set the recommendation for the entire user base. You might expect this feedback loop to be an outlier, but it&#39;s actually the norm. For example, even though not a lot of people watch anime, a few people really enjoy &quot;Demon Slayer&quot;; so when the movie came out, it became highly recommended for the general user base. Similarly, &quot;Squid Game&quot; also became popular this way along with many Korean movies like &quot;Parasite&quot; and &quot;Train to Busan&quot;. It&#39;s only when the systems do something about it (like deliberately lower its bias, don&#39;t recommend it anymore, or through time) that the feedback loop stops. . The bias for certain items in the latent factors may be due to representation bias. If you don&#39;t want your entire user base (and your system) to change, then you have to be wary of these feedback loops. Once the bias becomes too high, more people of the same group come along and your user base ends up being that group. . An easy way to prepare for feedback loops is to integrate your model slowly: . first, have people monitor the model and its recommendations; | then, monitor the recommendations over time; and | eventually let the model recommend on its own. | . But what about deep learning? . Our first method isn&#39;t deep learning, but instead called probabilistic matrix factorization. Instead of a neural network, we used the dot product to calculate our predictions. . With deep learning, we need a neural network, which contains all the layers with parameters that we optimize in each epoch. So, we also don&#39;t need the same number of latent factors for items and users since we won&#39;t be using the dot product. . What you&#39;ll find is that deep learning is a bit worse than probabilistic matrix factorization on its own. But, you can add other user and item information, date and time information, and/or any other information that might be relevant to the recommendation. .",
            "url": "https://geon-youn.github.io/DunGeon/tabular/2022/03/12/Collaborative-Filtering.html",
            "relUrl": "/tabular/2022/03/12/Collaborative-Filtering.html",
            "date": " • Mar 12, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "So, what is deep learning?",
            "content": "So what is deep learning? From what I&#39;ve seen so far of the fast.ai course (chapters 1 to 7), deep learning is training a model with labelled data such that it is able to predict labels for unlabelled data in practice. . With a standard algorithm, you get inputs, you give it to the algorithm, and you get an output; you know the code in the algorithm (or at least, you should understand it). It&#39;s similar in a deep learning model: you know how the model is going to be trained (by hyperparameters like loss function, architecture like resnet18, optimizer like SGD, number of epochs, etc.) and you can look into the parameters (what the model learned; e.g., in vision models you can look at the layers and see how the earlier layers learn things like edges, gradients, etc. while the later layers learn more of the complex things like dog faces). . The difference is that you didn&#39;t hard code the parameters - hopefully - like how you hard coded the algorithm (or copied from a library or Stack Overflow). You have to train the model by giving it labelled data. Emphasis on labelled data. The model can&#39;t learn if it doesn&#39;t have anything to learn from. Why? Because you&#39;re creating a model to predict labels so your data should be labelled. So, a lot of deep learning ends up just labelling your data correctly (or labelling your data in general). . Then, there&#39;s overfitting - an obstacle that prevents your model from being useful. Imagine this: your model predicts your training data with near 100% accuracy. Congrats! You should get an award! Oh, wait, your model can only predict the validation set with 90% accuracy. Oh, double wait, your model can only predict the test set with 50% accuracy... WHAT&#39;S HAPPENING? Your model is memorizing (over-fitting) your training data instead of actually learning from it. . So, you should be careful when training your model. There&#39;s so many options you can tune when training a model. From what I&#39;ve seen so far, you usually overfit by . having a large number of epochs, | using too large or small learning rate, | having a poorly split dataset (like having a random split on a time series data), and | making some post-training decisions that you end up overfitting on the validation set, and | not using a pre-trained model (if one exists for your intentions). | . Remember, you want your model to be good at generalizing on previously unseen data... unless you have a dataset that&#39;s so large that there is no previously unseen data. It doesn&#39;t matter how well your model does on your training and validation sets if it doesn&#39;t work with the test set or a random piece of data you give it! . Then, What is a Deep Learning Model? . A deep learning model consists of layers. Remember how I mentioned resnet18? That&#39;s an architecture (pre-trained too, which means it&#39;s been trained on a dataset before and we&#39;re retraining it with our own dataset which saves time and money since we&#39;ll start with a pretty high accuracy). An architecture is the skeleton of a deep learning model. It contains all the layers and parameters. resnet18 as you could assume, contains 18 layers. . A layer is technically composed of two things. The first is a linear layer ($w cdot x+b$, where $w$ are the weights, $x$ is the data or input, and $b$ is the bias; what people call the parameters of a model is the weights and biases) followed by a non-linear layer typically a ReLU (rectified linear unit, which turns all negative numbers to 0 and keeps positive numbers as is). So, you may be wondering: why do we need both? Well, if you only had linear layers, you could combine all of them into a single layer. So, we need some kind of nonlinearity so that we can prevent the linear layers from becoming one big linear layer. . Now, lets say you have an image. We transform the data, for example through resizing and augmenting it (sort of like distorting it, but in a good way), such that it&#39;s $224 times 224$ pixels large. That&#39;s $50176$ pixels. Then, there could be $4$ more categories for red, green, blue, and alpha (transparency). That becomes $200704$ input values for the first layer. With each layer, you want to decrease that value until the last layer, where you&#39;ll have $n$ outputs since you have $n$ different labels. . Then, how does the model actually learn? Every time you pass a piece of data and you get a prediction, that prediction is passed onto a function called the loss function (what you care about is the metric function, the computer cares about the loss function), using that loss function, each parameter takes a step in the opposite direction of the slope of that loss function at that paramter&#39;s value such that by the next prediction, the value of the loss function is smaller. How big is that step? It&#39;s determined by the learning rate you chose. That&#39;s why it&#39;s important to have a good loss function and a reasonable learning rate. . And, you may be saying, &quot;that&#39;s cool and all, but how does that step actually work?&quot; Each number in a layer (weights and biases) are set so that they keep track of what functions are applied to them. So, when you take a &quot;step&quot; (or optimize; hence, &quot;optimizer&quot; like SGD), you first calculate the function of the derivates of the functions applied through chain rule and takes the value of that gradient (so if the derivative is $f&#39;$ and the number is $x$, the value would be $f&#39;(x)$). Then, with the learning rate, you take a step in the opposite direction such that you descend (parameters -= gradient * learning_rate). Now you can guess why they called the process &quot;stochastic gradient descent.&quot; You descend the loss function (which should be easily differentiable and typically have non-zero gradients) based on the gradient and the parameters are usually randomly assigned, hence &quot;stochastic.&quot; . With each epoch (which is one complete pass through the data), the parameters are finetuned for your task, eventually memorizing the training set. You don&#39;t want that, so you train it for as few epochs as you need (to prevent overfitting and to save time). . Remember when I said &quot;you care about the metric function?&quot; That&#39;s what you want to pay attention to after each epoch. Metric functions are typically things like accuracy or error rate (which is 1 - accuracy), which are helpful for us to analyze our model, but terrible for our model to use as loss functions. . With an overfitting model, you usually see two things: the loss for the training set continues to decrease, but the loss for the validation set is suddenly increasing. The opposite is true for the metric: the metric for the training set is increasing, but the metric for the validation set is decreasing. That&#39;s why you typically want a test set. A training set is the set your model sees when it&#39;s training. A validation set is for your eyes only and used to test how well the model is learning. A test set is for no one&#39;s eyes. Only for God&#39;s eyes or whatever you believe in. Once you finished training your model and tuned all the hyperparameters you want, a test set will tell you how well your model actually trained. . Of course, the validation set and training set are going to be useless if you split your data badly. The common example is with a time series dataset. If you split your data randomly, the model can easily predict intermediate values, but why would you want to know about the past? You want to predict the future. So, you could have your training set consist of data not containing the most recent 6 months, for example. And your validation set consists of the remaining 6 months. You could even take the most recent month out and put the first 5 months into your validation set instead and the most recent month into the test set. Whenever you split your data, make sure it&#39;s with the intention that the model will be learning the patterns of the data such that it can generalize to future, never-seen-before data. . That&#39;s it? . Well, that&#39;s basically what I got so far from fast.ai. With Arthur Samuel&#39;s definition of deep learning, you essentially have a model, where you give it labelled data as input, it produces predictions, then those predictions are passed to a loss function, then you optimize the predictions and repeat for $n$ many epochs until you reach a certain point and stop training the model. . There are many parts that you can change in a model with some that you have to change depending on the task, particularly the loss function. . I&#39;ve mainly been focusing on computer vision since that&#39;s what the first 7 chapters were about (apart from chapter 1; I&#39;m also leaving chapter 3 for last, which is about ethics). . Next time you see me, I&#39;ll be talking about tabular data. .",
            "url": "https://geon-youn.github.io/DunGeon/2022/03/07/What-Is-Deep-Learning.html",
            "relUrl": "/2022/03/07/What-Is-Deep-Learning.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "A pet breed classifier",
            "content": "Intro . I remember volunteering at a hackathon and sitting in the award ceremony when I saw a group win in the &quot;fun&quot; category for creating a pet breed classifier. You give it an image and it&#39;ll tell you what breed it thinks it is and how confident it is. It was &quot;fun&quot; because you could override the threshold and allow images that aren&#39;t cats and dogs to be classified as a dog or cat breed. This blog post will show you how you can train your own pet breed classifer and how it isn&#39;t that hard nor time consuming to do so. You don&#39;t need a beefy computer either since you can use Colab&#39;s GPUs. . Training our own pet breed classifier . First, we&#39;ll download the Pet dataset and see what we&#39;re given: . path = untar_data(URLs.PETS) Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;images&#39;),Path(&#39;annotations&#39;)] . (path/&#39;images&#39;).ls() . (#7393) [Path(&#39;images/english_setter_69.jpg&#39;),Path(&#39;images/scottish_terrier_120.jpg&#39;),Path(&#39;images/basset_hound_113.jpg&#39;),Path(&#39;images/miniature_pinscher_87.jpg&#39;),Path(&#39;images/pomeranian_1.jpg&#39;),Path(&#39;images/Persian_68.jpg&#39;),Path(&#39;images/japanese_chin_39.jpg&#39;),Path(&#39;images/english_setter_107.jpg&#39;),Path(&#39;images/Birman_128.jpg&#39;),Path(&#39;images/staffordshire_bull_terrier_26.jpg&#39;)...] . In this dataset, there are two subfolders: images and annotations. images contains the images of the pet breeds (and their labels) while annotations contains the location of the pet in each image if you wanted to do localization. . The images are structured like so: the name of the pet breed with spaces turned into underscores, followed by a number. The name is capitalized if the pet is a cat. We can get the name of the pet breed by using regular expressions: . fname = (path/&#39;images&#39;).ls()[0] fname, fname.name . (Path(&#39;images/english_setter_69.jpg&#39;), &#39;english_setter_69.jpg&#39;) . # () = extract what&#39;s in the parentheses -&gt; .+ # .+ = any character appearing one or more times # _ = followed by an underscore # d+ = followed by any digit appearing one or more times # .jpg$ = with a .jpg extension at the end of the string re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;english_setter&#39;] . This time, we&#39;ll be using a DataBlock to create our DataLoaders . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = partial(get_image_files, folders = &#39;images&#39;), splitter = RandomSplitter(), get_y = using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms = Resize(460), batch_tfms = aug_transforms(size = 224, min_scale = 0.75)) dls = pets.dataloaders(path) . In our pets DataBlock, we give it the following parameters: . blocks = (ImageBlock, CategoryBlock): our independent variable is an image and our dependent variable is a category. | get_items = partial(get_image_files, folders = &#39;images&#39;): we are getting our images recursively in the images folder. If you&#39;ve used functional programming before, partial is like currying; we give a function some of its parameters and it returns another function that accepts the rest of its parameters, except partial allows us to specify which parameters we want to give. | splitter = RandomSplitter(): randomly splits our data into training and validation sets with a default 80:20 split. We can also specify a seed if we want to test how tuning our hyperparameters affects the final accuracy. | . The final two parameters are part of &quot;presizing&quot;: . item_tfms = Resize(460): picks a random area of an image (using its max width or height, whichever is smallest) and resizes it to 460x460. This process happens for all images in the dataset. | batch_tfms = aug_transforms(size = 224, min_scale = 0.75): take a random portion of the image which is at least 75% of it and resize to 224x224. This process happens for all images in a batch (like the batch we get when we call dls.one_batch()). | . We first resize an image to a much larger size than our actual size for training so that we can avoid the data destruction done by data augmentation. The larger size allows tranformation of the data without creating empty areas. . . We can check if our DataLoaders is created successfully by using the .show_batch() feature: . dls.show_batch(nrows = 1, ncols = 4) . We can then do some Googling to make sure our images are labelled correctly. . Fastai also allows us to debug our DataBlock in case we make an error. It attemps to create a batch from the source: . pets.summary(path) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_179.jpg applying PILBase.create gives PILImage mode=RGB size=500x334 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_179.jpg applying partial gives great_pyrenees applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(21) Final sample: (PILImage mode=RGB size=500x334, TensorCategory(21)) Collecting items from /root/.fastai/data/oxford-iiit-pet Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=500x334, TensorCategory(21)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(21)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(21)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([21, 30, 15, 2], device=&#39;cuda:0&#39;)) . . Now, let&#39;s get to training our model. This time, we&#39;ll be fine tuning a pretrained model. This process is called transfer learning, where we take a pretrained model and retrain it on our data so that it can perform well for our task. We randomize the head (last layer) of our model, freeze the parameters of the earlier layers and train our model for one epoch. Then, we unfreeze the model and update the later layers of the model with a higher learning rate than the earlier layers. . The pretrained model we will be using is resnet34, which was trained on the ImageNet dataset with 34 layers: . learner = cnn_learner(dls, resnet34, metrics = accuracy) . lrs = learner.lr_find() . learner.fit_one_cycle(3, lr_max = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 1.542125 | 0.296727 | 0.900541 | 01:14 | . 1 | 0.618474 | 0.227452 | 0.924222 | 01:13 | . 2 | 0.401809 | 0.214500 | 0.932341 | 01:12 | . learner.unfreeze() lrs = learner.lr_find() . learner.fit_one_cycle(6, lr_max = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 0.340459 | 0.213287 | 0.928281 | 01:16 | . 1 | 0.341917 | 0.233392 | 0.921516 | 01:16 | . 2 | 0.277254 | 0.187060 | 0.939107 | 01:16 | . 3 | 0.191343 | 0.192029 | 0.938430 | 01:16 | . 4 | 0.156336 | 0.178532 | 0.941813 | 01:16 | . 5 | 0.123608 | 0.174198 | 0.939107 | 01:16 | . When we use a pretrained model, fastai automatically freezes the early layers.We then train the head (last layer) of the model for 3 epochs so that it can get a sense of our objective. Then, we unfreeze the model and train all the layers for 6 more epochs. After training for a total of 9 epochs, we now have a model that can predict pet breeds accuractely 94% of the time. We can use fastai&#39;s confusion matrix to see where our model is having problems: . interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix(figsize = (12, 12), dpi = 60) . interp.most_confused(5) . [(&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 6), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 5), (&#39;chihuahua&#39;, &#39;miniature_pinscher&#39;, 5)] . Using the .most_confused feature, it seems like most of the errors come from the pet breeds being very similar. We should be careful however, that we aren&#39;t overfitting on our validation set through changing hyperparameters. We can see that our training loss is always going down, but our validation loss fluctuates from going down and sometimes up. . And that&#39;s all there is to training a pet breed classifier. You could improve the accuracy by exploring deeper models like resnet50 which has 50 layers; training for more epochs (whether before unfreezing or after or both); using discriminative learning rates (giving lower learning rates or early laters using split(lr1, lr2) in the lr_max key-word argument in fit_one_cycle). . Using our own pet breed classifier . First, let&#39;s save the model using .export(): . learner.export() . Then, let&#39;s load the .pkl file: . learn = load_learner(&#39;export.pkl&#39;) . Create some basic UI: . def pretty(name: str) -&gt; str: return name.replace(&#39;_&#39;, &#39; &#39;).lower() . def classify(a): if not btn_upload.data: lbl_pred.value = &#39;Please upload an image.&#39; return img = PILImage.create(btn_upload.data[-1]) pred, pred_idx, probs = learn.predict(img) out_pl.clear_output() with out_pl: display(img.to_thumb(128, 128)) lbl_pred.value = f&#39;Looks like a {pretty(pred)} to me. I &#39;m {probs[pred_idx] * 100:.02f}% confident!&#39; btn_upload = widgets.FileUpload() lbl_pred = widgets.Label() out_pl = widgets.Output() btn_run = widgets.Button(description = &#39;Classify&#39;) btn_run.on_click(classify) VBox([ widgets.Label(&#39;Upload a pet!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . And there we have it! You can make it prettier and go win a hackathon. . However, a bit of a downside with deep learning is that it can only predict what it has been trained on. So, drawings of pets, night-time images of pets, and breeds that weren&#39;t included in the training set won&#39;t be accurately labelled. . We could solve the last case by turning this problem into a multi-label classification problem. Then, if we aren&#39;t confident that we have one of the known breeds, we can just say we don&#39;t know this breed. . Siamese pair . When I was watching the fastai lectures, I heard Jeremy talking about &quot;siamese pairs&quot; where you give the model two images and it will tell you if they are of the same breed. Now that we have a model, let&#39;s make it! . def pair(a): if not up1.data or not up2.data: lbl.value = &#39;Please upload images.&#39; return im1 = PILImage.create(up1.data[-1]) im2 = PILImage.create(up2.data[-1]) pred1, x, _ = learn.predict(im1) pred2, y, _ = learn.predict(im2) out1.clear_output() out2.clear_output() with out1: display(im1.to_thumb(128, 128)) with out2: display(im2.to_thumb(128, 128)) if x == y: lbl.value = f&#39;Wow, they &#39;re both {pretty(pred1)}(s)!&#39; else: lbl.value = f&#39;The first one seems to be {pretty(pred1)} while the second one is a(n) {pretty(pred2)}. I &#39;m not an expert, but they seem to be of different breeds, chief.&#39; up1 = widgets.FileUpload() up2 = widgets.FileUpload() lbl = widgets.Label() out1 = widgets.Output() out2 = widgets.Output() run = widgets.Button(description = &#39;Classify&#39;) run.on_click(pair) VBox([ widgets.Label(&quot;Siamese Pairs&quot;), HBox([up1, up2]), run, HBox([out1, out2]), lbl ]) . You can now test out these cells here! .",
            "url": "https://geon-youn.github.io/DunGeon/vision/2022/02/21/Pet-Breeds.html",
            "relUrl": "/vision/2022/02/21/Pet-Breeds.html",
            "date": " • Feb 21, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Pixel Similarity vs. Basic Neural Net on the MNIST data set",
            "content": "Pixel similarity . We will take the average of each digit to get its &quot;perfect&quot; version. Then, we compare an image to each of those perfect numbers and see which one is the most similar. . First, we&#39;ll download the MNIST dataset: . path = untar_data(URLs.MNIST) . The dataset is separated into training and testing subfolders, where in those folders, there are separate folders for each digit: . Path.BASE_PATH = path . path.ls(),(path/&#39;training&#39;).ls() . ((#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)], (#10) [Path(&#39;training/7&#39;),Path(&#39;training/8&#39;),Path(&#39;training/1&#39;),Path(&#39;training/6&#39;),Path(&#39;training/3&#39;),Path(&#39;training/2&#39;),Path(&#39;training/5&#39;),Path(&#39;training/4&#39;),Path(&#39;training/0&#39;),Path(&#39;training/9&#39;)]) . We&#39;ll store the path of each image in an array, where the ith row contains the path for the ith digit: . nums = [(path/&#39;training&#39;/f&#39;{x}&#39;).ls().sorted() for x in range(10)] . im3_path = nums[3][0] im3 = Image.open(im3_path) im3 . Then, we&#39;ll open the images, put every image of the same digit into their own tensor and store them as a list of tensors: . nums_tens = [torch.stack([tensor(Image.open(j)) for j in nums[i]]) for i in range(10)] nums_tens = [nums_tens[i].float()/255 for i in range(10)] . We can then take the mean of one of the tensors to get its &quot;perfect&quot; version. Here is how it looks like for a 3: . stacked_threes = nums_tens[3].mean(0) show_image(stacked_threes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e3d3acd0&gt; . And to compare, here is just one of those threes: . a_3 = nums_tens[3][0] show_image(a_3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e36f1750&gt; . Next, we&#39;ll create a function that compares two tensors through absolute mean difference: . def mnist_distance(x1, x2): return (x1 - x2).abs().mean((-1, -2)) . Now we can compare one of the threes with its &quot;perfect&quot; version. The number doesn&#39;t really mean anything until we compare it with another number: . mnist_distance(a_3, stacked_threes) . tensor(0.1074) . So, we&#39;ll take the average seven and take the L1 norm (absolute mean difference) and compare that number with the number we just got (0.1074) . stacked_sevens = nums_tens[7].mean(0) show_image(stacked_sevens) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e0401510&gt; . mnist_distance(a_3, stacked_sevens) . tensor(0.1441) . As you can see, the distance between the 3 and the average 3 is smaller than the distance between the 3 and the average 7. So, it is more three than seven. We&#39;ll extend this approach by comparing an image with the average for each digit and say it is the digit it is the most similar to (its L1 norm with that average digit is the smallest). . We&#39;ll create the average number for each digit: . stacked_nums = [nums_tens[i].mean(0) for i in range(10)] show_image(stacked_nums[4]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e2b5e150&gt; . We can compare our 3 to each average digit: . L(mnist_distance(a_3, stacked_nums[i]) for i in range(10)) . (#10) [tensor(0.1750),tensor(0.1153),tensor(0.1501),tensor(0.1074),tensor(0.1635),tensor(0.1326),tensor(0.1579),tensor(0.1441),tensor(0.1345),tensor(0.1402)] . As you can see, it is most similar to the average three. . Now we&#39;ll import the validation set and put them into a list of tensors: . valid_nums = [(path/&#39;testing&#39;/f&#39;{i}&#39;).ls().sorted() for i in range(10)] . valid_nums_tens = [torch.stack([tensor(Image.open(j)) for j in valid_nums[i]]) for i in range(10)] valid_nums_tens = [valid_nums_tens[i].float()/255 for i in range(10)] . We&#39;ll create a function that returns the accuracy of our whole process: . def is_num(x1, x2s, x): # Get the distance between the number and the average digit for each digit vals = [mnist_distance(x1, x2s[i]) for i in range(10)] # Turn the tensors into floats so that we can perform the `min` function vals_2 = [[vals[i][j].item() for i in range(10)] for j in range(len(x1))] # Get a list of tensors that contain a bool value, where it&#39;s true when # the minimum distance is equal to the digit the given number is supposed # to be vals_3 = [tensor(vals_2[i].index(min(vals_2[i])) == x) for i in range(len(x1))] # Return how often our model is correct return tensor(vals_3).float().mean(0) . nums_accuracy = tensor([is_num(valid_nums_tens[i], stacked_nums, i) for i in range(10)]) nums_accuracy, nums_accuracy.mean(0) . (tensor([0.8153, 0.9982, 0.4234, 0.6089, 0.6680, 0.3262, 0.7871, 0.7646, 0.4425, 0.7760]), tensor(0.6610)) . Our model has an overall accuracy of 66.1%! Better than a random guess of 10%, but certainly not good. It is particularly good at guessing if a number is a 1, but particularly bad for 2s, 5s and 8s. . Now that we have a baseline, we can try how good we can get a simple model &quot;from scratch.&quot; . Basic neural network . For our &quot;from scratch&quot; learner, we&#39;ll have 2 layers, where each layer contains a linear layer and a ReLU (rectified linear unit, where all negative numbers become 0). . For our loss function, we will be using cross-entropy loss since we have multiple categories. . First, we&#39;ll make our training and validation datasets and dataloaders. Then, we&#39;ll initialize parameters, figure out how to make predictions, calculate the loss (cross-entropy), calculate the gradients, and then step (using the provided SGD optimizer). . Let&#39;s first redownload the MNIST dataset: . path = untar_data(URLs.MNIST) . Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)] . Then, we&#39;ll save the training and validation images into separate variables: . nums = [(path/&#39;training&#39;/f&#39;{x}&#39;).ls().sorted() for x in range(10)] . nums_tens = [torch.stack([tensor(Image.open(j)) for j in nums[i]]) for i in range(10)] nums_tens = [nums_tens[i].float()/255 for i in range(10)] . valid_nums = [(path/&#39;testing&#39;/f&#39;{i}&#39;).ls().sorted() for i in range(10)] . valid_nums_tens = [torch.stack([tensor(Image.open(j)) for j in valid_nums[i]]) for i in range(10)] valid_nums_tens = [valid_nums_tens[i].float()/255 for i in range(10)] . Next, we&#39;ll create our dataset from our training set. A dataset is a list of tuples, which contains the independent variable and its label (dependent variable) like so: (independent, dependent). . train_x = torch.cat(nums_tens).view(-1, 28*28) . It took a while for me to realize what the .view() function was doing, but what it does is pretty simple. We give it however many values we want (that makes sense) to change the shape of our tensor. Here we give it -1, 28*28 which will turn our rank-3 tensor (n-images of 28 by 28) into a rank-2 tensor (n-images of 28*28). -1 makes it so that we don&#39;t have to specify how many images there are and 28*28 means we want to compress our previous 28 by 28 grid into a 28*28 vector. It&#39;s like turning a 2D array into a 1D array: . # -1 makes it so that we don&#39;t have to know how many images there are nums_tens[0].view(-1, 28*28).shape, nums_tens[0].view(5923, 28*28).shape . (torch.Size([5923, 784]), torch.Size([5923, 784])) . # before we called .view(), our tensor was originally 28x28, but afterwards, it is 28*28 (784) nums_tens[0].size(), nums_tens[0].view(-1, 28*28).shape . (torch.Size([5923, 28, 28]), torch.Size([5923, 784])) . We&#39;ll form our labels by having as many tensors containing the digit&#39;s digit as there are of that digit: . train_y = torch.cat([tensor([i] * len(nums_tens[i])) for i in range(10)]) . train_x.shape, train_y.shape . (torch.Size([60000, 784]), torch.Size([60000])) . # when we take a random 3, we can index into the labels at the same spot and see # that we can 3 as its label show_image(nums_tens[3][200]), train_y[len(nums_tens[0]) + len(nums_tens[1]) + len(nums_tens[2]) +200] . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1e2aa69d0&gt;, tensor(3)) . Like I said before, a dataset is just a list of tuples containing our independent and dependent variables: . dset = list(zip(train_x, train_y)) x, y = dset[0] x.shape, y . (torch.Size([784]), tensor(0)) . And we can see that given a label 0, our image is indeed a zero: . # we have to reshape our image from a 784 long vector into a 28*28 matrix show_image(x.view(28, 28)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb1d9324110&gt; . Now we&#39;ll make the dataset for our validation set: . valid_x = torch.cat(valid_nums_tens).view(-1, 28*28) valid_y = torch.cat([tensor([i] * len(valid_nums_tens[i])) for i in range(10)]) valid_dset = list(zip(valid_x, valid_y)) . Next, we&#39;ll create DataLoaders for our training and validation sets. A DataLoader takes a dataset and each time we use it, it will give a portion of the dataset. We can then work on a portion of the dataset instead of just 1 tuple or the entire set. We can also toggle whether we want our given portion to be randomize (we wouldn&#39;t want to get all 0s, then 1s, then 2s, ... we want a mix): . dl = DataLoader(dset, batch_size = 128, shuffle = True) valid_dl = DataLoader(valid_dset, batch_size = 128, shuffle = True) xb, yb = first(dl) xb.shape, yb.shape . (torch.Size([128, 784]), torch.Size([128])) . Then, we&#39;ll create our DataLoaders. A DataLoaders is like the dataset of a DataLoader: it just contains our training and validation DataLoaders: . dls = DataLoaders(dl, valid_dl) . Our simple neural network uses PyTorch&#39;s nn.Sequential which takes modules and uses the GPU to handle the operations: . simple_net = nn.Sequential( # Our first layer takes in 28*28 inputs and outputs 250 nn.Linear(28 * 28, 250), nn.ReLU(), # Our second layer takes in 250 inputs and outputs 50 nn.Linear(250, 50), nn.ReLU(), # Our final layer takes in 50 inputs and outputs 10 # (its confidence for our image to be each digit) nn.Linear(50, 10) ) . We use cross-entropy loss so that we can turn our 10 outputs into numbers that are from 0 to 1 and sum to 1 like probabilities (through softmax). But, that&#39;s just the first part. We then take the negative log (-log(p)) of those probabilities to give emphasis on the higher probabilities. . We&#39;ll use the given Learner class from fastai (which handles epochs) with the SGD optimizer (stochastic gradient descent, which handles calculating gradients and stepping into lower loss) and use the accuracy metric (the number we care about). . learn = Learner(dls, simple_net, opt_func = SGD, loss_func = F.cross_entropy, metrics = accuracy) . We&#39;ll use the learning rate finder to select a good learning rate for us: . lrs = learn.lr_find() lrs . SuggestedLRs(valley=0.04786301031708717) . learn.fit(20, lr = lrs.valley) . epoch train_loss valid_loss accuracy time . 0 | 0.454874 | 0.400172 | 0.889500 | 00:02 | . 1 | 0.322119 | 0.292965 | 0.916900 | 00:02 | . 2 | 0.264725 | 0.245524 | 0.929800 | 00:02 | . 3 | 0.228520 | 0.216811 | 0.938200 | 00:02 | . 4 | 0.196191 | 0.187768 | 0.945100 | 00:02 | . 5 | 0.181299 | 0.170362 | 0.950000 | 00:02 | . 6 | 0.151747 | 0.152564 | 0.954600 | 00:02 | . 7 | 0.143972 | 0.141233 | 0.957100 | 00:02 | . 8 | 0.125890 | 0.130209 | 0.961300 | 00:02 | . 9 | 0.119570 | 0.117828 | 0.964100 | 00:02 | . 10 | 0.112297 | 0.120144 | 0.963900 | 00:02 | . 11 | 0.096248 | 0.108321 | 0.967100 | 00:02 | . 12 | 0.085236 | 0.100657 | 0.970100 | 00:02 | . 13 | 0.082346 | 0.094316 | 0.970200 | 00:02 | . 14 | 0.077736 | 0.090774 | 0.972000 | 00:02 | . 15 | 0.075661 | 0.093964 | 0.971500 | 00:02 | . 16 | 0.064168 | 0.087111 | 0.973000 | 00:02 | . 17 | 0.062180 | 0.080836 | 0.975900 | 00:02 | . 18 | 0.061466 | 0.077446 | 0.977300 | 00:02 | . 19 | 0.052969 | 0.078910 | 0.975300 | 00:02 | . And we see that our final accuracy is 97.5%! Certainly better than the 66.1% we got from our pixel similarity approach. . To compare, here&#39;s the results using fastai&#39;s provided cnn_learner which uses a pretrained model with 18 layers: . dls2 = ImageDataLoaders.from_folder(path, train=&#39;training&#39;, valid=&#39;testing&#39;) learn2 = cnn_learner(dls2, resnet18, loss_func=F.cross_entropy, metrics=accuracy) learn2.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.124300 | 0.056642 | 0.982900 | 02:04 | . Our model is not bad considering it&#39;s less than 1% in accuracy different from a pretrained model. . We could even make our model better by training for more epochs until the validation loss becomes worse or by using a deeper model. .",
            "url": "https://geon-youn.github.io/DunGeon/vision/2022/02/20/MNIST.html",
            "relUrl": "/vision/2022/02/20/MNIST.html",
            "date": " • Feb 20, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! . My name is Geon and I am an 18 year old Korean-Canadian. I graduated Westdale S.S. in 2021 and am currently attending McMaster University for computer science. I started fast.ai when I was in 12th grade and here I am continuing to learn about deep learning and now I am starting a blog. .",
          "url": "https://geon-youn.github.io/DunGeon/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://geon-youn.github.io/DunGeon/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}