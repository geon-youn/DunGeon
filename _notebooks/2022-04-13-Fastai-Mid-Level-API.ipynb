{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-04-09-Fastai-Mid-Level-API.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# \"How to use fastai's mid-level API\"\n",
        "> \"Now I can train models on Kaggle data sets!\"\n",
        "\n",
        "- categories: [fastai]"
      ],
      "metadata": {
        "id": "Zm_gCBcah3iW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb1boMv-h1Ws"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "!pip install -Uqq fastbook tornado==5.1.0\n",
        "import fastbook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "from fastbook import *"
      ],
      "metadata": {
        "id": "7WnrUxU9wuSO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers!"
      ],
      "metadata": {
        "id": "bSZH-Eh5t8cS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous blog where I trained a model to [predict movie review sentiments](https://geon-youn.github.io/DunGeon/nlp/2022/04/08/Movie-Review-Sentiment.html), I created my `DataLoaders` by using a `TextBlock`. The fastai library is built on a layered API where the top layer has applications; we used the high level API (the `DataBlock` API)."
      ],
      "metadata": {
        "id": "hqLcaOUguCR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![fast.ai is a layered API](https://www.fast.ai/images/fastai_paper/layered.PNG)"
      ],
      "metadata": {
        "id": "wMBIIyDOuhjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At a lower level, `TextBlock` applies both `Tokenize` and `Numericalize`, which tokenize, then numericalize the raw input text. Both classes inherit from the `Transform` class.\n",
        "\n",
        "---\n",
        "\n",
        "An instance of a `Transform` class is an object that has\n",
        "- an `encodes` method that can be called by `()`, allowing the object to be used like a function,\n",
        "- an optional `setup` method to initialize some inner state, and\n",
        "- an optional `decode` method to reverse the function (which may or may not be fully reversible; the importance is that it makes it easier for humans to read).\n",
        "\n",
        "---\n",
        "\n",
        "A `Transform` can work on a tuple. `Transform`s usually have their inputs specified with types so only items in the tuple with the correct type have the `Transform` applied. \n",
        "\n",
        "In writing your own `Transform`, you can either decorate a function with `@Transform` or inherit from `Transform`, where: \n",
        "\n",
        "| To call | To implement |\n",
        "| :-----: | :----------: |\n",
        "| ()      | encodes      |\n",
        "| setup() | setups       |\n",
        "| decode()| decodes      |\n",
        "\n",
        "You have to implement them in a different name since `Transform` does some things before calling `setups` and `decodes` in `setup()` and `decode()`. \n",
        "\n",
        "So with a decorator, we have:"
      ],
      "metadata": {
        "id": "gs4M46sAu7tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Transform\n",
        "def f(x: int):\n",
        "    return x + 1"
      ],
      "metadata": {
        "id": "HiuXVATRyz3y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you define a function with the `@Transform` decorator, you can only define the `encodes` part. "
      ],
      "metadata": {
        "id": "seoRd9avy5WF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f((2, 2.0, '2')), f(2), f((2.0,)), f([2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWD4pqt-NEeW",
        "outputId": "8892fc42-8f9b-4ea9-b268-572307b04ea2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3, 2.0, '2'), 3, (2.0,), [2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply the `Transform` to a tuple (and since we defined it for `int`s it only applies to `2` and not `2.0` or `'2'`), a single element, a single element tuple, or a list.\n",
        "\n",
        "If we wanted to implement `setup()` and `decode()`, we'll have to subclass `Transform`:"
      ],
      "metadata": {
        "id": "LBebRf4cNGei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalize_(Transform):\n",
        "    def setups(self, items, pop=False):\n",
        "        self.mean = sum(items) / len(items)\n",
        "        self.std  = sum([(i - self.mean)**2 for i in items]) / (len(items) - (0 if pop else 1))\n",
        "\n",
        "    def encodes(self, x):\n",
        "        return (x - self.mean) / self.std\n",
        "\n",
        "    def decodes(self, x):\n",
        "        return x * self.std + self.mean"
      ],
      "metadata": {
        "id": "yVeMIIztzbgd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we'll instantiate it and call `setup` with our items:"
      ],
      "metadata": {
        "id": "thE3e_WWzwBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [0, 1, 2, 3, 4, 5, 6]\n",
        "n  = Normalize_()\n",
        "\n",
        "n.setup(xs)\n",
        "n.mean, n.std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w6CXZXwNXX3",
        "outputId": "53f2bb76-374e-4bf2-ab09-8195bd63cd76"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.0, 4.666666666666667)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, we'll test it:"
      ],
      "metadata": {
        "id": "wlkh64wSwJw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x         = (3, 3 + n.std)\n",
        "x_encoded = n(x)\n",
        "x_decoded = n.decode(x_encoded)\n",
        "x, x_encoded, x_decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3CZqe4nwLX7",
        "outputId": "35736684-69a3-4550-880d-cc9703cda0ee"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3, 7.666666666666667), (0.0, 1.0), (3.0, 7.666666666666667))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers assemble!"
      ],
      "metadata": {
        "id": "W8h2VJh-NsPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, you want to use multiple `Transform`s on your raw items. There's 3 main ways to do that with fastai:\n",
        "1. `Pipeline`s,\n",
        "2. `TfmdLists`s, and\n",
        "3. `Datasets`s. \n",
        "\n",
        "Each have their own uses and differences (and there's a reason why some of end with an `s`).\n",
        "\n",
        "Starting with `Pipeline`s, you pass it a list of instantiated `Transform`s:"
      ],
      "metadata": {
        "id": "04E6lHKvNwel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Add 1 to each item, then normalize them\n",
        "p = Pipeline([f, n])\n",
        "p((2, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CmVmUu-Nr8M",
        "outputId": "2ed8b56b-9ed7-4888-dffb-191e0b15ce41"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.21428571428571427)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to pass in a list of `Transform` classes or functions, we need to use `TfmdLists` instead:"
      ],
      "metadata": {
        "id": "yp5k27xOOh3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Add 1 to each item, then use those values to setup Normalize\n",
        "# (now the values are centered at 4 instead of 3)\n",
        "tl = TfmdLists(xs, [f, Normalize])\n",
        "tl((2, 3, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B95rfTsQPt0h",
        "outputId": "e3963de0-13ab-4c91-b32d-37f11dbee969"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.21428571428571427, 0.0, 0.21428571428571427)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `TfmdLists`, we provide the raw items needed for the setup of each `Transform` and a list of the `Transform`s we want to use. At initialization, `TfmdLists` calls the `setup()` of each `Transform`, but it passes the raw items transformed by all previous `Transform`s in order instead of the raw items.  \n",
        "\n",
        "`TfmdLists` ends with an `s` since it can handle splits for training and validation sets. To split the data, you have to specify a split:"
      ],
      "metadata": {
        "id": "fTOJ6oJJQCNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tls = TfmdLists(xs, [f, Normalize], splits=RandomSplitter()(xs)) \n",
        "tls.mean, tls.std, tls.train.items, tls.valid.items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2a0b5UWRmJi",
        "outputId": "3ddbd501-1006-4f1a-9cda-39e8385c4422"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.5, 3.5, [6, 2, 4, 3, 5, 1], [0])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should be careful though because the `setup` of the `Transform`s will be done with the raw items in the `train` set instead of the entire set. \n",
        "\n",
        "Finally with `Datasets`, you can think of it as multiple `TfmsLists` put together in a tuple, where each item produced by a `Datasets` is `(tls1, tls2, ...)`. In general, we'll have two parallel pipelines of `Transform`s: (1) to process raw items into inputs and (2) to process raw items into targets. But, you can also have as many parallel pipelines as you want (for example, if you have multiple inputs and/or multiple targets; that's why there's the `...` in `(tls1, tls2, ...)`).  \n",
        "\n",
        "So, for a `Datasets`, it could look like this:"
      ],
      "metadata": {
        "id": "NGAZKcp2SVR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class better_f(Transform):\n",
        "    def encodes(self, x):\n",
        "        return x + 1\n",
        "    def decodes(self, x):\n",
        "        return x - 1\n",
        "\n",
        "x_tfms = [better_f, Normalize]\n",
        "y_tfms = []         # a pipeline can also be empty\n",
        "z_tfms = [Identity] # if empty is boring, you can also use Identity\n",
        "\n",
        "dsets = Datasets(xs, [x_tfms, y_tfms, z_tfms], splits=RandomSplitter()(xs))\n",
        "\n",
        "dsets, dsets.train, dsets.valid"
      ],
      "metadata": {
        "id": "-8V_hhB_TCjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4994d3e-a1fe-4f33-9340-db7f0ab577cf"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((#7) [(-0.5357142857142857, 0, 0),(-0.35714285714285715, 1, 1),(-0.17857142857142858, 2, 2),(0.0, 3, 3),(0.17857142857142858, 4, 4),(0.35714285714285715, 5, 5),(0.5357142857142857, 6, 6)],\n",
              " (#6) [(0.35714285714285715, 5, 5),(0.17857142857142858, 4, 4),(-0.35714285714285715, 1, 1),(-0.5357142857142857, 0, 0),(-0.17857142857142858, 2, 2),(0.5357142857142857, 6, 6)],\n",
              " (#1) [(0.0, 3, 3)])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And since we redefined `f` as a subclass of `Transform` with a `decode` method, we can get our raw items by decoding them:"
      ],
      "metadata": {
        "id": "FZmrJbPqDmeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[dsets.decode(dsets[i]) for i in range(len(xs))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYjaCTONDa87",
        "outputId": "41c092c9-8878-471d-ebb5-acc40b25f008"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.0, 0, 0),\n",
              " (1.0, 1, 1),\n",
              " (2.0, 2, 2),\n",
              " (3.0, 3, 3),\n",
              " (4.0, 4, 4),\n",
              " (5.0, 5, 5),\n",
              " (6.0, 6, 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can create `DataLoaders` from a `Dataset` using the `dataloaders` attribute:"
      ],
      "metadata": {
        "id": "tN65aSa9G6SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dls = dsets.dataloaders(bs=2)\n",
        "dls.train.one_batch(), dls.valid.one_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toqV3HT4HTG7",
        "outputId": "7e1902e3-c1ae-4818-8a95-35a50999590a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([0.1786, 0.5357], dtype=torch.float64),\n",
              "  tensor([4, 6]),\n",
              "  tensor([4, 6])),\n",
              " (tensor([0.], dtype=torch.float64), tensor([3]), tensor([3])))"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dataloaders` works by calling `DataLoader` on each subset of our `Datasets` (like train and valid) and then putting them together into a `DataLoaders`.\n",
        "\n",
        "The `dataloaders` has a few important parameters that are equivalent to the ones we use in `DataBlock`s: \n",
        "\n",
        "- `after_item` takes `Transform`s and applies them on each item after grabbing them from the dataset (equivalent to `item_tfms` in `DataBlock`).\n",
        "- `before_batch` is applied to each item in a batch before they're collated.\n",
        "- `after_batch` is applied on the batch after collation (equivalent to `batch_tfms` in `DataBlock`).\n",
        "\n",
        "When would you want to use `before_batch`? When you want to apply something to each item in a batch instead of on the entire batch like in `after_batch`. For example, padding the documents for text so that all the items in the batch are of the same token length.\n",
        "\n",
        "You can also specify the type of `DataLoader` you want. In NLP, you might want to use `SortedDL` through `dsets.dataloaders(dl_type=SortedDL)` which batches items of roughly the same length by sorting them beforehand.\n",
        "\n",
        "Finally, when we call `show_batch` or `show_results` on a `DataLoaders` (or `show` on a `TfmdLists` or a `Datasets`), it continues to decode items until it reaches a type that has a `show` method. If there's no types with `show` (like  `Tensor`s), we get an error:"
      ],
      "metadata": {
        "id": "wYKLAGFrInt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#collapse_output\n",
        "dls.show_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "_AuH7rB2Krx9",
        "outputId": "e0786082-46c3-492c-f7dd-9ebede5cb37b"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-03961d1c4ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#collapse_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/core.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, b, max_n, ctxs, show, unique, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_show_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_show_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_get_idxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastcore/dispatch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/core.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(x, y, samples, ctxs, max_n, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mctxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mctxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mctxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mctxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'show'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix this error, you'll have to define (or use) a custom type with a `show` method that can accept a `ctx` as a keyword argument (which could be a `matplotlib` axis for images or a row of a `DataFrame` for texts). \n",
        "\n",
        "Then, you need to include a `Transform` in the pipeline that converts your inputs into that custom type and ideally is the first `Transform` in the pipeline so that when you call `show`, it decodes all the way to the raw items. "
      ],
      "metadata": {
        "id": "smcZszf3ofbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "S4r5yd5h1-HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this blog, I covered the lower-level parts of the fastai library: `Transform`s and how to use them through `Pipeline`s, `TfmdLists`s, and `Datasets`s. In the real world, the higher-level `DataBlock` API might not be flexible enough. So, you'll have to use the more flexible lower-level APIs that let you define your own `Transform`s, data types, and `DataLoaders`."
      ],
      "metadata": {
        "id": "DQqE0C6j1_WJ"
      }
    }
  ]
}